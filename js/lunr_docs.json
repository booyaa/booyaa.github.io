

[


    { 
        "title" : "Adding missing functionality to Terraform",

        
        
        
        "tags": [
        
            "terraform"
            
        
        ],
        "href" : "2019/adding-missing-functionality-to-terraform",
        "content" : "I needed to codify the creation of PostgreSQL read replicas, so I did a bit of research around ways I could do this quickly without diving into the Terraform provider.The quickest way to do this was to:use the local-exec provisioner to invoke the Azure CLI commands (details to follow)wrap the code in a module (to allow for reuse and share with the community)The Azure docs requires the following steps to be carried out to create a read replica using the Azure CLI are:Enable replication support on the primary serverRestart the primary server (for the changes to take effect)Create the replica using the primary server as the sourceCaveat emptor: as the Terraform docs mention, provisioners are a last resort. A major downside of using this method to add missing functionality is that there's no state tracking, i.e. if you make a change to the resource, Terraform won't know about it.Here's the essence of the code (I've omitted certain details for brevity the full code is on GitHub):resource &quot;null_resource&quot; &quot;postgresql-read-replica&quot; {  triggers = {    resource_group_name            = var.resource_group_name    postgresql_primary_server_name = var.postgresql_primary_server_name    postgresql_replica_server_name = var.postgresql_replica_server_name  }The null_resource is also provisioner is used as a container for the local_exec calls. The triggers block allows the resource to be replaced, i.e. destroyed and recreated when the resource group, the PostgreSQL primary or replica server names changes.You can already see that modules are just ordinary bits of Terraform code.  provisioner &quot;local-exec&quot; {    command = &lt;&lt;ENABLE_REPLICATIONaz postgres server configuration set \\...ENABLE_REPLICATION  }  provisioner &quot;local-exec&quot; {    command = &lt;&lt;RESTART_SERVERaz postgres server restart \\...RESTART_SERVER  }  provisioner &quot;local-exec&quot; {    command = &lt;&lt;CREATE_REPLICAaz postgres server replica create \\...CREATE_REPLICA  }These three provisioner blocks perform the required actions to create a read replica using the Azure CLI. To avoid having to escape quotes we're using the here doc notation.  provisioner &quot;local-exec&quot; {    when = &quot;destroy&quot;    command = &lt;&lt;DESTROY_REPLICAaz postgres server delete \\  --name ${var.postgresql_replica_server_name} \\  --resource-group ${var.resource_group_name} \\  --yesDESTROY_REPLICA  }}Finally, we handle when what to do when the replica is destroyed.I've uploaded the module to the Terraform registry which means the module can be easily referenced just like another resource:module demo-replica {  source                         = &quot;booyaa/terraform-azurerm-postgresql-read-replica&quot;  resource_group_name            = azurerm_resource_group.demo.name  postgresql_primary_server_name = azurerm_postgresql_server.demo.name  postgresql_replica_server_name = &quot;${azurerm_postgresql_server.demo.name}-replica&quot;}Just like the Data Sources, modules can be used as a reference so we can now apply a firewall rule against the read replica:resource &quot;azurerm_postgresql_firewall_rule&quot; &quot;demo-replica&quot; {  name                = &quot;office&quot;  resource_group_name = azurerm_resource_group.demo.name  server_name         = module.demo-replica.replica_name  start_ip_address    = &quot;8.8.8.8&quot;  end_ip_address      = &quot;8.8.8.8&quot;  depends_on = [module.demo-replica]}"
    },


    { 
        "title" : "Learning about eBPF on macOS",

        
        
        
        "tags": [
        
            "linux",
            
        
            "ebpf"
            
        
        ],
        "href" : "2019/learning-about-ebpf-on-macos",
        "content" : "I've created this is a short post to talk about a new GitHub repo that might be useful to some: vagrant-bcctools.It's a simple Vagrant box using the latest (at the time of writing) version of Ubuntu (bionic) with the bcc tools package installed.I needed a way to play around with eBPF on macOS locally. So before embarking on a fool's errand, I did some research. For details about my findings, see the repo's README.md.I saw there was a Docker image, which doesn't work because I think it expects the underlying Docker host to be Linux base (volume mounts to /lib/modules, /usr/src and /etc/localtime). The Vagrantfile provided by IO Visor is using such an old version of Ubuntu that a modern version of Vagrant seems to choke on it.No doubt someone will point me to something that only requires xhyve (at me on Twitter or dev.to if you do know)."
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Exam Time!",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-exam-time",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.Exam Time!At the end of June, I sat the AWS DevOps Profession exam and sadly readers I did not pass. I hadn't really expected to pass the first time, but I scored 69% (you need to score 75% to pass)!After completing the exam you're given an immediate PASS or FAIL result. A few days later you get the actual score and areas that need improving.Domains where I have demonstrated adequate knowledgeConfiguration management and Infrastructure as CodeMonitoring and loggingPolicies and Standards AutomationDomains where I need to brush up onSDLC automationIncident and Events responseHigh Availability, Fault tolerance and Disaster recoveryThings I wish I knew beforehandThe main and most obvious thing I should've done was to focus my knowledge around best practices to build infrastructures and utilising AWS services.Most of the questions were of a similar format to the sample exam and mock exam. They wanted you to pick the best answer to meet the requirements of a customer. There were so many services that I hadn't had hands-on experience, which meant I spent a lot of time just trying to gain a basic understanding of what each service did and their use cases. Turns out I needed to a deeper understanding of the domains I scored poorly against.I think the only domain I was surprised at scoring poorly against was SDLC automation, as I do a lot of CI/CD on a daily basis, although this tends to be around non-AWS services: Circle CI, Travis CI and Azure DevOps.So, how can you find out what the best practices are? There are probably two key documents:The AWS whitepapers, which often contain solutions that combine various AWS products and servicesThe service's user or developer guideMy study before retaking exam will focus on this. I only spent the week before the exam skimming through the whitepapers, but this time I plan to make a lot more notes and try to memorise diagrams of solutions provided.Tips for sitting the examCheck your gearThe exam centres will require you to place all your belongings in a locker.Some centres will allow you to bring cups of water to the exam, take two cups if you can carry them. You'll also be given paper and pen, or something similar to make notes. Check the pen works before you start the exam!Empty your bladder before you sit the exam, some places require you get the attention of the invigilator which can cost you precious minutes of your exam time.TimekeepingStart getting used to scanning questions in 2-minute intervals. This is the max you should spend on reading and answering a question.In a perfect scenario where you could answer each question fully and not skip any, it would be easy to know the next 2-minute interval. In reality, you will probably skip or answer a question in less than 2 minutes, so whilst this might be obvious to most, I used the following to keep myself on track: as I started the next question I made a note of the current time on the exam countdown timer and worked out how long I had to answer the question, so if timer was at 1 hour and 7 minutes, I would need to answer the question on or before 1 hour and 5 minutes.Skip and don't ditherUnless you're feeling particularly confident about your capabilities, you'll probably be in a blind panic (like I was). Give yourself up to 30 seconds (better if you can do it in less) to scan the question, if you don't even know where to begin then skip it. That time is better spent on questions where you have a vague notion of what the answer is.Skipping questions is okay and you will find there's still time to revisit these questions once you've gone through all the questions.Don't panic if you find you're skipping lots of questions either, it's probably your nerves.Alternative study aidsWhilst studying for this exam, I've used the following to help absorb the study material:My AWS DevOps Pro Tinycards deck is a flashcard app by Duolingo. It's a bit rough and ready, but it's helped me retain some of the facts and figures that pop up.I've also been listening to Last week in AWS which is a podcast by Corey Quinn that covers the ever-changing AWS landscape.Until next timeI will, of course, let you know when I sit and pass the next exam (see what I did there). I'm aiming for mid-July so expect to hear from me soon!AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Study Gaps",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-study-gaps",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.Study GapsThis section will change a lot as I find new gaps whilst sitting in mock exams.I've had a go at the sample exam under exam condititions (which before AWS made the exams adaptive would leave you with about 2 mins per question). Here's some items where I need to fill in gaps:GeneralKnowing which services are able to use Resource Based Policies:Lambda (Configuration Management and Infrastructure as Code).ECR (via ECS - Configuration Management and Infrastructure as Code).CloudWatch Logs (Monitoring and Logging).AWS Secrets Manager (Policies and Standards Automation).SDLC AutomationNeed to read the blue/green whitepaper (SDLC automation). Pssst if you have the time you should read all the DevOps related whitepapers!Blue/Green Techniques using CloudFormation or manually provisioned i.e. through AWS ConsoleThis is based on the Blue/Green whitepaper.Update DNS Routing with Amazon Route 53 SetupRoute 53 DNSBlue/Green Environments Elastic Load Balancer (ELB)Autoscaling group behind xthe ELBBoth environments are point to the same database instance (Amazon RDS Multi-AZ)Sub patternsClassic DNS pattern - Flip alias (live) record from blue to greenClassic DNS-weighted distribution - Use split to send traffic to different environmentsSwap the Auto Scaling Group Behind Elastic Load BalancerSetupRoute 53 DNSELB pointing toBlue and Green Auto Scaling GroupsBoth ASGs point to the same database  instance (Amazon RDS Multi-AZ)Update Auto Scaling Group Launch Configurations SetupRoute 53 DNSELB point toAuto Scaling Group containingBlue Launch Config (LC)Green Launch Config (LC)LCs  are point to Amazon DynamoDB, Amazon RDS Multi-AZ or Amazon ElastiCacheThere’s patterns for OpsWorks and Elastic Beanstalk, will add if I have time.Configuration Management and Infrastructure as CodeLambdaDeploying new versionsWhat triggers are availableAPI GatewayAWS IoTApplication Load BalancerCloudWatch Events (Monitoring and Logging)CloudWatch Logs (Monitoring and Logging)CodeCommit (SDLC automation)Cognito Sync TriggerDynamoDB (High Availability, Fault Tolerance, and Disaster Recovery)Kinesis (Incident and Event Response)Also doesn't hurt to know the following services are supported: S3,SNS and SQSMonitoring and LoggingCloudWatch events for the services covered in the examSDLC AutomationCodeCommitCodeBuildCodeDeployCodePipelineConfiguration Management and Infrastrcuture as CodeAWS ConfigAWS OpsWorksAWS (Lambda) Step FunctionsAWS ECSMonitoring and LoggingCloudWatch (scheduled events)Policies and Standards AutomationAmazon MacieAWS Systems ManagerConfiguration ComplianceMaintenance WindowsParameter StoreTrusted AdvisorIncident and Event ReportingAmazon GuardDutyFault Tolerance, High Availability and Disaster RecoveryAmazon EC2 Auto ScalingCloudWatch Event Rule TargetsSDLC AutomationCode BuildCode PipelineConfiguration Management and Infrastructure as CodeLambda (and Step) functionIncident and Events ReportingKinesisData StreamsData FirehoseAmazon InspectorPolicies and Standards AutomationSystems ManagerRun CommandAutomationNice to knows: SNS and SQSFault Tolerance, High Availability and Disaster RecoveryRDSsnapshots and their use in a DR situation. (High Availability, Fault Tolerance, and Disaster Recovery).Understanding Recovery Time Objective (RTO) and Recovery Point Objective (RPO) with DR in mind. (High Availability, Fault Tolerance, and Disaster Recovery).Policies and Standards AutomationAWS Systems Manager - EC2 patch groups and Patch Manager's baselines (Policies and Standards Automation)AWS Service Catalogue - how to offer products that provide different tiers (web, web + db) or stacks (.NET or Ruby) (Policies and Standards Automation)AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Databases",

        
        
        
        "tags": [
        
            "aws",
            
        
            "databases",
            
        
            "rds",
            
        
            "dynamodb"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-databases",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Amazon RDS is a managed service for Relational Database engines Service (RDS). AWS supports the following engines:MySQL is an open source database engineMariaDB is a fork of MySQL when the former was acquired by Oracle (through the acquisition of Sun Systems)PostgreSQL is an open source database engineOracle is a commercial engine by OracleSQL Server is a commercial engine by MicrosoftAmazon Aurora is a MySQL / Postgres compatible relational database engineAmazon DynamoDB is a proprietary NoSQL database service offered by Amazon.Why?Managed services for database engines like all managed service remove key concerns:provisioning/scaling/termination of servers that host the database enginesmaintenance of servers (patching)backups and restoresThese concerns often affect the ability to provide a database server that is fault-tolerant, highly available and has a contingency for disaster recovery.N.B. there's a caveat around storage scaling that it won't be applicable to SQL Server instances. The details can be found in the SQL Server FAQs: Why can’t I scale my storage?.To understand the difference between Amazon RDS and Amazon DynamoDB, I've provided the following examples:Relational database engines (sometimes referred to as RDBMS) are tabular in natural, you can think of it visually as a spreadsheet with fields as the column headers and the rows being a single record of data. The term &quot;relational&quot; comes from the ability to link tables through a foreign key, an example of this might be a list of developers and their favourite foods. The link would be a column in the developers table called food_id, which would reference the column called id in the food table.developer(s) tableidnamefood_id1alice12bob13carol2food(s) tableidname1banana2nutsIf you were to delete nuts (id: 2) from the food table you would trigger an error warning there were dependencies in the developer table.The key takeaway from this example is to remember Relational Databases store records tabularly in rows.NoSQL database engines are a bit of a catch-all, but in essence, if you don't store your data tabularly you're probably a NoSQL database engine. Amazon DynamoDB is a Key/Value pair and Document store. Key/Value store allows you to store data like a Hash (associative array/dictionary), you provide a key and the value is returned, you may have used one without knowing as they're often referred to as cache servers i.e. Redis. Document stores allow you to data in a structured way common formats are XML and JSON, often these are the database engines most people associate with NoSQL i.e. CouchDB and MongoDB.When?Amazon Aurora provides a compatible engine that is 3x faster than PostgreSQL and 5x faster than MySQL. In terms of cost-effectiveness, you need to compare the other RDS engines as a multi-AZ deployment and memory optimised instances.Things that set it apart from the other RDS offerings in terms of this domain are:Aurora can failover over to 1 of (up to) 15 read replicas with low impact to the primary instance.You can use MySQL replicas instead of Aurora native, but you're limited to 5 replicas and there's a high impact to the primary instance.The order which replicas are promoted to primary can be customised.Data is stored in 10GB chunks with 6 copies replicated across three availability zone.Aurora will continue to handle:write capability with the loss of 2 copies of dataread capability with the loss of 3 copies of dataThe data blocks and disks are scanned for errors and repaired automaticallyAmazon RDS can be part of your disaster recovery strategy by keeping replicas of your production Oracle or SQL Server database servers.Amazon DynamoDB requires a lot more consideration to make use of its features that make it relevant to this domain. Choosing the wrong scheme for partition keys can see your database starved of I/O.Things to consider:You pay for the number of I/O you use (rather than instance size) the size of units is in kilobytes and vary depending on the type of I/O operation:Read Capacity Unit (RCU) are measured in 4KB blocks, so an 8KB block of data would consume 2 RCUsWrite Capacity Unit (WCU) are measured in 1KB blocks, so a 5KB block of data would consume 5 WCUsData in tables are stored as 10GB partitions that can handle 3K RCU and 1K WCUAs you expand the table into more 10GB partitions, the RCU and WCU are distributed across the partitions i.e. if you 2 partitions then you have a max of 1.5K RCUs and 0.5K WCUs across the 2 partitions.TerminologyTable (highest level unit for Dynamo DB)Item (a record)Attributes (columns or fields of a record)Primary Keys can consist ofjust a Partition Key (which is often referred to as a Primary Index and is used to query the table)the partition key and sort key this is known as a composite primary keySecondary Indexes (allows you to query on a different attribute)Local - requires the same partition key, but the sort key can be differentGlobal - can have a different attribute for the partition and sort keysStrategies for write-heavy use cases it's recommended you add a randomly generated number from a predetermined range. e.g. if Partition Key is a composite attribute based on invoice number (1234), then you would suffice the randomly generated number (1) to the end, so the composite key would be: 1234-1.Further reading:AWS DynamoDB Cheat SheetAWS DynamoDB Partitions and Key DesignAWS DynamoDB: Choosing the right partition keyKey concepts of DynamoDBHow?Amazon Aurora requires you to choose your engine compatibility i.e. MySQL or PostgreSQL after which you define:Create DB Cluster Parameter Group (parameters to be applied to all instance in a DB cluster)Create DB Cluster (the group of instances associated with a DB cluster)Create the Database Instance (adds a new instance to a DB cluster)The AWS CLI features a cli walkthrough of how to provision a table, store an item, perform a query. It should be noted that as a rule of thumb you would probably use the [AWS SDK][aws_sdk] to store and retrieve data from DynamoDB.API and CLI features and verbsAmazon RDSFeaturesDB (Cluster) Parameter GroupDb ClusterDB InstanceDB (Cluster) SnapshotVerbs (CRUD)create/copydescribemodifydeleteOutliersNot my best work will see if I can optimise this list.add-option-to-option-groupadd-role-to-db-clusteradd-role-to-db-instanceadd-source-identifier-to-subscriptionadd-tags-to-resourceapply-pending-maintenance-actionauthorize-db-security-group-ingressbacktrack-db-clustercopy-option-groupcreate-db-cluster-endpointcreate-db-instance-read-replicacreate-db-security-groupcreate-db-subnet-groupcreate-event-subscriptioncreate-global-clustercreate-option-groupdelete-db-cluster-endpointdelete-db-instance-automated-backupdelete-db-parameter-groupdelete-db-security-groupdelete-db-snapshotdelete-db-subnet-groupdelete-event-subscriptiondelete-global-clusterdelete-option-groupdescribe-account-attributesdescribe-certificatesdescribe-db-cluster-backtracksdescribe-db-cluster-endpointsdescribe-db-cluster-parametersdescribe-db-cluster-snapshot-attributesdescribe-db-engine-versionsdescribe-db-instance-automated-backupsdescribe-db-log-filesdescribe-db-parameter-groupsdescribe-db-parametersdescribe-db-security-groupsdescribe-db-snapshot-attributesdescribe-db-subnet-groupsdescribe-engine-default-cluster-parametersdescribe-engine-default-parametersdescribe-event-categoriesdescribe-event-subscriptionsdescribe-eventsdescribe-global-clustersdescribe-option-group-optionsdescribe-option-groupsdescribe-orderable-db-instance-optionsdescribe-pending-maintenance-actionsdescribe-reserved-db-instancesdescribe-reserved-db-instances-offeringsdescribe-source-regionsdescribe-valid-db-instance-modificationsdownload-db-log-file-portionfailover-db-clustergenerate-db-auth-tokenlist-tags-for-resourcemodify-current-db-cluster-capacitymodify-db-cluster-endpointmodify-db-cluster-snapshot-attributemodify-db-snapshot-attributemodify-db-subnet-groupmodify-event-subscriptionmodify-global-clusterpromote-read-replicapromote-read-replica-db-clusterpurchase-reserved-db-instances-offeringreboot-db-instanceremove-from-global-clusterremove-option-from-option-groupremove-role-from-db-clusterremove-role-from-db-instanceremove-source-identifier-from-subscriptionremove-tags-from-resourcereset-db-cluster-parameter-groupreset-db-parameter-grouprestore-db-cluster-from-s3restore-db-cluster-from-snapshotrestore-db-cluster-to-point-in-timerestore-db-instance-from-db-snapshotrestore-db-instance-from-s3restore-db-instance-to-point-in-timerevoke-db-security-group-ingressstart-activity-streamstart-db-clusterstart-db-instancestop-activity-streamstop-db-clusterstop-db-instancewaitAmazon DynamoDBFeaturesItemBackup(Global) TableVerbs (CRUD)create (global table, table and backup)describe/list (global table, table and backup), get-item, batch-get-itemupdate (global table, table and backup), put-item, batch-put-itemdeleteOutliersdescribe-continuous-backupsdescribe-endpointsdescribe-global-table-settingsdescribe-limitsdescribe-time-to-livelist-tags-of-resourcequeryrestore-table-from-backuprestore-table-to-point-in-timescantag-resourcetransact-get-itemstransact-write-itemsuntag-resourceupdate-continuous-backupsupdate-global-table-settingsupdate-time-to-livewaitAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster RecoveryAmazon Single-Sign OnAmazon CloudFrontAuto ScalingAmazon Route53DatabasesAmazon RDSAmazon AuroraAmazon DynamoDB"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Amazon Single Signon, CloudFront, Autoscaling and Route53",

        
        
        
        "tags": [
        
            "aws",
            
        
            "route53",
            
        
            "sso",
            
        
            "cloudfront"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-sso-cloudfront-autoscaling-route53",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Amazon Single Sign-On is a managed single sign-on (SSO) service that you can use to simplify access to applications and 3rd party services. If SSO is not a term you're familiar with, if you've ever signed up for a service using your Google, Facebook or Twitter account (instead of using your email address and password specific to that site) then you've used SSO.Amazon CloudFront is a managed Content Delivery Network (CDN) service, you may have heard of CloudFront's competitors like CloudFlare, Akamai and Fastly. CDN speed up your website performance by strategically placing mirrors of popular content (static files, API or streaming audio/video) at locations nearer to the user accessing your website. These mirrors are referred to as Edge locations popular content for the region (not specific to client) is cached here. In more densely populated areas there are also Regional Caches that hold content for longer than Edge locations.Amazon Route53 is a managed Domain Name Service (DNS). At its very basic level of functionality DNS servers allow you to connect to servers using friendly domain names i.e. dev.to rather than IP addresses like 151.101.123.4, 151.101.12.34, 151.101.1.234. It's designed to work with other Amazon Web Services that is you can point DNS records directly to Elastic Load Balancer, S3 and EC2 instances.Autoscaling as we saw in the Domain intro comes in two varieties: AWS Auto Scaling and Amazon EC2 Auto Scaling. The general rule of thumb for use if you want to just autoscale EC2 instances then use EC2 Auto Scaling service, otherwise, AWS Auto Scaling is a better use case for when you want to scale multiple resources (not just EC2) i.e. DynamoDB tables and indexes, ECS tasks.An important thing to note that to use AWS Auto Scaling, your resource must be created using CloudFormation or Elastic Beanstalk.For the rest of this post, we'll only be referring to Amazon EC2 Auto Scaling.Why?Amazon Single Sign-On or generically any single sign-on (SSO) service is better than managing the administrative overhead of keeping separate logins for each application / service, you reduce the impact on day to day operations should disaster strike (think the number of helpdesk tickets will be raised for DR systems that rarely get used). You'll also get the undying love of your users because it means fewer logins to track, which in turn means they will be less likely to keep a scrap paper lying around their desk with the various logins and passwords written down.Amazon CloudFront distributes your content geographically rather than storing in a single location or S3 bucket. By careful design (falling back graceful should the backend be unavailable) ensures your website is highly available.Amazon Route 53 provides the following routing policies whose attributes are suitable for this domain:Failover routing - used for active-passive failover, a good use case for automated disaster recovery.Geolocation routing - used to route traffic based on the location of users, a good use case for highly availableGeoproximity routing - similar to Geolocation routing, but also allows you to route to a secondary location. This also makes a good use case for fault tolerance.Latency-based routing - used to route users to the resources with the best (least) latencyMultivalue answer routing - this is similar to round robin, in that you can randomly pick a route from up to eight healthy resourcesWeighted routing - routes traffic to different resources using a percentage split (useful for A/B testing or load balancing). Weights are between 0 to 255.Amazon EC2 Auto Scaling allows you to launch or terminate a number of EC2 instances by defining conditions when scaling out (increase) or scaling in (decreasing) the number of instances. The condition might be metrics like CPU or Memory utilisation, or health checks. This combined with an elastic load balancer provides a system that can be highly available and fault tolerant.Terminology to be aware of:Auto Scaling Group (ASG) - this is a group of EC2 instances associated with one or more scale in/out conditions.Minimum size - that the ASG never goes belowMaximum Size - that the ASG never goes aboveDesired capacity - that the ASG will already try to maintain (unless the condition requires further scaling-in).Scaling capacity is between desired capacity and Maximum SizeWhen?Amazon Single Sign-On should ideally be implemented as soon as possible, but it's still possible to retrofit into an existing environment. Doing this soon rather than later could mean you're not having to re-organise the team who are responsible for user and access management if the headcount reduces because of efficiency savings through the implementation of SSO.Amazon CloudFront should be implemented once you have some metrics (via Amazon X-Ray or something similar) to indicate you have customers in regions that are experiencing poor response times because of their proximity in relation to the region where your load balancers, EC2 instances or S3 buckets are hosted.Amazon Route 53's routing policy provides a lot of desirable features that are relevant for this domain. This combined with the fact that Amazon also offers an [SLA] of 100% availability and the ability to create and modify DNS record programmatically mean the use of Route 53 is a bit of a no-brainer.Amazon EC2 Auto Scaling has the concept of lifecycle hooks. These allow you to perform custom actions by pausing the instances as the ASG launches (EC2_INSTANCE_LAUNCHING) or terminates  (EC2_INSTANCE_TERMINATING) them. Whilst the instance is paused, it is in a wait state until you complete the action by issuing the complete-lifecycle-action action in the CLI/API or the timeout period ends (one hour by default). You can extend the timeout period by either:set a longer heartbeat timeout period using the put-lifecycle-hook action (CLI/API) when you create the lifecycle hookrestart the timeout period using record-lifecycle-action-heartbeat action (CLI/API)The maximum time you can place an instance in a wait state is 48 hours or 100 times the heartbeat timeout (whichever is smaller).How?Amazon Single Sign-On requires an AWS Organization to exist and then you can enable single sign-on via the AWS Console. The specifics for setting up the service with the AWS Account or Cloud Applications (3rd party services) can be found in the [guide][sso_guide]. There is an option to link to your existing Microsoft Active Directory, but if you don't need this option then the service will use its own directory.Amazon CloudFront to setup you define a distribution that determines the content origins (S3 bucket or HTTP server), access, security (TLS/SSL/HTTPS), session/object tracking, geo restrictions and logging. The provisioning of CloudFront can take a while as the content is being distributed to edge locations.I've found the following article in the AWS blog very helpful in terms of an application that I was already familiar with, but also knew the difficulty in optimising for response time: How to accelerate your WordPress site with Amazon CloudFrontAmazon EC2 Auto Scaling terminology:Auto-scaling group - this is a collection of EC2 instances that will be scaled in or out depending on conditions definedThere's a minimum sizeDesired capacityMax capacityAutoscaling lifecyclestarts when an ASG launches an instanceends, when you terminate an instance or ASG, takes an instance out of service and terminates itCooldowns prevents the ASG from launching or terminating more instances before the previous scaling activity event has taken effect. The default period is 300 seconds (5 minutes)API and CLI features and verbsAmazon Single Sign-OnThis service has no API/CLI.Amazon CloudFrontFeatures(Streaming) Distributions (this is probably the most important one to be aware of) with or without tagsField Level Encryption (Config/Profile)Invalidation (cache)(CloudFront) Origin Access IdentityPublic keyVerbs (CRUD)create (distrbution/streaming-with-tags)get/listupdate (except invalidation)delete (except invalidation)Outliersget-field-level-encryption-profile-configget-distribution-configget-public-key-configget-cloud-front-origin-access-identity-configget-streaming-distribution-configlist-distributions-by-web-acl-idlist-tags-for-resourcesigntag-resourceuntag-resourcewaitAmazon Route 53I've opted for the main API/CLI for Route 53 instead of Domains and Resolvers as I've been using this more on a day to day basis.FeaturesHealth CheckHosted ZoneReusable Delegation SetTraffic Policy (Instance/Version)Create Query Logging ConfigVerbs (CRUD)createget/listupdatedeleteOutliersCreateVPCAssociationAuthorizationAssociateVPCWithHostedZoneDeleteVPCAssociationAuthorizationChangeResourceRecordSetsChangeTagsForResourceDisassociateVPCFromHostedZoneGetAccountLimitGetChangeGetCheckerIpRangesGetGeoLocationListGeoLocationsTestDNSAnswerEC2 Auto ScalingFeaturesThe API has a lot of features, but the API actions I've focussed on have been around the Lifecycle Hooks.Verbs (CRUD)describe (types)putdeleteOutliersCompleteLifecycleActionRecordLifecycleActionHeartbeatAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster RecoveryAmazon Single-Sign OnAmazon CloudFrontAuto ScalingAmazon Route53DatabasesAmazon RDSAmazon AuroraAmazon DynamoDB"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: High Availability, Fault Tolerance and Disaster Recovery",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-high-availability-fault-tolerance-disaster-recover",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Determine appropriate use of multi-AZ versus multi-region architecturesDetermine how to implement high availability, scalability, and fault toleranceDetermine the right services based on business needs (e.g., RTO/RPO, cost)Determine how to design and automate disaster recovery strategiesEvaluate a deployment for points of failureThis domain is 16% of the overall mark for the exam.What whitepapers are relevant?According to the AWS Whitepapers page we should look at the following documents:Backup and Recovery Approaches Using AWS (June 2016)Building Fault-Tolerant Applications on AWS (October 2011)What services and products covered in this domain?AWS Single Sign-On is Amazon's managed SSO service allow your users to sign in to AWS and other connected services using your existing Microsoft Active Directory (AD).Amazon CloudFront is a managed Content Delivery Network (CDN) service.Autoscaling resources - Amazon has two offerings Amazon Autoscaling and Amazon EC2 Auto ScalingAmzon Route 53 is a managed Domain Name Service (DNS).DatabasesAmazon RDS is a managed relational database service with a large choice of engines: Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database and SQL Server.Amazon Aurora is part of the RDS offering but is unique in that it provides compatibility with MySQL and PostgreSQL engines whilst outperforming them considerably (5x for MySQL and 3x for PostgreSQL).Amazon DynamoDB is a managed NoSQL (non-relational) database service that can be used for storing key-value pairs or document based records.What about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages.Amazon Single-Sign OnAmazon CloudFrontAmazon Autoscaling and Amazon EC2 AutoscalingAmazon Route53DatabasesAmazon RDSAmazon AuroraAmazon DynamoDBAlternatively, get familiar with the services using the FAQs:Amazon Single-Sign OnAmazon CloudFrontAmazon Autoscaling and Amazon EC2 AutoscalingAmazon Route53DatabasesAmazon RDSAmazon AuroraAmazon DynamoDBYou're all expected to know the APIsAmazon CloudFrontAmazon Autoscaling and Amazon EC2 AutoscalingAmazon Route53DatabasesAmazon RDSAmazon Aurora uses the same API as RDSAmazon DynamoDBBefore you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsAmazon CloudFrontAmazon Autoscaling and Amazon EC2 AutoscalingAmazon Route53 has three subcommands: DNS and Healthchecking, Service Discovery and Domain RegistrationDatabasesAmazon RDSAmazon Aurora uses the same CLI as RDSAmazon DynamoDB has two sub commands: dynamodb and dynamodbstreamsAs with the API, there are patterns to the commands.High Availability, Fault Tolerance and Disaster Recovery, oh my!Let's the basics out of the way and discuss the core concepts around this domain.I'm going to use an excellent example provided by Patrick Benson in his blog post: The Difference Between Fault Tolerance, High Availability, &amp; Disaster RecoveryAn airplane has multiple engines and can operate with the loss of one or more engines. The design of the airplane has been made it resilient to falling out of the sky because of engine failure. This design is fault tolerant.In terms of infrastructure, this is likely to be a managed service like RDS, where under the hood the database engine has multiple disks and CPUs to cope with catastrophic failure.Whereas spare tire in car, isn't fault tolerant i.e. you have to stop change the tire, but having the spare tire in the first place makes the car still highly available. In terms of infrastructure is any type of technology like an autoscaling group.It's very common for a solution to implement a system that is fault tolerant (resilience) and highly available (scalable).Finally, ejector seats in Fighter aircraft are disaster recovery (DR) measure. The goal is to preserve the pilot, or in our case, the service after all other measures have failed (Fault Tolerance and HA).Often in terms of infrastructure, this might be a standby infrastructure or database replica in a different AWS region and using Route 53 to point to the stand by infrastructure. Whilst it's still common for DR strategies to be manual, for this domain we'll be expected to provide an automated solution.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster RecoveryAmazon Single-Sign OnAmazon CloudFrontAuto ScalingAmazon Route53DatabasesAmazon RDSAmazon AuroraAmazon DynamoDB"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Incident and Event Response",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-incident-and-event-response",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Troubleshoot issues and determine how to restore operationsDetermine how to automate event management and alertingApply concepts required to implement automated healingApply concepts required to set up event-driven automated actionsThis domain is 18% of the overall mark for the exam.What whitepapers are relevant?According to the AWS Whitepapers for Security and Compliance we should look at the following documents:AWS Security Incident Response (May 2019)AWS Security Best Practices  (August 2016)Other whitepapers as recommended by product pages:Real-Time Insights on AWS Account ActivityReal-Time Web Analytics with Kinesis Data AnalyticsStreaming Data Solutions on AWS with Amazon Kinesis (July 2017)What services and products covered in this domain?Amazon GuardDuty - thread detectionAmazon Inspector - best practice in securityAmazon Kinesis has separate pages for each sub-productKinesis Video Streams allows you to securely stream video from connected devices for analytics, machine learning and other forms of processing.Kinesis Data Streams is scalable and durable real-time data stream serviceKinesis Data Firehose allows you to capture, transform and load data streams into AWS data stores (S3, ) whilst allowing you to perform near real-time analytics with existing Business Intelligence toolsKinesis Data Analytics is the next tier up when you've outgrown CloudWatch for analytics. Allowing you to process data streams in real time using SQL or Java.What about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages.Amazon GuardDutyAmazon InspectorAmazon KinesisKinesis Video StreamsKinesis Data StreamsKinesis Data FirehoseKinesis Data AnalyticsAlternatively, get familiar with the services using the FAQs:Amazon GuardDutyAmazon InspectorAmazon KinesisKinesis Video StreamsKinesis Data StreamsKinesis Data FirehoseKinesis Data AnalyticsYou're all expected to know the APIsAmazon GuardDutyAmazon InspectorAmazon KinesisKinesis Video StreamsKinesis Data StreamsKinesis Data FirehoseKinesis Data AnalyticsBefore you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsAmazon GuardDutyAmazon InspectorAmazon KinesisKinesis Video Streams has 3 different subcommands: kinesis-video-archived-media, kinesis-video-media, kinesisvideoKinesis Data Streams has no CLI subcommandsKinesis Data Firehose has no CLI subcommandsKinesis Data AnalyticsAs with the API, there are patterns to the commands.Please put down your weapon. You have 20 seconds to comply.Quote: ED-209, Robocop (1987)This is another domain, that probably requires awareness of the products rather than actual use (especially in the case of the Kinesis suite). As we'll see Amazon GuardDuty and Amazon Inspector do have an immediate use for small to medium-sized business if you don't have an Information Security function in your company.As the title of this section suggests, this is domain focussed on how to responding to incidents and events. Incidents are usually around a security context. whereas events are often related to a data stream.I'll be covering all the products in this domain within this post since it's a relatively small domain with very little opportunities to provide practical hands-on experience using the AWS CLI.What?Amazon GuardDuty is a threat detection service that analyses meta-data generated by your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Log, and DNS Logs. It can detect:Reconnaissance - port scanning, login failures, unusual API activity and access from a known bad IP.Instance compromise - malware, outbound denial of service activity, unusually high volumes of traffic, activity from a known bad IP, data exfiltration via DNSAccount compromise - attempt to disable AWS CloudTrail logging, unusual instance or infrastructure launches (think: GPU instance types for cryptocurrency mining), resources being spun up in regions outside of normal use, activity from a known bad IP.Data retention for a GuardDuty-generated finding is 90 days.Amazon Inspector is another automated security assessment service just like GuardDuty, but instead of identifying issues with your account or network activity it primarily focusses on your EC2 instances. It has built-in rules (created and maintained by AWS security researchers) to check for access to your instances from the internet, remote root login being enabled, or vulnerable software versions installed.Amazon Kinesis is a suite of products:Kinesis Video Streams accepts video streams from devices and allow you to perform analytics, machine learning (Amazon Rekognition Video and Amazon SageMaker) and other forms of processing.Kinesis Data Streams is a scalable and durable real-time data streaming service that can capture high volumes of data (gigabytes of data per second) from hundreds of thousands of sources. You can then take this data and process it using Kinesis Data Analytics, Spark or code running on an EC2 instance (API) or AWS Lambda.Kinesis Data Firehose allows you to take your high volumes of streaming data and load them into data stores (S3, Amazon Redshift, Amazon Elasticsearch and a 3rd party tool called Splunk) for analytics.Kinesis Data Analytics can take data streams from Kinesis Data Firehose and Kinesis Data Streams and perform queries against the data using SQL or Java.Why?Amazon GuardDuty provides a comprehensive threat detection service, whilst it does offer reporting through a dashboard, the response workflow does require more work i.e. you're expected to write the tooling (AWS Lambda) around the events (CloudWatch). This is probably why Amazon is touting partners who can help implement these workflows.Amazon Inspector as with GuardDuty, you get an all-around security assessment tool for your EC2 instances.The Amazon Kinesis suite provides managed services that can handle large amounts of data. Whilst you run your own Apache Kafka environment, you would need a large team to support it setup and on-going maintenance. Trivia: Amazon also provides a managed service for Apache Kafka called Amazon Managed Streaming for Apache Kafka. The key take away for why Kinesis exists is that it has specialist tooling for Video and High Volume Data streams. Because of tight integration into AWS data services Kinesis Data FireHose can load large volumes of data into their services with a minimal amount of configuration. Kinesis Data Analytics can be used to perform analytics against these data streams or when you've outgrown the likes of CloudWatch for log analysis. Finally, all of these products in this suite can be used together or separately, although Kinesis Data Analytics does require the source to come from Kinesis Data FireHouse or Kinesis Data StreamsWhen?Amazon GuardDuty should be used when you don't want the overhead of managing your own threat detection and response system. To do this yourself you would need to use tools like Snort or Tripwire and maintain subscriptions to 3rd party threat intelligence sources like Proofpoint and CrowdStrike.Amazon Inspector is a complementary tool to Amazon GuardDuty, so the chances are if you're using it (Amazon GuardDuty), you should be using Amazon Inspector.Some of the use cases for Amazon Kinesis are:Capturing live video streams of cars, identifying the license plates and then taking action.Transform batch loading (and subsequent overnight reporting), into the real-time analysis.Collecting telemetry data from a fleet Internet of Things devices and reacting to the data using other AWS services like AWS Lambda.How?Amazon GuardDuty provides a 30-day free trial, after that you charged by the volume of events (CloudTrail) and volume of data (Amazon VPC Flow Log and DNS log). The key things (also that form the navigation menu) you need to be aware of in the service page in the AWS Console are:Findings are potential security issues discovered by GuardDuty.Settings are where:permissions for the service are definedSet the interval that GuardDuty sends events to CloudWatchPreload GuardDuty with sample findingsSuspend or disable GuardDutyLists are the list management page where you can upload a list of Trusted IPs or Threat Lists (malicious IP addresses). You can upload have a max of 6 Threat Lists per AWS account per region.Accounts - to manage Master (admin) and Member (users) Accounts within the serviceWhat's New - features announcement (you can also subscribe via SNS topic)Free trial - how much time you have leftAmazon Inspector provides a 90 day free trial for the first 250 instance assessments. The key things to be aware of are:Agents - can analyse your EC2 instances without the use of it's agent, but you will be only limited to Network Reachability rules.Assessment target is a collection of EC2 instances group together using AWS tags.Assessment templateFindingRule packages there are two categories: Network and Host assessments. Important: you can only run the Network assessment rule if you don't have the agent installed.Rules within these categories areNetwork assessmentNetwork ReachabilityHost assessmentCommon Vulnerability and ExposuresCentre for Internet Security (CIS) BenchmarksSecurity Best Practices for Amazon InspectorRuntime Behaviour AnalysisTelemetry is data (behaviour, configuration, etc) collected by Amazon Inspector from the EC2 instance when an assessment is run.Amazon Kinesis does require development to utilise the services. The examples provided require quite a bit of setup. It would be nice if you could pre-load the services with sample data much as you can with Amazon GuardDuty or Amazon Inspector.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseAmazon GuardDutyAmazon InspectorAmazon KinesisKinesis Video StreamsKinesis Data StreamsKinesis Data FirehoseKinesis Data AnalyticsDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Policy and Standards Automation",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-policy-standards-automation",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Apply concepts required to enforce standards for logging, metrics, monitoring, testing, and securityDetermine how to optimize cost through automationApply concepts required to implement governance strategiesWhat services and products covered in this domain?AWS Service CatalogAWS Trusted AdvisorAWS Systems ManagerAWS Organizations Product PageAWS Secrets Manager Product PageAWS Macie Product PageAWS Certificate Manager Product PageWhat about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages. Alternatively, get familiar with the services using the FAQs:AWS Service Catalog FAQAWS Trusted Advisor FAQAWS Systems Manager Guide/FAQAWS Organizations Guide/FAQAWS Secrets Manager Guide/FAQAWS Macie Guide/FAQAWS Certificate Manager Guide/FAQYou're all expected to know the APIsAWS Service Catalog APIAWS Trusted AdvisorAWS Systems Manager APIAWS Organizations APIAWS Secrets Manager APIAWS Macie APIBefore you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsAWS Systems Manager CLIAWS Organizations CLIAWS Secrets Manager CLIAWS Macie CLIAs with the API, there are patterns to the commands.10 per centI've decided to keep all of the services around this domain within this intro, you're expected to be aware of the services, their purposes and uses cases rather than have in-depth knowledge around their implementation.Whilst this domain is only 10 per cent, it probably doesn't hurt to know of these services it might make the difference between a pass and fail grade when you take the exam.What?AWS Service Catalog...allows you to build a catalogue of products (of AWS resources) that your users can use.through the use of the API, you can provision these services through your own front end instead of AWS consoleuses the following terminologyProducts - are application stacks based around AWS resources created by adminsPortfolios - are a collection of productsUsers - can launch these products without needed access to AWS or the consoleAWS Trusted Advisor is a service that provides a dashboard to report if AWS resources are not in line with current best practice. The reporting is categorised as follow:Cost Optimisation - will report on any low or underutilised resources to save you moneyPerformance - will report any aspect of configuration that has an impact on resource performanceSecurity -  will report any concerns around security i.e. MFA not enabled on the root accountFault Tolerance - will report on any resources that are not fault tolerantService Limits - will report on resources that are close to reaching the service limits What reports you to get depend on the support plan you're on.Basic/Developer - get seven core checks (six from Security and all of the Service Limit reports)Business/Enterprise - get all the reporting and checksAWS Systems Manager is an infrastructure management service that can perform the following tasks:Inventory - allows you to collect information about your instances and the software installed on them.Configuration compliance - scans your instance for patch compliance and configuration inconsistenciesAutomation - automates daily or repetitive tasks or reportsRun Command - allows you to perform commands against a single or group of resourcesSession Manager simplifies connections to resources without needing to open inbound ports, setup bastion servers or manage SSH keysPatch Manager - allows you to deploy patches and updates to your instances either immediately or via a maintenance windowMaintenance Windows - allows you to schedule pre-arranged outagesDistributor is a software package distribution mechanismState manager to store policies that can be reapplied to resources when there is a drift in configurationParameter Store (secrets and configuration data)AWS Organizations allows you to organise your environment based on your own hierarchy or functional roles (security, compliance, operations, developer and finance).AWS Secrets Manager encrypts secrets at rest using keys in KMS. Secrets can be database credentials, passwords and 3rd party API keys. You can store and control access through Console service, CLI, API and SDK.Secrets can be rotated automatically via a scheduleSecret types you can store are:Credentials for RDS databasesCredentials for other databasesOther types of secrets (API keys, passwords, etc)Amazon Macie uses machine learning to automatically identify, classify and protect sensitive data in AWS. It recognises personally identifiable information (PII) or intellectual property. It currently protects S3, but data store related services are being planned for the future.AWS Certificate Manager centralised managed certificates in AWS. It has good integration with AWS services allowing it to provision TLS/SSL certificates on their behalf. You can also use it to set up a private certificate authority which can be internally within an organisation when there's no need to use trusted Internet Root CAs.Why?AWS Service Catalog allows you to hide the specific implementation of your cloud infrastructure. By giving your users a separate front end, you can remove a direct association to an AWS account.AWS Trusted Advisor is an automated tool to gather information about your AWS environment and report against those not configured as per best practice. It's also a cost-saving tool, making recommendations to remove unused resources or downgrade them. A full list of checks can be found on the AWS Support page.AWS Systems Manager is a tool to centralised various tasks and activities you may wish to perform against your infrastructure. The CloudWatch Dashboards and Trusted Advisor reports are also integrated to make this a one-stop shop for systems management.AWS Organizations allows you to map access to AWS resources based on your own business functions. The most simple example is to only allow developers access to the development environment and maybe read-only access to production.Amazon Macie uses machine learning to identify personallyWhen?AWS Service Catalog should be used when you want your users to have a specific set of products based around AWS services, but don't want to give them access to the AWS Console or a specific IAM account.AWS Trusted Advisor should be used when you need to find out if you're following best practice.AWS Systems Manager is an enterprise tool, allow you to identify resources by attribute (AMI image id, OS type, instance type) rather than navigate to specific resources.AWS Organizations when you need to map your business functions against AWS resources and IAM users, groups and roles are not sufficient enough controls.Amazon Macie should be used by organisations that require careful handling of customer data i.e. health care or government. Where the volumes of data are not easily managed by a data administration team or monitored by security/compliance business function.AWS Secrets Managers when you don't want to hard code secrets or store sensitive data on servers or have a regulatory requirement cycle secrets on a regular basis.AWS Certificate Manager when you need to provision SSL/TLS certificates or need to a private certificate authority.How?If there are no specific instructions, it's assumed the service is available through the AWS Console.AWS Service Catalog requires development so the Developer Guide should be consulted.AWS Trusted Advisor is available through the AWS Console. In addition to the dashboard, you can send out weekly reports to receipts based on role: billing, operations and security.AWS Systems Manager is available through the AWS Console, you can create groups to manage by searching for resources based on tags (resource type, operating system, etc). It will require an agent to be installed on the resource (as well as an IAM instance profile) to become a managed instance.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationAWS Service CatalogAWS Trusted AdvisorAWS Systems ManagerAWS OrganizationsAWS Secrets ManagerAmazon MacieAWS Certificate ManagerDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: AWS X-Ray",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "xray"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-xray",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?AWS X-Ray is an application performance monitoring (APM) tool similar to New Relic, Solarwind TraceView or Microsoft Applications Insights. APM tools go beyond the usual metric level of capturing, instead of performing traces which are data rich with information about application requests, the response time of blocks of code and latency. In short, they can often give you a context into what the application was done with at a request at a given time.In addition to full data capture around an application, X-Ray can also build a service map that shows the various components that your application interacts with. The example application in the Getting Started guide shows the interactions between the Java app, DynamoDB, SNS and external API.The following AWS service (that are in scope for this exam) that provide integration with X-Ray are:AWS LambdaELBAWS Elastic BeanstalkThe full list of services integrated with X-Ray can be found in the Developer GuideAdditional resources:AWS X-Ray Product PageAWS X-Ray FAQAWS X-Ray GuideAWS X-Ray APIAWS X-Ray CLIWhy?Previously debugging of application could be done in a post-mortem fashion analysing dumps, stack traces. This was often done offline. With the advent of APMs, you can now look at your application's health whilst in-flight.When?X-Ray only makes sense in a Production environment as it's used as a troubleshooting tool. There might be some value in using in other environments as a learning aid.How?There's an excellent tutorial that is part of the Getting Started guide. It's highly recommended to get familiar with the feature set of X-Ray.As always, remember you may incur a charge for the resources started using this guide.API and CLI features and verbsFeatures*GroupsSampling RulesVerbs (CRUD)create group/sampling-ruleget group(s)/sampling-ruleupdate group/sampling-ruledelete groupsampling-ruleOutliersbatch-get-tracesget-encryption-configget-sampling-statistic-summariesget-sampling-targetsget-service-graphget-time-series-service-statisticsget-trace-graphget-trace-summariesput-encryption-configput-telemetry-recordsput-trace-segmentsRemember there's specific APIs for Java, .Net (and Core), Ruby, Node and Python SDKs.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingCloudWatchAWS X-RayDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: CloudWatch",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "cloudwatch"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-cloudwatch",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?CloudWatch...is a service that allows you to collect, track and analyse metrics AWS resources and applications.consists of:Dashboards - this is a customisable web page that can display graphs, metrics, text and query results. DevOps engineers would use a dashboard to provide an at a glance view of the system/environment's health.Metrics allow you to visualise your metrics and create alarms.The following services (in scope for the exam) publish metrics to CloudWatch. A full list can be found in the User Guide.Amazon CloudFrontCloudWatch Events and LogsAWS CodeBuildAWS EC2 / Auto ScalingAWS ECSAWS / EFSELBAWS KinesisAWS Lambda / Step FunctionsAWS OpsWorksYou can publish custom metrics from EC2 instances via the AWS CLI/API or applications by using the AWS SDK.Alarms are generated off values from your metrics or a maths expression based around that value.Common use cases for alarms are:CPU/Memory utilisationLoad Balancer LatencyStorage ThroughputBilling (exceeding a spending threshold)Alarms can even be used to stop, terminate, reboot or recover an instanceThere are three states for an Alarm:OK - The metric or expression is within a defined thresholdALARM - The metric or expression is outside the defined thresholdINSUFFICIENT DATA - The alarm has just started, but the metric is not available or there is not enough data for the metric to define its alarm stateAlarms only invoke actions for sustained state changes. So you won't keep generating alarms because you're in a state ALARM.Logs are aggregated by Log Groups, to drill down to the actual logs you need to pick a log stream.Insights allow you to run SQL like queries over your log data. An example query might be to view the most expensive request for your Lambda function.Events are a near real-time stream of system events around changes to your AWS resourcesRules match incoming events and route them to the targetTargets something that will process the events (SNS topic, Lambda)Metric retention periodsHigh res (&lt;60 seconds) is 3 hours. High resolution has to be activated and incurs an additional charge.Data points of 60 secs (1 min) is 15 daysData points of 300 secs (5 mins) is for 63 daysData points of 3600 seconds (1 hour) is for 455 (15 months)Additional resources:CloudWatch Product PageCloudWatch FAQCloudWatch User GuideCloudWatch APICloudWatch CLIWhy?CloudWatch enables you to aggregate your logging and metrics into a centralised location. You can then perform analysis or visualise the data. Furthermore, you can use Alarms and Events to notify your team or perform automatic reactive action.When?You should use the CloudWatch Agent to gather metrics and logs from your EC2 instances or on-premises servers (Linux or Windows based).You should use the AWS CLI if you only need to publish custom metrics from your EC2 instance or on-premises servers.How?TODO: Unfortunately I've found these sections have been eating up a lot of my study time. I will try and revisit this during my revision stage.Here's what I had planned for this section:How to install the CloudWatch Agent log to capture logs from an EC2 instance, this would be based around this guide.How to publish a custom metric and generate an alarm using the AWS CLI, this would be based around this guide.API and CLI features and verbsI'd hazard the guess the one API/CLI call you need to be familiar with is put-metric-data CLIAPI because it's got an immediate and practical use and isn't possible through the AWS Console.FeaturesAlarmsDashboardsMetricsVerbs (CRUD)??? oddly creation verbs are via Putsdescribe (alarms)/get (dashboard/metric-data)/list (dashboard/metrics)put (dashboard/metric-[alarm/data])delete (alarms/dashboards)Outliersdescribe-alarm-historydescribe-alarms-for-metricdisable-alarm-actionsenable-alarm-actionsget-metric-statisticsget-metric-widget-imagelist-tags-for-resourceset-alarm-statetag-resourceuntag-resourcewaitAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingCloudWatchAWS X-RayDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Monitoring and Logging",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-monitoring-and-logging",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Determine how to set up the aggregation, storage, and analysis of logs and metricsApply concepts required to automate monitoring and event management of an environmentApply concepts required to audit, log, and monitor operating systems, infrastructures, and applicationsDetermine how to implement tagging and other metadata strategiesWhat whitepapers are relevant?According to the AWS Whitepapers for DevOps we should look at the following documents:Introduction to DevOps on AWS (20 pages)What services and products covered in this domain?CloudWatch - Complete visibility of your cloud resources and applications. Chances are if you provision a resource (well at least through the Console), it's probably logging in CloudWatch.AWS X-Ray - Analyze and debug production, distributed applications. Similar products are Rollbar, Sentry, Azure Application Insights.Source: AWS DevOps - Monitoring and Logging pageWhat about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages. Alternatively, get familiar with the services using the FAQs:CloudWatchAWS X-RayYou're all expected to know the APIsCloudWatchAWS X-Ray. There's specific APIs for the Java, .Net (and Core), Ruby, Node and Python SDKs.Before you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsCloudWatch. This is for the command that is part of the AWS CLI. The standalone CloudWatch CLI has been deprecated.There's no CLI for X-rayAs with the API, there are patterns to the commands.Monitoring and Logging (yawn)Monitoring and logging are what DevOps engineers do on a daily basis.  We use the data from our logs to spot patterns, and the monitoring alerts to react to issues and mitigate outages.It's not the most exciting topic to cover, it's a necessity for the role. I hope in future versions of the exam, this domain extends to Observability.This domain contains the fewest services, but make no mistake CloudWatch integrates with so many services it makes sense to have a good understanding of the features it provides. AWS X-Ray if you've used similar products, should be an easier concept to grasp (famous last words).AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingCloudWatchAWS X-RayDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: AWS Config and Managed Services",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-config-managed-services",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?I've decided to cover AWS Config and AWS Managed services together since it didn't seem to have two very short blog posts.AWS Config is a service that allows you to assess, audit and evaluate the configuration of your AWS resources. At a very basic level, this allows you to audit and track changes made to your environment.AWS Managed Service is a service where AWS managed and operate your infrastructure on AWS. That's right, AWS will run your infrastructure (with some caveats to discussed in the next section).Additional resources:AWS Config Product PageAWS Config FAQAWS Config Developer GuideAWS Config APIAWS Config CLIAWS Managed Services Product PageAWS Managed Services FAQWhy?AWS Config strong leans towards auditing, change control and compliance. There are some use cases in the next section, but this would help AWS users who are heavily regulated like financial organisations.AWS Managed Service is generally aimed at large organisations (enterprises) that want a safer way to migrate from on-prem to cloud-based services. Whilst the service itself doesn't handle migration (this is handled by an AWS partner), you've probably had seen this scenario before:You work for an enterprise organisation, and the powers that be want you to move to the cloud. They don't want to pay for an expert, so they expect you to learn on the job.Months later, when the service goes up in smoke (because using one VPC and making it visible to the Internet seemed like a quick way to get things working). The powers that be finally hire the expert to design the cloud architecture from the ground up. Who then proceeds to berates you for not knowing your EC2s from your TF2s.The key fact to take away from that vent (sorry) is that AWS will deliver an infrastructure that is based on their own best practice. You can concentrate on getting the continuous delivery pipeline working and making the necessary changes to your applications to get them working on AWS.Eventually, you can take over this infrastructure, but this can be done when the team feels confident enough to do so.When?Some of the suggested (by AWS) uses for AWS Config are:Discovery (of resources)Change ManagementContinuous audit and complianceCompliance as codeTroubleshootingSecurity AnalysisAWS Managed Service is aimed at enterprises want to migrate to Cloud Services, but do not have to experience to do this themselves.How?There are no practical sections for these two services.Reminder: AWS Config will cost money if you decide to play around with it as it's not covered by the free tier.API and CLI features and verbsThere's no section for AWS Managed Services because there's no API/CLI.Whilst there is an API/CLI for AWS Config, the common consensus is that you just need to be aware of the service. If you have come across a question during the exam that did rely on knowing the API please let me know.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: AWS ECS",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "ecs"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-ecs",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?AWS ECS ...is Amazon's Docker managed service. Naturally, you also get a container registry service in the form of Elastic Container Registry (ECR).comes in two varieties (launch types): EC2 or Fargate.EC2 based clusters as the name implies uses EC2 instances as Docker hosts for containers.You pay for the resources spun up i.e. EC2 instances and/or EBS volumes.You're also responsible for the upkeep of the EC2 instances (patching, monitoring and making secure).Fargate abstracts away EC2 instances (much like serverless functions).You pay for the vCPU (virtual CPU) and memory resources the container uses. This is charged at per second rate (with a minimum charge of a minute).Fargate is only available to few regions (13 at the time of writing this)There's no EC2 instances to maintain, just your tasks.is not a Kubernetes managed services, this is a separate offering with the catchy name of Elastic Container Service for Kubernetes Service (EKS)Additional resources:AWS ECS User GuideAWS ECS FAQAWS ECS APIAWS ECS CLIWhy?As with all managed services, you want to focus on the functionality rather than the upkeep of a service.With Docker becoming the lingua franca of the Cloud you can utilise 3rd party images and build solutions in a build block manner. Granted, Amazon has been using this modularized concept for many years (OpsWorks and Elastic Beanstalk) before Docker became mainstream.When?Microservices and batch jobs are good workloads for a cluster.You want to migrate away from Docker managed through on-premises infrastructure or EC2 instances that are not ECS managed.How?For this section, we're going to use Fargate rather than an EC2 based ECS cluster. This is mostly to reduce the additional complexity of provisioning the EC2 instances that will join the ECS cluster. This guide is loosely based around the Fargte tutorial in the Developer Guide. I've just removed the part about setting up a service and ran a task with public IP we could visit to test.That said, for day to day work you may find yourself still using EC2 based cluster until Fargate is available across all regions. You may also still find yourself using EC2 based clusters from a cost-saving perspective. If anyone has a calculator they've created when it makes more sense to go with the Fargate launch-type over EC2, please share in the comments or @ me on Twitter!Pre-requisitesA VPC with at least one subnet that has inbound and outbound access to the internet.A suitable security group as per the previous pointexport ECS_CLUSTER_NAME=your-cluster-nameexport ECS_SECURITY_GROUP=your-security-groupexport ECS_SUBNETS=yoursubnet# create a clusteraws ecs create-cluster --cluster-name $ECS_CLUSTER_NAME# register a taskcat &lt;&lt;EOF &gt; fargate-task.json{  &quot;family&quot;: &quot;sample-fargate&quot;,  &quot;networkMode&quot;: &quot;awsvpc&quot;,  &quot;containerDefinitions&quot;: [    {      &quot;name&quot;: &quot;fargate-app&quot;,      &quot;image&quot;: &quot;httpd:2.4&quot;,      &quot;portMappings&quot;: [        {          &quot;containerPort&quot;: 80,          &quot;hostPort&quot;: 80,          &quot;protocol&quot;: &quot;tcp&quot;        }      ],      &quot;essential&quot;: true,      &quot;entryPoint&quot;: [&quot;sh&quot;, &quot;-c&quot;],      &quot;command&quot;: [        &quot;/bin/sh -c \\&quot;echo &#39;&lt;html&gt; &lt;head&gt; &lt;title&gt;Hello dev.to-ers&lt;/title&gt; &lt;style&gt;body {margin-top: 40px; background-color: #333;} &lt;/style&gt; &lt;/head&gt;&lt;body&gt; &lt;div style=color:white;text-align:center&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&#39; &gt;  /usr/local/apache2/htdocs/index.html &amp;&amp; httpd-foreground\\&quot;&quot;      ]    }  ],  &quot;requiresCompatibilities&quot;: [&quot;FARGATE&quot;],  &quot;cpu&quot;: &quot;256&quot;,  &quot;memory&quot;: &quot;512&quot;}EOFaws ecs register-task-definition \\  --cli-input-json file://fargate-task.json# create a task, assign it to our network and enable the public IPaws ecs run-task \\  --cluster $ECS_CLUSTER_NAME \\  --task-definition sample-fargate:2 \\  --count 1 \\  --launch-type &quot;FARGATE&quot; \\  --network-configuration &quot;awsvpcConfiguration={subnets=[$ECS_SUBNETS],securityGroups=[$ECS_SECURITY_GROUP],assignPublicIp=&quot;ENABLED&quot;}&quot;# list tasksaws ecs list-tasks --cluster $ECS_CLUSTER_NAME# describe the taskexport ECS_TASK_ID=$(aws ecs list-tasks --cluster $ECS_CLUSTER_NAME --query &quot;taskArns&quot; --output text)aws ecs describe-tasks --cluster $ECS_CLUSTER_NAME --tasks $ECS_TASK_ID# get the public IP of the task (well, the one bound to the ENI)export ECS_TASK_NETWORK_ID=$(aws ecs describe-tasks --cluster $ECS_CLUSTER_NAME --tasks $ECS_TASK_ID --query &#39;tasks[*].attachments[*].details[?name==`networkInterfaceId`].value&#39; --output text)export ECS_TASK_PUBLIC_IP=$(aws ec2 describe-network-interfaces --network-interface-ids $ECS_TASK_NETWORK_ID --query &quot;NetworkInterfaces[*].PrivateIpAddresses[*].Association.PublicIp&quot; --output text)# testcurl $ECS_TASK_PUBLIC_IP# tear downaws ecs stop-task --task $ECS_TASK_ID --cluster $ECS_CLUSTER_NAMEaws ecs delete-cluster --cluster $ECS_CLUSTER_NAMEAPI and CLI features and verbs### FeaturesClustersServicesTask Sets### Verbs (CRUD)createdescribe/list (cluster/services)update (service/task-set)delete### Outliersdelete-account-settingdelete-attributesdeployderegister-container-instancederegister-task-definitiondescribe-container-instancesdescribe-task-definitiondescribe-tasksdiscover-poll-endpointlist-account-settingslist-attributeslist-container-instanceslist-tags-for-resourcelist-task-definition-familieslist-task-definitionslist-tasksput-account-settingput-account-setting-defaultput-attributesregister-container-instanceregister-task-definitionrun-taskstart-taskstop-tasksubmit-container-state-changesubmit-task-state-changetag-resourceuntag-resourceupdate-container-agentupdate-container-instances-stateupdate-service-primary-task-setwaitAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: AWS Lambda",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "lambda"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-aws-lambda",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?AWS Lambda is ...a Serverless function frameworkhighly integrated with AWS servicesa good fit for DevOps tasksAdditional resources:AWS Lambda User GuideAWS Lambda FAQAWS Lambda APIAWS Lambda CLIWhy?Here are some ideas that a DevOps / Infra team might use cases for AWS Lambda. None of this is new or groundbreaking innovations. The only difference is that when trying to implement these in AWS Lambda we no longer need to factor new servers, billing is per second and Lambda was built to talk with other AWS services in mind.Automate backups/cycle through EBS snapshotsGenerating reports - use it to audit resources on AWS (if you don't want to shell out on AWS Config)Perform S3 ops i.e. moving code build artefacts to a location on a given S3 bucketBatch log processing - extract, transform and load (ETL) from various resources and consolidate into a central data warehouseScheduled Tasks - perform any of the above use cases at a given schedule, just like cron jobsChatOps - running Slack chatbots to manage and report against your infrastructureUse cases inspired by the epagon blog post: Why DevOps Engineers Love AWSWhen?You need to perform a DevOps task, but don't want to go through the trouble of provisioning an application server to host it.You're trying to break up a monolithic management server that's responsible for scheduling and running DevOps tasks.How?We're going to use a simple example where the DevOps engineer wants to log all files being uploaded for a given S3 bucket. A lot of the hard work around this tutorial was done via Sunil Dalal's blog post,  &quot;Using Lambda as S3 events processor&quot;. Thanks, Sunil for sharing!Pre-requisitesCreate a Lambda execution role to grant lambda access to services and resources. This can be done through the console using this guide. Copy the ARN you'll need it when we upload the function. The role should have the following permissions applied:allow the role to create CloudWatch log entries.allow the role S3 read-only accessYou've got an S3 bucket (might be a good idea to create one just for this walkthrough)Have the following tools installed:jq for wrangling JSON data out of AWS CLIawslogs if you want to view CloudWatch log entries in the terminal)The AWS CLI default profile has been configuredto output JSONhas a fixed regionCreate filesCopy the following snippet and call it index.jsexports.handler = async (event) =&gt; {    var srcBucket = event.Records[0].s3.bucket.name;    var srcKey = decodeURIComponent(event.Records[0].s3.object.key);    console.log(&quot;bucket:&quot;, srcBucket, &quot; file: &quot;, srcKey);};Copy the following snippet and call it payload-test.json{  &quot;Records&quot;:[      {        &quot;eventVersion&quot;:&quot;2.0&quot;,      &quot;eventSource&quot;:&quot;aws:s3&quot;,      &quot;awsRegion&quot;:&quot;us-west-2&quot;,      &quot;eventTime&quot;:&quot;1970-01-01T00:00:00.000Z&quot;,      &quot;eventName&quot;:&quot;ObjectCreated:Put&quot;,      &quot;userIdentity&quot;:{          &quot;principalId&quot;:&quot;AIDAJDPLRKLG7UEXAMPLE&quot;      },      &quot;requestParameters&quot;:{          &quot;sourceIPAddress&quot;:&quot;127.0.0.1&quot;      },      &quot;responseElements&quot;:{          &quot;x-amz-request-id&quot;:&quot;C3D13FE58DE4C810&quot;,        &quot;x-amz-id-2&quot;:&quot;FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/JRWeUWerMUE5JgHvANOjpD&quot;      },      &quot;s3&quot;:{          &quot;s3SchemaVersion&quot;:&quot;1.0&quot;,        &quot;configurationId&quot;:&quot;testConfigRule&quot;,        &quot;bucket&quot;:{            &quot;name&quot;:&quot;sourcebucket&quot;,          &quot;ownerIdentity&quot;:{              &quot;principalId&quot;:&quot;A3NL1KOZZKExample&quot;          },          &quot;arn&quot;:&quot;arn:aws:s3:::sourcebucket&quot;        },        &quot;object&quot;:{            &quot;key&quot;:&quot;HappyFace.jpg&quot;,          &quot;size&quot;:1024,          &quot;eTag&quot;:&quot;d41d8cd98f00b204e9800998ecf8427e&quot;,          &quot;versionId&quot;:&quot;096fKKXTRTtl3on89fVO.nfljtsv6qko&quot;        }      }    }  ]}The remainder of the session can be done via the command line:export LAMBDA_NAME=your-lambda-function-nameexport LAMBDA_ROLE=your-lambda-execution-roleexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity | jq -r &quot;.Account&quot;)export LAMBDA_ROLE_ARN=arn:aws:iam::$AWS_ACCOUNT_ID:role/$LAMBDA_ROLEexport LAMBDA_S3_BUCKET=your-s3-bucketexport LAMBDA_S3_ARN=arn:aws:s3:::$LAMBDA_S3_BUCKET# package the lambda functionzip function.zip index.js# create functionaws lambda create-function --function-name $LAMBDA_NAME \\  --zip-file fileb://function.zip \\  --handler index.handler \\  --runtime nodejs10.x \\  --role $LAMBDA_ROLE_ARN# setup S3 notifications, first we&#39;ll allow the s3 bucket to invoke the lambdaaws lambda add-permission \\  --function-name $LAMBDA_NAME \\  --principal s3.amazonaws.com \\  --statement-id $LAMBDA_NAME$RANDOM \\  --action &quot;lambda:InvokeFunction&quot; \\  --source-arn $LAMBDA_S3_ARN \\  --source-account $AWS_ACCOUNT_IDexport LAMBDA_ARN=$(aws lambda get-function --function-name $LAMBDA_NAME  | jq -r .Configuration.FunctionArn)cat &lt;&lt; EOF &gt; notification.json{    &quot;LambdaFunctionConfigurations&quot;: [      {        &quot;Id&quot;: &quot;1234567890&quot;,        &quot;LambdaFunctionArn&quot;: &quot;$LAMBDA_ARN&quot;,        &quot;Events&quot;: [ &quot;s3:ObjectCreated:*&quot; ]      }    ]}EOF# next create the notification event in the bucket# DANGER: this will overwrite any existing event notifications in your bucket# DO NOT RUN THIS on a bucket that is important to you or work!aws s3api put-bucket-notification-configuration \\  --bucket $LAMBDA_S3_BUCKET \\  --notification-configuration file://notification.json# test the integration to see if the message formatting is correct. # this should look identical to the actual CloudWatch entry.# n.b. base64 appears to use the same switch to decode for both BSD and GNU # variantsaws lambda invoke \\  --invocation-type RequestResponse \\  --function-name $LAMBDA_NAME \\  --log-type Tail \\  --payload file://payload-test.json outputfile.txt \\  | jq -r .LogResult | base64 --decode# Finally let&#39;s test it properlytouch hello.txtaws s3 cp hello.txt s3://$LAMBDA_S3_BUCKET/# If you didn&#39;t install awslogs, you can still use AWS Console to view logs in # Cloud Watch.awslogs get /aws/lambda/s3-blab --start=&#39;5 min&#39;# Teardownaws lambda delete-function --function-name $LAMBDA_NAMEaws s3api put-bucket-notification-configuration \\  --bucket $LAMBDA_S3_BUCKET \\  --notification-configuration &#39;LambdaFunctionConfigurations=[]&#39;# you may wish to clear down the S3 bucket of your test files and the log group # that was created in CloudWatch.API and CLI features and verbs### FeaturesAliasEvent Source MappingFunction### Verbs (CRUD)createget/listupdate (function-[code/configuration])delete### Outliersadd-layer-version-permissionadd-permissiondelete-function-concurrencydelete-layer-versionget-account-settingsget-function-configurationget-layer-versionget-layer-version-by-arnget-layer-version-policyget-policyinvokelist-layer-versionslist-layerslist-tagslist-versions-by-functionpublish-layer-versionpublish-versionput-function-concurrencyremove-layer-version-permissionremove-permissiontag-resourceuntag-resourcewaitAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: OpsWorks",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "opsworks"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-opsworks",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?OpsWorks isprovides three managed servicesAWS OpsWorks for Chef AutomateAWS OpsWorks for Puppet EnterpriseAWS OpsWorks Stacks which uses Chef SoloIn this post, we'll primarily be focussing on AWS OpsWorks Stacks, but know that the others are managed services that can be utilised instead of running yourself (think of the comparison to self-hosting a database server versus using RDS).OpsWorks Stack...is a declarative state engine (as is Chef Automate and Solo). By this mean you define what you want to happen i.e. have a web server running, OpsWorks/Chef will then determine how to do it on the operating system that the Chef agent is installed on. Recall that package managers and configuration files live in different places in Operating Systems, even in distributions of Operating Systems (CentOS, Debian et al).is also unique compared to other Chef managed services in that it uses the concept of &quot;Layers&quot;autoscaling and scheduled scaling enabledprovides you with a choice of Chef 11 or 12 stacks. The difference between the two versions is that Chef 11 has built-in cookbooks, whereas Chef 12 allows you to use your own or community cookbooks.has a concept of Layers (think of these as different layers of functionality i.e. web server, database server)these layers can be OpsWorks, ECS or RDS basedAdditional resources:FAQsOpsWorks for Chef AutomateOpsWorks for Puppet EnterpriseOpsWorks StacksOpsWorks Stacks User GuideAPIsOpsWorks StacksOpsWorks Configuration ManagementCLIsOpsWorks StacksOpsWork Configuration ManagementWhy?If CloudFormation is most configurable (and ultimate the most complex) orchestration tool (requiring a fair bit of Operations/Infrastructure experience) and Elastic Beanstalk is developer friendly and quick to get up and running. Then OpsWorks Stacks is probably the middle ground.CloudFormation is a tool that is very specific to AWS, whereas your Chef recipes could be used on other Chef installations.When?You have familiarity with ChefWant a certain degree of control, but not the complexity of CloudFormation.How?I've not had the time to hunt down the CLI equivalent for this section, please drop me a comment if you have resources I can use!I've loosely based this around the Getting Started guide for Linux with the following exceptions:Using a Chef 11 StackThe application is a static siteThe application deployment is via https (the GitHub download as zip link)Go to AWS Console and find the OpsWorks service (if you don't have a Stack defined you'll be at the introductory page which provides info about Stacks and the other managed services offerings).Create a stack (Chef 11)Provide a stack nameEverything else is based on the region you choose i.e. VPC and subnet or sensible defaults provided by AWS (Operating System)Add a layer (OpsWorks / ECS / RDS) - well drill into OpsWorksSet Layer Type as Static Web ServerThe layer type is based on blueprints and can be load balancing, app server (Static, Node, PHP and Rails) and a few other types (MySQL, Memcache, Ganglia and custom). You can only have one type as a Layer.You can assign to an Elastic Load Balancer if you have one available (it will become a new layer). This makes sense if you've got multiple instances.Add an instance(s)Hostname - will be predefined based on the layer typeSize - AROOGA! AROOGA! aws wants to set this as c4.large maybe pick something cheaper like t2.micro? ;)Subnet - you will want to distribute your instances across different subnets.Advanced is where you can configure scaling type (24/7, time or load based) or override default settings for OS and SSH keyStart the instance(s)Add an ApplicationProvide a nameSet Repository Type as HTTP Archive (note: other options as Git, Subversion, S3 and Other)Set Repository URL as https://github.com/booyaa/static-site-demo/archive/master.zipClick •Add App* buttonDeploy the appClick the Deploy link next to your newly created &quot;App&quot;By default, the instances associated with your Static Web Server will be selectedClick the Deploy buttonWait until the app has been deployed to your instance, then go to the Instances link in the sidebar and click on the Public IP of your instance. This should launch the static site.To tear down:Delete the appStop the instancesDelete the instancesDelete the Static Web Server layerDelete the appAPI and CLI features and verbsStacksFeaturesAppInstanceLayerStackUser ProfileVerbs (CRUD)createdescribeupdatedeleteOutliersassign-instanceassign-volumeassociate-elastic-ipattach-elastic-load-balancerclone-stackcreate-deploymentderegister-ecs-clusterderegister-elastic-ipderegister-instancederegister-rds-db-instancederegister-volumedescribe-agent-versionsdescribe-commandsdescribe-deploymentsdescribe-ecs-clustersdescribe-elastic-ipsdescribe-elastic-load-balancersdescribe-load-based-auto-scalingdescribe-my-user-profiledescribe-operating-systemsdescribe-permissionsdescribe-raid-arraysdescribe-rds-db-instancesdescribe-service-errorsdescribe-stack-provisioning-parametersdescribe-stack-summarydescribe-time-based-auto-scalingdescribe-user-profilesdescribe-volumesdetach-elastic-load-balancerdisassociate-elastic-ipget-hostname-suggestiongrant-accesslist-tagsreboot-instanceregisterregister-ecs-clusterregister-elastic-ipregister-instanceregister-rds-db-instanceregister-volumeset-load-based-auto-scalingset-permissionset-time-based-auto-scalingstart-instancestart-stackstop-instancestop-stacktag-resourceunassign-instanceunassign-volumeuntag-resourceupdate-elastic-ipupdate-my-user-profileupdate-rds-db-instanceupdate-volumewaitConfiguration ManagementFeaturesServerEngineBackupEventsAccount attributesVerbs (CRUD)create (backup/server)describe (backups/servers/account-attribute/events)update (server/server-engine-attributes)delete (backup/server)Outliersassociate-nodedescribe-node-association-statusdisassociate-nodeexport-server-engine-attributerestore-serverstart-maintenancewaitAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Elastic Beanstalk",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "elasticbeanstalk"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-elasticbeanstalk",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Elastic Beanstalk...is a Platform as a Service (just like Heroku, Netlify), you deploy your code and it provisions the servers you need to get your app running.is powered by CloudFormation behind the scene.(Elastic Beanstalk) Extensions are the equivalent of User Data field for EC2 instances, in that you can add some tasks that need to run during the provisioning of servers i.e. enable automatic updates on windows.comes with a cli (eb), this is primarily aimed at developer giving them a similar PaaS experience to other providers like Heroku i.e. link your version control system (Git, etc) and Elastic Beanstalk will do the rest. There's a lot of simplification in the eb cli versus the aws cli e.g. eb create my-env (which creates an environment) would require three aws cli Elastic Beanstalk commands: check-dns-availability, create-application-version and create-environment.Additional resources:dev.to tag for [Elastic Beanstalk][devto_elasticbeanstalk]Elastic Beanstalk FAQsElastic Beanstalk User GuideElastic Beanstalk APIElastic Beanstalk CLIWhy?Of all the Orchestration tools provided by AWS, this is probably the easiest to use. Just like Heroku which pioneered this method of simplifying the application deployments, this is aimed at Developers who don't want to get involved in the operational side of provisioning infrastructure.Recall the level of complexity around CloudFormation, this isn't for everyone.When?Time poor or not willing to learn a more complex, but ultimately highly configurable Orchestration tool like CloudFormation.Don't have a sysadmin handyWant to release your application quickly and not worry about the details around the infrastructure.How?We're going to work through one of the example apps provided by AWS, download the Go example (go-v1.zip). The environment setup commands were cribbed from user guide too.Important: You'll need an S3 bucket to store the solution.export APP_NAME=hello-ebexport APP_BUCKET=your-bucket# copy the zip file to your bucketaws s3 cp path/to/downloaded/go-v1.zip s3://$APP_BUCKET/# create the applicationaws elasticbeanstalk create-application --application-name $APP_NAME{    &quot;Application&quot;: {        &quot;ApplicationArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3:xxx:application/hello-eb&quot;,        &quot;ApplicationName&quot;: &quot;hello-eb&quot;,        &quot;DateCreated&quot;: &quot;2019-05-29T16:03:18.885Z&quot;,        &quot;DateUpdated&quot;: &quot;2019-05-29T16:03:18.885Z&quot;,        &quot;ConfigurationTemplates&quot;: [],        &quot;ResourceLifecycleConfig&quot;: {            &quot;VersionLifecycleConfig&quot;: {                &quot;MaxCountRule&quot;: {                    &quot;Enabled&quot;: false,                    &quot;MaxCount&quot;: 200,                    &quot;DeleteSourceFromS3&quot;: false                },                &quot;MaxAgeRule&quot;: {                    &quot;Enabled&quot;: false,                    &quot;MaxAgeInDays&quot;: 180,                    &quot;DeleteSourceFromS3&quot;: false                }            }        }    }}# create the application versionaws elasticbeanstalk create-application-version \\  --application-name $APP_NAME \\  --version-label v1 \\  --source-bundle S3Bucket=&quot;$APP_BUCKET&quot;,S3Key=&quot;go-v1.zip&quot;{    &quot;ApplicationVersion&quot;: {        &quot;ApplicationVersionArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3:x:applicationversion/hello-eb/v1&quot;,        &quot;ApplicationName&quot;: &quot;hello-eb&quot;,        &quot;VersionLabel&quot;: &quot;v1&quot;,        &quot;SourceBundle&quot;: {            &quot;S3Bucket&quot;: &quot;your_bucket&quot;,            &quot;S3Key&quot;: &quot;go-v1.zip&quot;        },        &quot;DateCreated&quot;: &quot;2019-05-29T16:07:20.796Z&quot;,        &quot;DateUpdated&quot;: &quot;2019-05-29T16:07:20.796Z&quot;,        &quot;Status&quot;: &quot;UNPROCESSED&quot;    }}# create a configuration template, this tells Elastic Beanstalk which# specialised server image to useaws elasticbeanstalk create-configuration-template \\  --application-name $APP_NAME \\  --template-name v1 \\  --solution-stack-name &quot;64bit Amazon Linux 2018.03 v2.11.1 running Go 1.12.4&quot;  {    &quot;SolutionStackName&quot;: &quot;64bit Amazon Linux 2018.03 v2.11.1 running Go 1.12.4&quot;,    &quot;PlatformArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3::platform/Go 1 running on 64bit Amazon Linux/2.11.1&quot;,    &quot;ApplicationName&quot;: &quot;hello-eb&quot;,    &quot;TemplateName&quot;: &quot;v1&quot;,    &quot;DateCreated&quot;: &quot;2019-05-29T16:09:58Z&quot;,    &quot;DateUpdated&quot;: &quot;2019-05-29T16:09:58Z&quot;}# to get a list of the prebuilt platforms you can useaws elasticbeanstalk list-available-solution-stacks | jq -r &#39;.SolutionStacks[]&#39;64bit Amazon Linux 2018.03 v2.8.3 running Python 3.664bit Amazon Linux 2018.03 v2.8.3 running Python 3.464bit Amazon Linux 2018.03 v2.8.3 running Python64bit Amazon Linux 2018.03 v2.8.3 running Python 2.7*snip*# create the environment, well need to tell Elastic Beanstalk what IAM role to use to create ec2 instancescat options.txt[    {        &quot;Namespace&quot;: &quot;aws:autoscaling:launchconfiguration&quot;,        &quot;OptionName&quot;: &quot;IamInstanceProfile&quot;,        &quot;Value&quot;: &quot;aws-elasticbeanstalk-ec2-role&quot;    }]# create the environmentaws elasticbeanstalk create-environment \\  --cname-prefix $APP_NAME \\  --application-name $APP_NAME \\  --template-name v1 --version-label v1 \\  --environment-name $APP_NAME-env \\  --option-settings file://options.txt{    &quot;EnvironmentName&quot;: &quot;hello-eb-env&quot;,    &quot;EnvironmentId&quot;: &quot;e-g8pwbwxc5j&quot;,    &quot;ApplicationName&quot;: &quot;hello-eb&quot;,    &quot;VersionLabel&quot;: &quot;v1&quot;,    &quot;SolutionStackName&quot;: &quot;64bit Amazon Linux 2018.03 v2.11.1 running Go 1.12.4&quot;,    &quot;PlatformArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3::platform/Go 1 running on 64bit Amazon Linux/2.11.1&quot;,    &quot;CNAME&quot;: &quot;hello-eb.eu-west-3.elasticbeanstalk.com&quot;,    &quot;DateCreated&quot;: &quot;2019-05-29T16:11:32.355Z&quot;,    &quot;DateUpdated&quot;: &quot;2019-05-29T16:11:32.355Z&quot;,    &quot;Status&quot;: &quot;Launching&quot;,    &quot;Health&quot;: &quot;Grey&quot;,    &quot;Tier&quot;: {        &quot;Name&quot;: &quot;WebServer&quot;,        &quot;Type&quot;: &quot;Standard&quot;,        &quot;Version&quot;: &quot;1.0&quot;    },    &quot;EnvironmentArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3:xxx:environment/hello-eb/hello-eb-env&quot;}# examine the status of the new environmentaws elasticbeanstalk describe-environments --environment-names $APP_NAME-env{    &quot;Environments&quot;: [        {            &quot;EnvironmentName&quot;: &quot;hello-eb-env&quot;,            &quot;EnvironmentId&quot;: &quot;e-g8pwbwxc5j&quot;,            &quot;ApplicationName&quot;: &quot;hello-eb&quot;,            &quot;VersionLabel&quot;: &quot;v1&quot;,            &quot;SolutionStackName&quot;: &quot;64bit Amazon Linux 2018.03 v2.11.1 running Go 1.12.4&quot;,            &quot;PlatformArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3::platform/Go 1 running on 64bit Amazon Linux/2.11.1&quot;,            &quot;EndpointURL&quot;: &quot;awseb-e-g-AWSEBLoa-1S8LUETQTXR9H-746312279.eu-west-3.elb.amazonaws.com&quot;,            &quot;CNAME&quot;: &quot;hello-eb.eu-west-3.elasticbeanstalk.com&quot;,            &quot;DateCreated&quot;: &quot;2019-05-29T16:11:32.324Z&quot;,            &quot;DateUpdated&quot;: &quot;2019-05-29T16:14:11.172Z&quot;,            &quot;Status&quot;: &quot;Ready&quot;,            &quot;AbortableOperationInProgress&quot;: false,            &quot;Health&quot;: &quot;Green&quot;,            &quot;Tier&quot;: {                &quot;Name&quot;: &quot;WebServer&quot;,                &quot;Type&quot;: &quot;Standard&quot;,                &quot;Version&quot;: &quot;1.0&quot;            },            &quot;EnvironmentLinks&quot;: [],            &quot;EnvironmentArn&quot;: &quot;arn:aws:elasticbeanstalk:eu-west-3:xxx:environment/hello-eb/hello-eb-env&quot;        }    ]}# Once the status is Ready and health is Green, you can open the fully qualified domain name in `CNAME`curl -sv http://$APP_NAME.eu-west-3.elasticbeanstalk.com/ | head*snip**&lt; HTTP/1.1 200 OK&lt; Accept-Ranges: bytes&lt; Content-Type: text/html; charset=utf-8&lt; Date: Wed, 29 May 2019 16:15:33 GMT&lt; Last-Modified: Thu, 17 Sep 2015 17:53:06 GMT&lt; Server: nginx/1.14.1&lt; Content-Length: 3049&lt; Connection: keep-alive&lt;{ [1208 bytes data]* Connection #0 to host hello-eb.eu-west-3.elasticbeanstalk.com left intact&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;  &lt;!--    Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.    Licensed under the Apache License, Version 2.0 (the &quot;License&quot;). You may not use this file except in compliance with the License. A copy of the License is located at        http://aws.Amazon/apache2.0/# Let&#39;s tear downaws elasticbeanstalk terminate-environment \\    --environment-name $APP_NAME-envaws elasticbeanstalk delete-configuration-template \\    --application-name $APP_NAME --template-name v1aws elasticbeanstalk delete-application \\    --application-name $APP_NAMEN.B. I've noticed there's some delay in deleting the application. You may need to check on the AWS console to confirm all the resources have been deleted.API and CLI features and verbsFeaturesApplication(s)VersionResource LifecycleEnvironmentConfigurationPlatform VersionStorageVerbs (CRUD)ApplicationsCreate (Application [Version])Describe (Applications)Update (Application [Resource Lifecycle])Delete (Application [Version])EnvironmentPlatform VersionStorageOutliersStacksCancelUpdateStackContinueUpdateRollbackDescribeStackDriftDetectionStatusEstimateTemplateCostGetTemplateSummaryUpdateTerminationProtectionValidateTemplateChange setsExecuteChangeSetStack setsStopStackSetOperationAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: CloudFormation",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification",
            
        
            "cloudformation"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-cloudformation",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?CloudFormation is a templating language that can be expressed as JSON or YAML. This is one tool that falls under the Infrastructure as Code (IaC) category for this domain.The core concepts you need to be aware of are:Stack - an instantiation of a templateA template consist of the following core elementsParameters - User configuration optionsMapping - Hashes (array of key/value pairs), allows you to apply logic i.e. choose the correct AMI based on region.Resources - The resources we'll use CloudFormation to provision. At the time of writing there 89 services that are directly accessible through CloudFormation, later we'll see how to use services that aren't via Custom Resources.Output - Results from the template, usually fed into another template as Parameters.Other elements to be aware ofFormat Version - identifies the capabilities of the templateDescription - self-explanatoryMetadata - provides information about the templateConditions - commonly used to define if a resource is created i.e. create this resource if the environment type is productionTransform - allows you to execute macros that are either template snippets (AWS::Include) or serverless (aka Lambda - AWS:Serverless).Stack Policies - IAM style policy statement which governs what can be changed and who can change itProgrammabilityIntrinsic functions - helper functions.Custom ResourcesTransform section in a templateIntrinsic functions (helpers)FunctionUsecasesFn::Base64UserData of EC2 instancesFn::CidrCIDR address blocks / NetworkingFn::FindInMapLookups of values i.e. AMIs by regionFn::GetAttCross referencing templates (including self)Fn::GetAZsNetworking / SubnetsFn::ImportValueCross referencing templates (including self)Fn::JoinMerges an array into a stringFn::SelectPicking a value from an arrayFn::SplitTurns a string into an arrayFn::SubSubstituting one value for anotherFn::TransformCalling CloudFormation MacrosRefCross referencing templates (including self)Fn::AndConditional operationFn::OrConditional operationFn::EqualsConditional functionFn::IfConditional functionFn::NotConditional functionResource AttributesCreationPolicy, requires a signal to be sent by the resource or a timeout occursCreationPolicy:  ResourceSignal:    Count: &#39;3&#39; # 3 instances have been created, so 3 signals will need to be generated before fulfilling the CreationPolicy requirements    Timeout: PT15MTimeout value is ISO8601 durations format: PT#H#M#S where # is the number of hours, minutes and seconds. Give the instances as long as possible, if the timeout is too short you will trigger rollbacks. PT15M is a timeout of 15 minutesTo send a signal, you need to install help script called cfn-signal on the resources (usually done in the User Data area of EC2 instances).DeletionPolicy - define what happens to a resource when the stack is deleted. Possible values are:Delete - defaultRetainSnapshot - only available to EBS, RDS and RedShift. Storage costs for storing the snapshot.DependsOn - has no guarantees that the process will have completed successfullyUpdatePolicy - defines what happens to resource when the stack is updated.UpdateReplacePolicy - use this retain / backup a physical instance of a resource it is replaced during a stack update.Know when to use the Wait condition over a CreationPolicy.Pseudo ParametersAre predefined parameters that return a valued on the current context i.e. current account or region in use.ParameterReturnsAWS::AccountIdThe account IDAWS::NotificationARNsA list of notification ARNs for a stackAWS::NoValueRemoves the corresponding resource when specified using Fn::IfAWS::PartitionThe partition a resource is in, only relevant to specialist regions like China and US GovernmentAWS::RegionThe current regionAWS::StackIdThe ID of the stack currently createdAWS::StackNameThe Name of the stack currently createdAWS::URLSuffixThe suffix for a domain typically amazonaws.com, but may vary for specialized regionsNested stacksA stack contains resources: S3 bucket, EC2 instances and other AWS servicesA stack can also contain another stack as a resource.Allow a complex infrastructure to be split up into manageable templates.Allows you to get around Stack limits (200/60/60 resources/outputs/parameters)Stack updatesStack Policy is JSON only.The general rule is to allow everything, but deny specific resourcesThe absence of stack policy means all updates are permittedOnce applied it can't be deletedOnce applied all objects are protected by default and updates are deniedCustom ResourcesCustom Resources are a way to provision and track resources that are not supported directly through CloudFormation.The request/response mechanism is either an SNS topic or Lambda backed ARN.AWS has a [walk through][docs_custom_resource] that demonstrates how to create a custom resource to perform an AMI lookup (using Lambda) to provide the correct AMI for a given region (in this case the region where we create the stack) and CPU type.Before custom resources, you would've had to keep a static list of AMIs in the Mappings section of a template.Why?This allows you to define your infrastructure as code, rather than manual steps carried out via various UIs (Console and CLI)When?Deploying infrastructure in a systematic and repeatable fashion rather than doing it manually.Repeat pattern environment i.e. You host WordPress business and you have a template that deploys a web server and database to each new customer.If you are using an automated CI/CD environment, but want to expand to a Blue/Green (Red/Black) and create a mirror of your environment to allow for zero downtime.Create an environment in any region of the AWS cloud without manual reconfiguration i.e. AMI selection or subnet allocation.You want to track change to your environment, by using CloudFormation templates these can be stored in a version control system i.e. GitHow?Here's a very basic example of CloudFormation, we'll use it to create an S3 bucket.The CloudFormation template: hello-bucket.yaml.N.B. To keep things terse, I've decided to only use YAML as the template format. CloudFormation can use JSON (in fact this was the original format, so you will still find a lot of examples in this format).Resources:  HelloBucket:    Type: AWS::S3::BucketHelloBucket is the logical name of our resource, and the type we've gone for is an S3 bucket.Let's use the CLI to create the stack based on this template.aws cloudformation create-stack \\  --stack-name hellostack \\  --template-body \\  file:///path/to/hello-bucket.yaml {    &quot;StackId&quot;: &quot;arn:aws:cloudformation:eu-west-3:xxx:stack/hellostack/4a3b0220-7552-11e9-acf0-0a230f532f04&quot;}  aws cloudformation describe-stacks{    &quot;Stacks&quot;: [        {            &quot;StackId&quot;: &quot;arn:aws:cloudformation:eu-west-3:xxx:stack/hellostack/4a3b0220-7552-11e9-acf0-0a230f532f04&quot;,            &quot;StackName&quot;: &quot;hellostack&quot;,            &quot;CreationTime&quot;: &quot;2019-05-13T07:39:50.524Z&quot;,            &quot;RollbackConfiguration&quot;: {},            &quot;StackStatus&quot;: &quot;CREATE_COMPLETE&quot;,            &quot;DisableRollback&quot;: false,            &quot;NotificationARNs&quot;: [],            &quot;Tags&quot;: [],            &quot;DriftInformation&quot;: {                &quot;StackDriftStatus&quot;: &quot;NOT_CHECKED&quot;            }        }    ]}Let's check on the status of stack creation. aws cloudformation describe-stack-resources --stack-name hellostack{    &quot;StackResources&quot;: [        {            &quot;StackName&quot;: &quot;hellostack&quot;,            &quot;StackId&quot;: &quot;arn:aws:cloudformation:eu-west-3:xxx:stack/hellostack/4a3b0220-7552-11e9-acf0-0a230f532f04&quot;,            &quot;LogicalResourceId&quot;: &quot;HelloBucket&quot;,            &quot;PhysicalResourceId&quot;: &quot;hellostack-hellobucket-1ux8azkoq7t0t&quot;,            &quot;ResourceType&quot;: &quot;AWS::S3::Bucket&quot;,            &quot;Timestamp&quot;: &quot;2019-05-13T07:40:15.289Z&quot;,            &quot;ResourceStatus&quot;: &quot;CREATE_COMPLETE&quot;,            &quot;DriftInformation&quot;: {                &quot;StackResourceDriftStatus&quot;: &quot;NOT_CHECKED&quot;            }        }    ]}Useful fields:StackName - this is the logic name of Stack (which we defined when using the create-stack sub command in the CLI)PhysicalResourceId - this is the bucket name, note the default naming convention: &lt;Stack Name&gt;-&lt;Logical Name of Resource in the Template&gt;-&lt;Random Hex String&gt;ResourceStatus - This tells us if the resource creation was successful.We can verify the name of the bucket by using the s3api command:aws s3api list-buckets | jq &#39;.Buckets[] | select(.Name | contains(&quot;hellostack&quot;))&#39;{  &quot;Name&quot;: &quot;hellostack-hellobucket-1ux8azkoq7t0t&quot;,  &quot;CreationDate&quot;: &quot;2019-05-13T07:39:55.000Z&quot;}Finally let's tear down, and verify the bucket has been deleted.aws cloudformation delete-stack --stack-name hellostack# no outputaws s3api list-buckets | jq &#39;.Buckets[] | select(.Name | contains(&quot;hellostack&quot;))&#39;# no outputAPI and CLI features and verbs### FeaturesStacks [Drift/Events/Policy/Resource(s)Drift(s)]TemplateImports/ExportsChange setsStack setsInstance(s)SetSetOperationSetOperationResults### Verbs (CRUD)StacksCreateList/Describe (stacks)Set/Update/PutDeleteChange setsCreateList/DescribeSet/Update/PutDeleteStack setsCreateList/DescribeSet/Update/PutDelete### OutliersStacksCancelUpdateStackContinueUpdateRollbackDescribeStackDriftDetectionStatusEstimateTemplateCostGetTemplateSummaryUpdateTerminationProtectionValidateTemplateChange setsExecuteChangeSetStack setsStopStackSetOperationAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Configuration Management and Infrastructure as Code introduction",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-configuration-management-and-infrastructure-as-code-intro",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Determine deployment services based on deployment needsDetermine application and infrastructure deployment models based on business needsApply security concepts in the automation of resource provisioningDetermine how to implement lifecycle hooks on a deploymentApply concepts required to manage systems using AWS configuration management tools and servicesWhat whitepapers are relevant?According to the AWS Whitepapers for DevOps we should look at the following documents:Infrastructure as Code (39 pages)Introduction to DevOps on AWS (20 pages)Practicing Continuous Integration and Continuous Delivery (32 pages)Jenkins on AWS (48 pages)Import Windows Server to Amazon EC2 with PowerShell (20 pages)What services and products covered in this domain?Useful https://aws.amazon.com/devops/#infrastructureascodeCloudFormation - This is a templating language that allows you to codify your infrastructure. This is the &quot;Infrastructure as Code&quot; part of this domain.OpsWorks - This service provides managed versions of Chef and Puppet. These are both industry standard configuration management systems.Elastic Beanstalk - is AWS' Platform as a Service (PaaS) offering.AWS Lambda - A service to run microservices / Serverless functions / Buzzword bingoAWS ECS - Managed container services. IaC (codified)AWS Config - Auditing services of your AWS services.AWS Managed Services - Let's AWS manage your AWS!What about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages. Alternatively, get familiar with the services using the FAQs:CloudFormationOpsWorks has multiple FAQs for their various offerings Chef Automate, Puppet Enterprise and StacksElastic BeanstalkAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesYou're all expected to know the APIsCloudFormationOpsWorks has two APIs Stacks and Configuration ManagementElastic BeanstalkAWS LambdaAWS ECSThere's no API for AWS Managed Services because this a professional or technical services offering.Before you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsCloudFormationOpsWorks has two commands opswork and opswork-cmElastic BeanstalkAWS LambdaAWS ECSThere's no CLI for AWS Managed Services because there's no corresponding API.As with the API, there are patterns to the commands.Configu-what? And Infra as Who?In the previous domain, we learnt that SDLC specifically the continuous delivery pipeline ensures that our code's integrity is being tested repeatedly and in a consistent manner.Now we'll see how we can achieve something similar to the underlying infrastructure that powers both our build and hosting of our applications.Configuration Management is a systematic way of handling changes to servers in such as a way that it maintains integrity over time. The key thing to remember is that often we talking about maintaining lots of servers i.e. more than one. To do this manually introduces risks that steps will be missed and inconsistencies in your environments will occur.By automating this process for server builds and maintenance we reduce this risk. Whilst you could do this yourself through a series of shell scripts and ssh, it's better to use a dedicated tool, some popular choices are Puppet, Chef, and, Salt Stack and Ansible.If Configuration Management ensures that our servers are patched to the correct version of operating system and contain the correct software to operate, then Infrastructure as Code ensures that provisioning of drum roll Infrastructure is done as reproducible steps. Tools you can expect to find in this space are Terraform, Azure Resource Manager and of course CloudFormation.Both configuration management and infrastructure as code are often expressed as templates or a programming language. This makes both ideal candidates for using version control systems like Git to track changes.Our next blog post will be about CloudFormationAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationCloudFormationElastic BeanstalkOpsWorksAWS LambdaAWS ECSAWS ConfigAWS Managed ServicesDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Code Pipeline",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-code-pipeline",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try to refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?CodePipeline is continuous delivery service that can be used to orchestrate the various services required to release software automatically: CodeCommit, CodeBuild and CodeDeploy.Why?Whilst it is possible to trigger CodeComit, CodeBuild and CodeDeploy manually, CodePipeline will do this for you automatically (via CloudWatch Events)When?SDLC automation~~~~~~~~~~~~~~~~CodePipeline+---------------------------------------+|                                       || CodeCommit -&gt; CodeBuild -&gt; CodeDeploy ||                                       |+---------------------------------------+How?TODO: going to revisit the previous blog posts for CodeCommit, CodeBuild and CodeDeploy to have a lab that spans these posts.API and CLI features and verbsFeaturesPipelinesCustom Action TypeWebhooksVerbs (CRUD)create/registerget/listupdate/putdeleteOutliersacknowledge-jobacknowledge-third-party-jobdisable-stage-transitionenable-stage-transitionget-job-detailsget-pipeline-executionget-pipeline-stateget-third-party-job-detailslist-action-executionslist-action-typeslist-pipeline-executionspoll-for-jobspoll-for-third-party-jobsput-action-revisionput-approval-resultput-job-failure-resultput-job-success-resultput-third-party-job-failure-resultput-third-party-job-success-resultretry-stage-executionstart-pipeline-executionAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationCode CommitCode BuildCode DeployCode PipelineDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Code Deploy",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-code-deploy",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Code Build is a managed deployment service.Why?Deployment services are often part of the build farm so would also contribute to infrastructure expenditure.When?SDLC automation~~~~~~~~~~~~~~~~CodeCommit -&gt; CodeBuild -&gt; [CodeDeploy] -&gt; ???How?This is based around the tutorial for deploying to an EC2 Auto Scaling Group. The key difference is that I've condensed it to only include instructions for using on Linux/MacOS based machine using the Amazon v2 AMI.Creating the CodeDeploy service role and EC2 IAM Instance ProfileThis is part of the Getting Started guide. N.B. We've opted to use the managed policy for deploying to EC2/On-Premises compute platform.Copy the JSON object below and paste into a new file called CodeDeployDemo-Trust.json{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [      {          &quot;Sid&quot;: &quot;&quot;,          &quot;Effect&quot;: &quot;Allow&quot;,          &quot;Principal&quot;: {              &quot;Service&quot;: [                  &quot;codedeploy.amazonaws.com&quot;              ]          },          &quot;Action&quot;: &quot;sts:AssumeRole&quot;      }  ]}aws iam create-role \\  --role-name CodeDeployServiceRole \\  --assume-role-policy-document file://CodeDeployDemo-Trust.jsonaws iam attach-role-policy \\  --role-name CodeDeployServiceRole \\  --policy-arn arn:aws:iam::aws:policy/service-role/AWSCodeDeployRoleCopy the JSON object below and paste into a file called CodeDeployDemo-EC2-Trust.json{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [      {          &quot;Sid&quot;: &quot;&quot;,          &quot;Effect&quot;: &quot;Allow&quot;,          &quot;Principal&quot;: {              &quot;Service&quot;: &quot;ec2.amazonaws.com&quot;          },          &quot;Action&quot;: &quot;sts:AssumeRole&quot;      }  ]}Copy the JSON object below and paste into a file called CodeDeployDemo-EC2-Permissions.json{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [      {          &quot;Action&quot;: [              &quot;s3:Get*&quot;,              &quot;s3:List*&quot;          ],          &quot;Effect&quot;: &quot;Allow&quot;,          &quot;Resource&quot;: &quot;*&quot;      }  ]}aws iam create-role \\  --role-name CodeDeployDemo-EC2-Instance-Profile \\  --assume-role-policy-document file://CodeDeployDemo-EC2-Trust.jsonaws iam put-role-policy \\  --role-name CodeDeployDemo-EC2-Instance-Profile   --policy-name CodeDeployDemo-EC2-Permissions   --policy-document file://CodeDeployDemo-EC2-Permissions.jsonaws iam create-instance-profile \\  --instance-profile-name CodeDeployDemo-EC2-Instance-Profileaws iam add-role-to-instance-profile \\  --instance-profile-name CodeDeployDemo-EC2-Instance-Profile \\  --role-name CodeDeployDemo-EC2-Instance-ProfileCreate the autoscaling group (ASG)Copy the script below and paste into a new file called instance-setup.sh. This will install the CodeDeploy agent will work deploy the application to the instances associated by the Deployment Group (via the ASG).#!/bin/bashyum -y updateyum install -y rubycd /home/ec2-usercurl -O https://aws-codedeploy-????.s3.amazonaws.com/latest/installchmod +x ./install./install autoEdit ???? to reflect your region i.e. if your region is Paris then the value would be eu-west-3.Environment variables we want to define are:AMI_ID = The Amazon v2 AMI for your regionKEY_NAME = Your Key Pair for the regionAZ = The available zone(s) for our regionHere are a few aws cli commands to help you get the right values for your own environment.# Find the Amazon v2 AMI for your regionaws ec2 describe-images --owners amazon \\  --filters \\    &#39;Name=name,Values=amzn2-ami-hvm-2.0.????????-x86_64-gp2&#39; \\    &#39;Name=state,Values=available&#39; \\  --output json | \\jq -r &#39;.Images | sort_by(.CreationDate) | last(.[]).ImageId&#39;)# List key pairs in your regionaws ec2 describe-key-pairs | jq -r &#39;.KeyPairs[].KeyName | sort_by(.CreationDate)&#39;# List availability zones for your regionaws ec2 describe-availability-zonesCreate the launch configuration:AMI_ID=__FILL_ME_IN__KEY_NAME=__FILE_ME_IN__aws autoscaling create-launch-configuration \\  --launch-configuration-name CodeDeployDemo-AS-Configuration \\  --image-id $AMI_ID \\  --key-name $KEY_NAME \\  --iam-instance-profile CodeDeployDemo-EC2-Instance-Profile \\  --instance-type t2.micro \\  --user-data file://instance-setup.shCreate the autoscaling group:set AZ eu-west-3aaws autoscaling create-auto-scaling-group \\  --auto-scaling-group-name CodeDeployDemo-AS-Group \\  --launch-configuration-name CodeDeployDemo-AS-Configuration \\  --min-size 1 \\  --max-size 1 \\  --desired-capacity 1 \\  --availability-zones $AZRun the following command to check on the state of your ASG. Proceed to the next step when the status is &quot;Healthy Inservice&quot;:aws autoscaling describe-auto-scaling-groups \\  --auto-scaling-group-names CodeDeployDemo-AS-Group \\  --query &quot;AutoScalingGroups[0].Instances[*].[HealthStatus, LifecycleState]&quot; \\  --output textDeploy the Application to the ASGEnvironment variables we want to define are:SERVICE_ROLE_ARN = The Service Role we created at the beginning of the labREGION = Our regionSERVICE_ROLE_ARN=$(aws iam get-role --role-name CodeDeployServiceRole --query &quot;Role.Arn&quot; --output text)REGION eu-west-3BUCKET_NAME aws-codedeploy-$REGIONaws deploy create-application --application-name SimpleDemoAppaws deploy create-deployment-group \\  --application-name SimpleDemoApp \\  --auto-scaling-groups CodeDeployDemo-AS-Group \\  --deployment-group-name SimpleDemoDG \\  --deployment-config-name CodeDeployDefault.OneAtATime \\  --service-role-arn $SERVICE_ROLE_ARNDEPLOYMENT_ID=$(aws deploy create-deployment \\  --application-name SimpleDemoApp \\  --deployment-config-name CodeDeployDefault.OneAtATime \\  --deployment-group-name SimpleDemoDG \\  --s3-location bucket=$BUCKET_NAME,bundleType=zip,key=samples/latest/SampleApp_Linux.zip | jq -r .deploymentId)AppSpec fileLet's download a copy of the sample application and examine the contents: curl -LO https://s3.$REGION.amazonaws.com/$BUCKET_NAME/samples/latest/SampleApp_Linux.zip unzip SampleApp_Linux.zip tree # to see the structure of the app cat appspec.ymlThe contents of the archive is:the web page we'll be deploying via CodeDeployscripts to install/stop/start the web serverAppSpec file which is what the CodeDeploy agent will use to deploy the webpage to each instance in the deployment group.Let's return back to our deployment.Keep checking on the deployment until the following command outputs &quot;Succeeded&quot;.aws deploy get-deployment --deployment-id $DEPLOYMENT_ID \\  --query &quot;deploymentInfo.status&quot; \\  --output textTip: If you get an access denied error at the Download stage, the EC2 IAM Instance Profile maybe configured incorrectly or the policy to allow access to S3 wasn't attached.Let's verify that our deployment worked, by getting the public address of our instance.INSTANCE_ID=$(aws autoscaling describe-auto-scaling-groups \\  --auto-scaling-group-names CodeDeployDemo-AS-Group \\  --query &quot;AutoScalingGroups[0].Instances[*].InstanceId&quot; --output text)curl $(aws ec2 describe-instances \\  --instance-id $INSTANCE_ID \\  --query &quot;Reservations[0].Instances[0].PublicDnsName&quot; \\  --output text)Increase the number of instances in the ASGWe'll increase the instance count by ASG to bring it to a total of 2.aws autoscaling update-auto-scaling-group \\  --auto-scaling-group-name CodeDeployDemo-AS-Group \\  --min-size 2 \\  --max-size 2 \\  --desired-capacity 2Keep checking on the instances in the ASG, only proceed to the next step when they are both &quot;Healthy InService&quot;.aws autoscaling describe-auto-scaling-groups \\  --auto-scaling-group-names CodeDeployDemo-AS-Group \\  --query &quot;AutoScalingGroups[0].Instances[*].[HealthStatus, LifecycleState]&quot; \\  --output textNow check the status of the deployment, it should be &quot;Succeeded&quot;.DEPLOYMENT_ID=$(aws deploy list-deployments \\  --application-name SimpleDemoApp \\  --deployment-group-name SimpleDemoDG \\  --query &quot;deployments&quot; | jq -r &#39;last(.[])&#39;)aws deploy get-deployment \\  --deployment-id $DEPLOYMENT_ID \\  --query &quot;deploymentInfo.[status, creator]&quot; \\  --output text  Next, let's curl all the instances to verify we've got a working site.INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \\  --auto-scaling-group-names CodeDeployDemo-AS-Group \\  --query &quot;AutoScalingGroups[0].Instances[*].InstanceId&quot; \\  --output text)  --instance-ids $INSTANCE_IDS \\  --query &quot;Reservations[0].Instances[0].PublicDnsName&quot; \\  --output textaws ec2 describe-instances \\  --instance-ids $INSTANCE_IDS \\  --query &quot;Reservations[*].Instances[*].PublicDnsName&quot; \\  | jq -r .[][] | xargs curl {}Clean upaws autoscaling delete-auto-scaling-group \\  --auto-scaling-group-name CodeDeployDemo-AS-Group \\  --force-deleteaws autoscaling delete-auto-scaling-group \\  --auto-scaling-group-name CodeDeployDemo-AS-Group \\  --force-deleteaws deploy delete-application \\  --application-name SimpleDemoAppAPI and CLI features and verbsFeaturesDeploymentsDeployment groupsApplication revisionsApplicationsDeployment configurationsOn-premise instancesVerbs (CRUD)createbatch-get/get/list/describeupdate/putdeleteOutliersregister-on-premises-instancebatch-get-on-premises-instancesadd-tags-to-on-premises-instancesremove-tags-from-on-premises-instancesderegister-on-premises-instancecontinue-deploymentstop-deploymentdelete-git-hub-account-tokenderegisterinstallpushregisterregister-application-revisionput-lifecycle-event-hook-execution-statusAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationCode CommitCode BuildCode DeployCode PipelineDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Code Build",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-code-build",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Code Build is a managed build service.Why?Asides the infrastructure required to host your applications and services, build farms are the next largest expenditure. Also orchestration to provision a build farm is non-trivial. Managed build services like Travis CI, Circle CI and Appveyor have helped Open Source projects thrive and allowed maintainers to support a wide range of CPU and operating systems.When?SDLC automation~~~~~~~~~~~~~~~~CodeCommit -&gt; [CodeBuild] -&gt; ???How?This is loosely based around the [Getting Started]https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started.html) section of the User Guide.The main differences are that I'm going to use the CLI instead of the Web UI to aid in learning these commands too, also I'm going to use Rust to demonstrate how to distribute build artefacts.Create S3We're going to use S3 to store our build artefacts, so we'll create a bucket in the same region as our CodeBuild project (eu-west-2). This is an important thing to note, as you can't use an S3 bucket in a different region to your CodeBuild project.aws s3api create-bucket \\  --bucket hello-codebuild \\  --region eu-west-2 \\  --create-bucket-configuration LocationConstraint=eu-west-2Setup CodeCommitNext let's create a new CodeCommit repository to store our code.aws codecommit create-repository \\  --repository-name hello-codebuild  --region eu-west-2Pull in the source codeTo avoid adding more steps (which aren't relevant), I've created a simple Rust app that'll we'll use for this lab.Now we're going to do the following:clone the CodeCommit repodownload our sample appunzip the sample app archivedo a bit of house keep to place the sample app at the right directory level, and remove the archivepush the code back up to the CodeCommit repoREPO_URL=$(aws codecommit get-repository --repository-name $REPO_NAME | jq -r .repositoryMetadata.cloneUrlSsh)git clone $REPO_URLcd hello-codebuildcurl -sLO https://github.com/booyaa/hello-codebuild/archive/master.zipunzip master.ziprm -rf master.zipmv hello-codebuild-master/* .rm -rf hello-codebuild-mastergit add -Agit commit -m &#39;initial commit&#39;git push -u origin masterBuild specificationThe build specification is similar to other configuration files used by CI services, it tells the build service how to build our code.version: 0.2phases:  install:    commands:      - apt-get update -y      - apt-get install -y build-essential      - curl https://sh.rustup.rs -sSf | sh -s -- -y      - PATH=/root/.cargo/bin:$PATH  build:    commands:      - cargo test      - cargo build --releaseartifacts:  files:    - ./target/release/hello-codebuildThe points of interest for us, are the following sections or phases:install - what commands do we need to run to install the build tools for our language. We're using apt because our CodeBuild project will use an Ubuntu 14.04 based server image.build - what commands are required to build the codeartifacts - what do we want to upload to our S3 bucket?Create a service role for CodeBuildNext we'll create a new service role (this is done for us if we create the project through UI), luckily the User Guide has instructions on how to do this through the CLI.Let's create the following files:create-role.json{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Principal&quot;: {        &quot;Service&quot;: &quot;codebuild.amazonaws.com&quot;      },      &quot;Action&quot;: &quot;sts:AssumeRole&quot;    }  ]}put-role-policy.json{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Sid&quot;: &quot;CloudWatchLogsPolicy&quot;,      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;logs:CreateLogGroup&quot;,        &quot;logs:CreateLogStream&quot;,        &quot;logs:PutLogEvents&quot;      ],      &quot;Resource&quot;: [        &quot;*&quot;      ]    },    {      &quot;Sid&quot;: &quot;CodeCommitPolicy&quot;,      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;codecommit:GitPull&quot;      ],      &quot;Resource&quot;: [        &quot;*&quot;      ]    },    {      &quot;Sid&quot;: &quot;S3GetObjectPolicy&quot;,      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;s3:GetObject&quot;,        &quot;s3:GetObjectVersion&quot;      ],      &quot;Resource&quot;: [        &quot;*&quot;      ]    },    {      &quot;Sid&quot;: &quot;S3PutObjectPolicy&quot;,      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;s3:PutObject&quot;      ],      &quot;Resource&quot;: [        &quot;*&quot;      ]    }  ]}Let's run the following commands to create the Service Role.aws iam create-role \\  --role-name CodeBuildServiceRole   --assume-role-policy-document file://create-role.jsonaws iam put-role-policy --role-name CodeBuildServiceRole \\  --policy-name CodeBuildServiceRolePolicy \\  --policy-document file://put-role-policy.jsonOnce we've created this Service Role, we can reference it for all for future CodeBuild projects.Create the CodeBuild projectFinally we can create our CodeBuild project!REPO_NAME=hello-codebuildS3_BUCKET=$REPO_NAMESERVICE_ROLE_ARN=$(aws iam get-role --role-name CodeBuildServiceRole | jq -r .Role.Arn)REPO_URL=$(aws codecommit get-repository --repository-name $REPO_NAME | jq -r .repositoryMetadata.cloneUrlHttp)aws codebuild create-project \\  --name $REPO_NAME \\  --source type=CODECOMMIT,location=$REPO_URL \\  --artifacts type=S3,location=$S3_BUCKET \\  --environment type=LINUX_CONTAINER,image=aws/codebuild/ubuntu-base:14.04,computeType=BUILD_GENERAL1_SMALL,imagePullCredentialsType=CODEBUILD \\  --service-role $SERVICE_ROLE_ARNKey parameters/switches to point out:source We've opted for the CodeCommit type, but there are also options for BitBucket, CodePipeline, GitHub, S3 and finally No Source (when there's no source code). The other parameter we provided was the location of the Git repo URL. More details about the sourceType can be found in the API.artifacts We've opted for the type S3, the only other options are CodePipeline and No artifacts. The other parameter we provided was the location of the S3 bucket. More details about the artifacts can be found in the APIenvironment The image type is Linux, instead of Windows. The image is Ubuntu 14.04 base (which looks like the only choice if you go through the UI). The computeType is the smallest general purpose build serverThe imagePullCredentialsType is AWS CodeBuild's own credentials to pull the image, which is fine since the Container Registry we're using is the one provided by AWS for it's own images.service-role is the ARN of the IAM role that we created earlier.Run buildBUILD_ID=$(aws codebuild start-build --project-name $REPO_NAME | jq -r .build.id)View build logsaws codebuild batch-get-builds --ids $BUILD_IDGet artifact and test itaws s3 cp s3://hello-codebuild/hello-codebuild/target/release/hello-codebuild hello-codebuildchmod +xdocker run --rm -it -v $PWD:/root ubuntu bash -c /root/hello-codebuildClean upaws codebuild delete-project --name hello-codebuildaws codecommit delete-repository --repository-name hello-codebuildaws s3api delete-bucket --bucket hello-codebuildYou could remove the Service Role, but it might be handy if you plan to do some more practice sessions with this lab.API and CLI features and verbsFeaturesProject(s)Build(s)WebhookSource CredentialsVerbs (CRUD)createbatch-get/get/list/describeupdate/putdeleteOutlierslist-curated-environment-imagesimport-source-credentialsinvalidate-project-cachestop/start buildAWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationCode CommitCode BuildCode DeployCode PipelineDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: Code Commit",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-code-commit",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationCaveat emptorUsing AWS costs money, some of these services may not be part of the AWS Free Tier. You can keep costs down by tearing down anything you've created whilst learning, but it's still possible to run up a hefty bill so pay attention to the instances you setup!I'm very lucky to be able to use my employer's AWS account. You should ask your place of work if a similar arrangement can be made as part of your study.Velocius quam asparagi conquanturThe format of the blog posts is liable to change as I try refine my mental model of each domain, so be sure to revisit the blog posts on a regular basis.What?Code Commit is:a managed Version Control serviceGit based (using HTTPS or SSH for connections)Files are encrypted at rest, keys can be found in can be found in IAM &gt; Encryption Keys.AWS CLI:  aws kms list-aliases | jq '.Aliases[] | select(.AliasArn | contains(&quot;codecommit&quot;))'Security credentials for HTTPS and SSH connections are defined in IAM per user. Pro-tip: if you need to brush up on IAM, see Helen Anderson's excellent guide: Don't you know who IAM?Why?Spotted a little gem in the FAQ that asks how Code Commit compares against a versioned S3 bucket. Enabling versioning on an S3 bucket means that when you upload a file that already exists, the older version is replaced, but you can still view and download it through the version history (see screenshot).Whilst this might be okay for reverting back to the previous version of the code, it's handled on a per-file basis. Now imagine trying to track all the files that make up a version of your code?This is really an argument against version control systems (VCS) and file versioning. VCS was created to simplify change tracking in the source code.Pro-tip: bit rusty on S3? Yup Helen's got you covered again with What's in the bucket?When?SDLC automation~~~~~~~~~~~~~~~~CodeCommit -&gt; ???This is the start of SDLC automation. Everything else hinges on a source code repository. CI can't build and test code until it can pull the code from somewhere.How?This is loosely based around the Getting Started section of the User Guide.The main difference is that I'm going to use the CLI instead of the Web UI to aid in learning these commands too.Create a repo aws codecommit create-repository --repository-name hello-codecommit{    &quot;repositoryMetadata&quot;: {        &quot;accountId&quot;: &quot;123456&quot;,        &quot;repositoryId&quot;: &quot;dc2c437d-e850-4a8b-82e7-7ad85fee99d1&quot;,        &quot;repositoryName&quot;: &quot;hello-codecommit&quot;,        &quot;lastModifiedDate&quot;: 1553613552.003,        &quot;creationDate&quot;: 1553613552.003,        &quot;cloneUrlHttp&quot;: &quot;https://git-codecommit.xxx.amazonaws.com/v1/repos/hello-codecommit&quot;,        &quot;cloneUrlSsh&quot;: &quot;ssh://git-codecommit.xxx.amazonaws.com/v1/repos/hello-codecommit&quot;,        &quot;Arn&quot;: &quot;arn:aws:codecommit:xxx:1234567890:hello-codecommit&quot;    }}We're going to use SSH to communicate with our new repo, so let's set up a dedicated key (hello-codecommit) and upload the public key to our IAM account.ssh-keygen -b 4096 -f hello-codecommit-ssh# output has been omittedaws iam upload-ssh-public-key --user booyaa \\  --ssh-public-key-body &quot;$(cat hello-codecommit-ssh.pub)&quot;{    &quot;SSHPublicKey&quot;: {        &quot;UserName&quot;: &quot;booyaa&quot;,        &quot;SSHPublicKeyId&quot;: &quot;VALUE_OF_SSHPUBLICKEYID&quot;,        &quot;Fingerprint&quot;: &quot;FINGERPRINT&quot;,        &quot;SSHPublicKeyBody&quot;: &quot;REDACTED&quot;,        &quot;Status&quot;: &quot;Active&quot;,        &quot;UploadDate&quot;: &quot;2019-03-26T15:46:14Z&quot;    }}Make a note of your SSHPublicKeyId, we'll need to add new entry to your ssh config (~/.ssh/config):Host git-codecommit.*.amazonaws.comUser VALUE_OF_SSHPUBLICKEYIDIdentityFile /path/to/hello-codecommit-sshLet's clone our empty repo, you can find the Git URL by looking at the output from running aws codecommit create-repository. The key name is cloneUrlSsh.git clone ssh://git-codecommit.xxx.amazonaws.com/v1/repos/hello-codecommitCloning into &#39;hello-codecommit&#39;...# omitting boring git stuffwarning: You appear to have cloned an empty repository.Let's go into our new repo, add a file, commit and push it back up.cd hello-codecommitecho &quot;Hello world&quot; &gt; index.htmlgit add index.htmlgit commit -m &#39;initial commit&#39;git pushNext, let's create a feature branch so we can use the pull request feature.git checkout -b feature/index-markupreplace the contents index.html with the following:&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;utf-8&quot;&gt;  &lt;title&gt;Hello world&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  Hello world&lt;/body&gt;&lt;/html&gt;git commit -am &#39;(feat) use markup&#39;git push -u origin feature/index-markupaws codecommit create-pull-request \\  --title &quot;Feature: use markup for the index page&quot; \\  --description &quot;What: switch from plain text to markup. Why: markup allows for a richer web experience&quot; \\  --client-request-token booyaa-markup-1234 \\  --targets repositoryName=hello-codecommit,sourceReference=feature/index-markupThe client-request-token switch is a token we randomly generated, it's used by the API to ensure request cannot be repeated with a changed parameter. If you run that command again, you get the output from the previous transaction instead of a new pull request.The targets switch requires the repository name (hello-codecommit) and the branch you're raising a pull request for (feature/index-markup).Finally let's teardown aws codecommit delete-repository --repository-name hello-codecommit-ui, you'll get back the repositoryId if you were successful.API and CLI features and verbsFeaturesRepositoryBranchFile/FolderCommit/Differences/BlobPullRequestCommentsTriggerVerbs (CRUD)createbatch-get/get/list/describeupdate/putdeleteOutlierstest-repository-triggersmerge-pull-request-by-fast-forwardAWS DevOps Pro Certification Blog Post SeriesIntroSDLC automationCode CommitCode BuildCode DeployCode PipelineDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series: SDLC automation introduction",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-sdlc-intro",
        "content" : "This is part of the blog post series: AWS DevOps Pro CertificationWhat does the exam guide say?To pass this domain, you'll need to know the following:Apply concepts required to automate a CI/CD pipelineDetermine source control strategies and how to implement themApply concepts required to automate and integrate testingApply the concepts required to build and manage artefacts securelyDetermine deployment/delivery strategies (e.g., A/B, Blue/green, Canary, Red/black) and how toimplement them using AWS ServicesWhat whitepapers are relevant?According to the AWS Whitepapers for DevOps we should look at the following documents:Introduction to DevOps on AWS (20 pages)Development and Test on AWS (17 pages)Practicing Continuous Integration and Continuous Delivery (32 pages)Blue/Green Deployments on AWS (35 pages)Jenkins on AWS (48 pages)What services and products covered in this domain?Code Commit - This service provides Source Control Management (SCM). This is where you store your source code and keep a track of the changes made (commits). Examples of similar services: GitHub, Bitbucket, GitLab, and Azure DevOps. You only have one choice of Version Control Service (VCS) and that is Git.Code Build - This service builds code and produces artefacts. This is often part of a Continuous Integration (CI) pipeline, so this is where tests can be run to ensure the code is stable. Examples of similar services: Travis CI, Circle CI, Azure DevOps and Jenkins.Code Deploy -  This service automates the deployment of your applications or publishes files to a website. This is often part of a Continuous Deployment (CD) pipeline. Examples of similar services: Travis CI, Circle CI, Azure DevOps and Octopus.Code Pipeline - This service provide a workflow, allow for more complex release step. This is often part of a Continuous Deployment (CD) pipeline. Examples of similar services: Circle CI, Azure DevOps and Jenkins. Shrewd readers will have spotted that these are part of Developer Tools suite in the AWS Documentation page. So why have I excluded CodeStar, X-Ray and Tools &amp; SDKs? They're not critical to SDLC automation.What about other types of documentation?If you have the time, by all means, read the User Guides, but they are usually a couple of hundred pages. Alternatively, get familiar with the services using the FAQs:Code CommitCode BuildCode DeployCode PipelineYou're all expected to know the APIsCode CommitCode BuildCode DeployCode PipelineBefore you panic, you'll start to spot a pattern with the API verbs.And the CLI commandsCode CommitCode BuildCode DeployCode PipelineAs with the API, there are patterns to the commands.CI, see what?If you're not familiar with SDLC automation, this involves the automation of the following:build process that compiles the code, or optimises the content for a static site, docker imagerunning tests to verify the build was successfuldeploying the new build to a target e.g. application servers, app store, web server, etc. This can also involve publishing the artefacts from the build process to GitHub Releases page of your repository.Usually, the trigger to initiate these steps is a commit or merge to the master branch of your code.The build and testing stages are often provided by a Continuous Integration service like Travis CI, Circle CI or Jenkins.The deployment stages are handled by you guessed it Continuous Delivery service like Octopus, Azure DevOps (release pipelines) and often services that provide CI will also handle deployment.Next, we're going to look at the Code Commit in greater detail.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationCode CommitCode BuildCode DeployCode PipelineDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "AWS DevOps Pro Certification Blog Post Series",

        
        
        
        "tags": [
        
            "aws",
            
        
            "study",
            
        
            "certification"
            
        
        ],
        "href" : "2019/aws-devops-pro-certification-intro",
        "content" : "This is the start of a blog post series about studying for the AWS DevOps Professional certification.BackgroundFirst a bit of background about me, I'm an engineer at Made Tech who specialises in Site Reliability Engineering. My job is to support our fellow software engineers on two of the three big cloud providers: AWS and Azure. I'm responsible for provisioning the infrastructure that the applications run on, setting up continuous delivery pipelines and application monitoring.This year I decided I want to strengthen my capabilities with AWS and a good way to do this is to become certified as an AWS DevOps Professional.My tried and tested method for learning stuff, is to blog about it which is why I'm creating this blog post series! Also, I'm accountable for my study by journaling the experience.CertificationBefore we begin, let's be clear about which version of the exam we're referring to. This is the certification exam taken after February 2019.The AWS DevOps Professional certification costs 300 USD and takes 170 minutes to complete, with a scoring system of 100-1000 points: you need 750 points to pass.In this version of the exam, they no longer provide the number of questions, but previously it was 80 questions so this works out about 2 minutes per question.The questions in the exam are information dense. There are two twos of question format:Multiple-choice - pick one as the correct answer.Multiple-response - two or more choice to make up the correct answer.If you're wondering how dense, take a look at the free sample exam questions (can't provide an example because it's copyrighted material).Go have a look, and come back when you've done it, I'll wait.Unless you're already a DevOps Pro or used AWS for a really long time you're probably in a state a shock.Whilst we wait for the shock subside, let's cover the areas you'll be tested on.The master of your domainThere are six domains of study, this is the high-level view and we'll go into greater detail in the following posts as part of the series.DomainSubject% of exam1Software Delivery Lifecycle (SDLC) Automation222Configuration Management and Infrastructure as Code193Monitoring and Logging154Policies and Standards Automation105Incident and Event Response186High Availability, Fault Tolerance, and Disaster Recovery16ResourcesLuckily there's a variety of resources to help you study.Your first port of call should be the exam guide aka the blueprint. This guide will give you a list of whitepapers to ready. A detailed breakdown of the topics you're expected to know for each domain (see the table from the previous section).You've already seen the sample exam questions.The Exam Readiness Training is another helpful resource with more sample questions.Knowing when it's time to sit the examI didn't feel particularly confident I would pass the exam cold i.e. no study. The exam costs 350 USD, so I wasn't eager to spend that much money (even if it is covered by my employers) to fail.The next best thing was to try the practice the exam which costs 40 USD which has 20 questions you need to answer within an hour. The practice exam is booked through the AWS Certification site. This is the same method for booking the actual exam, except you can take the practice exam online, whereas the actual exam requires you to visit a test centre.I sat the practice exam and scored 40% :(My study plan is to:create a series of blog postsdo lots of actual practice using the services covered in the exam. I'll be using aws cli, Amazon AMI, MacOS, bash and jq. Reminder: some of the unix commands on MacOS are the BSD variant rather than GNU, but I'll try to limit using variant specific switch where possible.drill against a set of flashcards I'll be creating as part of the study planI'm tentatively giving myself a month to study, spending at minimum two hours a day. After which I'm going to retake the practice exam if I score at least 80% I'll consider booking the exam.So stay tuned.AWS DevOps Pro Certification Blog Post SeriesIntroDomain 1: SDLC automationDomain 2: Configuration Management and Infrastructure as CodeDomain 3: Monitoring and LoggingDomain 4: Policies and Standards AutomationDomain 5: Incident and Event ResponseDomain 6: High Availability, Fault Tolerance, and Disaster Recovery"
    },


    { 
        "title" : "Gatsby and WordPress: Summary",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-summary",
        "content" : "We've reached the end of our blog series, so what have we learnt?Gatsby has a great starter template that is highly performant (gatsby new). Whilst it's React based, the organisation of resources (components, images, pages, templates) made it fairly intuitive to edit or create new items.The build system allows you to get immediate feedback on your changes via live reloading. (gatsby develop)The Gatsby plugin ecosystem is healthy and provides us with many ways to pull in content to our site.The builtin GraphQL explorer (GraphiQL) allow us to query the WordPress data that we pulled in.The Gatsby APIs are powerful, but we didn't get overwhelmed by them as we only needed to know about createPages.Netlify took a lot of the hard work in continuously deploy new versions of our site.Using webhooks we can wire up WordPress and our site on Netlify.There's still so much we haven't covered:Creating pages based on tags, categories, media, users, taxonomies or custom post types.Provide replacements for shortcodes, user comments and search.Image processing (which in turn requires the use of the ACF Entities (Advanced Custom Fields) WordPress Plugin).Authentication to have content for signed up members of your site.Thank you for finishing this series! Please let me know if there are other topics you'd like me to cover around the Gatsby WordPress plugin.The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Yarr! Cutlasses and WebHooks!",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-cutlasses-and-webhooks",
        "content" : "We’re almost finished! All we need to do is get WordPress to tell Netlify when we’ve published any new posts. To do this, we’ll use webhooks.Setup webhook in Netlify (Build &amp; Deploy &gt; Continuous Deployment &gt; Build hooks &gt; Add build hook)Go to your WordPress site’s Admin page (Settings &gt; Webhooks &gt; Add webhook)Action: publish_postFields: ID (this doesn’t matter)URL: paste your Netlify web hookNote: this will only cause a rebuild of the site Netlify for new posts, you need to create a separate webhook event to trigger for pages. Although if it can wait, until your next post. Gatsby will pull in those changes to posts too.You can test this works, by creating a new post in WordPress. You should see a new build job in Netlify almost immediately after hitting publish in WordPress. Once build, you should see your new post!Let’s the click deploy button to publish the site. If there were any errors, check out the logs in the Deploys section of your Netlify site.Bonus material: Have you talked to your kid about PWAs?At no point have we compared how performant our “starter” Gatsby site is. Installing and configuring the Offline site we can beat the default Lighthouse (Google Chrome Dev Tools) rating for a stock WordPress.com site. Let’s do this last thing!Note: this is a copy of the instructions from the Gatsby docs.Install the plugin npm install --save gatsby-plugin-offlineEnable the offline plugin in gatsby-config.js.    {      resolve: `gatsby-plugin-manifest`,      options: {        // details omitted      },    },    &#39;gatsby-plugin-offline&#39;,  ],}Note: the offline plugin must come after the manifest plugin.Finally, add code to notify the user that they need to refresh the page when the service worker has an update:exports.onServiceWorkerUpdateFound = () =&gt; {  const answer = window.confirm(    `This application has been updated. ` +      `Reload to display the latest version?`  )  if (answer === true) {    window.location.reload()  }}Let’s compare Lighthouse audits, first up is our stock WordPress blog:And now our Gatsby site *drum roll*This is pretty amazing, we turned out site into a compliant and highly performant Progressive Web App (PWA) in just a few simple steps. This is largely in part to the excellent boilerplate that you get when you run gatsby new.At last count, there were 86 Gatsby starters created by the team and community. There’s bound to be a starter to fit your needs.If you got stuck, you can check out the following Git hash: b1a4cc77a3d5ff0b0ad364ed5eff57fce30da5cfThe Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Netlify or Die!",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-netlify",
        "content" : "We’re done with our new site, it’s time to setup Netlify.Install gatsby-cli in your repo (netlify needs it to build your site): npm install --save gatsby-cliGet your node version node -v and stick it in .nvmrc to pin netlify to the version you used to build the site (note: don’t include the v in version number i.e. v11.3.0 becomes  11.3.0Complete netlify setup up to step 5, but don’t hit deploy: A Step-by-Step Guide: Gatsby on Netlify | NetlifyExpand advanced build settingsRemember those environmental variables we set up on gatsby-config.js? It’s time add them up in Netlify.Let’s the click deploy button to publish the site. If there were any errors, check out the logs in the Deploys section of your Netlify site.The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Creating WordPress Page Types",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-pages",
        "content" : "WordPress Pages are another Post Type, they differ from Post Post(!) Types insofar that they aren’t time-related or be assigned categories and tags. Generally, they’re used for navigational items like About, Contact or Archive pages.We’re going to update our gatsby-node.js to also pull in WordPress Pages so we can add an About link in our footer.  const createWpPages = new Promise((resolve, reject) =&gt; {    const query = graphql(`      {        allWordpressPage {          edges {            node {              id              slug            }          }        }      }    `)    query.then(result =&gt; {      if (result.errors) {        console.error(result.errors)        reject(result.errors)      }      const pageEdges = result.data.allWordpressPage.edges      pageEdges.forEach(edge =&gt; {        createPage({          path: `/${edge.node.slug}`,          component: path.resolve(`./src/templates/page.js`),          context: {            id: edge.node.id,          },        })      })      resolve()    }) // query.then  }) // createWpPages  return Promise.all([createWpPosts, createWpPages])} // createPagesIt’s almost identical to our createWpPosts bar a few places such as query and template.Here’s the page template src/templates/pages.jsimport React from &#39;react&#39;import { graphql } from &#39;gatsby&#39;import Layout from &#39;../components/layout&#39;export default ({ data }) =&gt; {  const page = data.wordpressPage  return (    &lt;Layout&gt;      &lt;div&gt;        &lt;h1 dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: page.title &amp;#124;&amp;#124; /&gt;        &lt;div dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: page.content &amp;#124;&amp;#124; /&gt;      &lt;/div&gt;    &lt;/Layoqut&gt;  )}export const pageQuery = graphql`  query($id: String!) {    wordpressPage(id: { eq: $id }) {      title      content    }  }`Finally, let’s update our footer (which lives in the Layout component src/components/layout.js)&lt;footer&gt;© 2018, Built with &lt;a href=&quot;https://www.gatsbyjs.org&quot;&gt;Gatsby&lt;/a&gt; | &lt;a href=&quot;/about&quot;&gt;About Us&lt;/a&gt;&lt;/footer&gt;Checkpoint: Let’s talk about usIf we restart Gatsby, and we’ll see a new link in our site footer.Clicking on it takes us to our newly created WordPress Page!If you got stuck, you can check out the following Git hash: ce6cc022a881e813af31279ff857f908ecc599f4The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Creating an index page",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-index-page",
        "content" : "Order order! Let’s create an index to list our postsLet’s visit http://localhost::8000/___graphql to fire up the builtin GraphQL explorer and paste the following query.{  allWordpressPost(sort: {fields: [date], order:DESC} ) {    totalCount    edges {      node {        title        excerpt        slug        date(formatString: &quot;Do MMMM&quot;)      }    }  }}Things to note here how to sort the collection of posts. If you’re wondering what fields are available,  the explorer provides documentation for any plugins you have loaded so in the case of WordPress Post we can explore this clicking on the hover dialogue for allWordpressPost.Let’s update our index page (src/pages/index.js) to list our WordPress posts.First, we import graphql and we can remove our import of Image component.import { Link, graphql } from &#39;gatsby&#39;Then we update IndexPage definition,  we use the map function transform each array item into an HTML fragment containing information about each post.const IndexPage = ({data}) =&gt; (  &lt;Layout&gt;    &lt;SEO title=&quot;Home&quot; keywords={[`gatsby`, `application`, `react`]} /&gt;    &lt;h1&gt;Welcome to the Gatsby demo&lt;/h1&gt;    &lt;h3&gt;There are {data.allWordpressPost.totalCount} posts&lt;/h3&gt;    {data.allWordpressPost.edges.map(({ node }) =&gt; (      &lt;div key={node.id}&gt;        &lt;Link to={node.slug}&gt;          &lt;h4&gt;&lt;span dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: node.title &amp;#124;&amp;#124;/&gt; - {node.date}&lt;/h4&gt;        &lt;/Link&gt;        &lt;div dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: node.excerpt &amp;#124;&amp;#124; /&gt;      &lt;/div&gt;    ))}  &lt;/Layout&gt;)Finally, we add our pageQuery that we tested in our GraphQL explorer at the start of the post.export const pageQuery = graphql`  query {    allWordpressPost(sort: { fields: [date], order: DESC }) {      totalCount      edges {        node {          id          title          excerpt          slug          date(formatString: &quot;Do MMMM&quot;)        }      }    }  }`Checkpoint: Index page rebootedIf we visit http://localhost:8000 we should see a different page to what we started at the beginning of this blog post seriesIf you got stuck, you can check out the following Git hash: 2249ea842a18e4da39c6e3abcf8eeabd78a17116The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Creating Content",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-creating-content",
        "content" : "Pulling in content from WordPressNow that we’ve verified that the plugin can pull in data from our WordPress site, let’s start creating static content based on our posts and pages!Let’s start with something simple and pull in the posts and display them in the developer console. This involves two steps:Create a GraphQL queryAct upon the results of the query being run (in this case we’ll just display the contents of the query)Page creation is handled by createPages API, using promises we can perform the required steps to programmatically create pages. Let’s edit the gatsby-node.js add the following code.exports.createPages = ({ graphql, actions }) =&gt; {  const createWpPosts = new Promise((resolve, reject) =&gt; {    const query = graphql(`      {        allWordpressPost {          edges {            node {              id              slug            }          }        }      }    `)    query.then(result =&gt; {      console.log(JSON.stringify(result, null, 4))      resolve()    }) // query.then  }) // createWpPosts  return Promise.all([createWpPosts])} // createPagesGo ahead and restart gatsby develop, you should see the following in your terminal:success building schema — 0.920 s⠁ {    &quot;data&quot;: {        &quot;allWordpressPost&quot;: {            &quot;edges&quot;: [                {                    &quot;node&quot;: {                        &quot;id&quot;: &quot;51ec1d4e-7b5f-54b8-b5ea-f36aea0c1d8f&quot;,                        &quot;slug&quot;: &quot;a-long-form-post&quot;                    }                },Whilst this is not very exciting, it confirms that we’ve been able to pull our posts from WordPress. It turns out the two fields  id and slug are just what we need to begin programmatically creating pages.Create pages based on WordpressLet’s update our code, for brevity we've omitted blocks of code that haven’t changed if you get stuck there’s a commit hash at the end of this checkpoint.const path = require(&#39;path&#39;);exports.createPages = ({ graphql, actions }) =&gt; {    const { createPage } = actions      const createPostPages = new Promise((resolve, reject) =&gt; {      // omitted        query.then(result =&gt; {        if (result.errors) {          console.error(results.errors)          reject(result.error)        }          const postEdges = result.data.allWordpressPost.edges          postEdges.forEach(edge =&gt; {          createPage({            path: `/${edge.node.slug}`,            component: path.resolve(`./src/templates/post.js`),            context: {              id: edge.node.id,            },          })        })          resolve()// omittedBefore we create any pages, let’s do a bit of error handling and return early from our promise if we hit any errors.Next, we extract our results into a const to make it easier to read the code.const postEdges = result.data.allWordpressPost.edgesYou’ll recall that this is just a flattening of our GraphQL query.{  allWordpressPost {    edges {        ...    }  }}We then iterate over our results calling the createPage API, which expects three parameters:path which will be used to create a page slug, WordPress already has this data, so rather than compute it ourselves let’s reuse it.component is React parlance for a unit of markup. Think of a controls toolbox with various components you can add to a form. We’ll take about this in the next section.context allows us to pass data to the component, in this case, we’ll pass the WordPress Id of our posts.Checkpoint: Create a blog post templateThat’s a lot of information to take in, but we’re almost there!Remember the component parameter from the last section?component: path.resolve(`./src/templates/post.js`),Let’s go ahead and create that file post.js. Note: you’ll need to create a new subdirectory within src called templates.src/templates/post.jsimport React from &#39;react&#39;;import Layout from &#39;../components/layout&#39;;export default () =&gt; {    return (        &lt;Layout&gt;            &lt;div&gt;Hello blog post&lt;/div&gt;        &lt;/Layout&gt;    )}Let’s stop and restart Gatsby:gatsby developWe don’t have an index page to view our newly created post, but the Gatsby 404 development page acts as handy makeshift.Go to a non-existent page: http://localhost:8000/xyzYou should now see your WordPress blog posts alongside other pages that Gatsby knows about.You may have gathered that our current post template is pretty basic, so clicking on any of our posts will just display the same contents. The important thing to observe is that we now have pages for our WordPress posts and the template is being used for each post.If you got stuck, you can check out the following Git hash: 13a036ae2a8dea2ea0f7a910c35c2fe4789f9a50Turning that template into something usefulAs you gathered from the last section, all we’ve managed to do with our template is to generate slugs. The means to pull the content is available, but we need to wire it up. Again we’ll only show you the bits you need to add/modifyimport { graphql } from &#39;gatsby&#39;export default ({ data }) =&gt; {  const post = data.wordpressPost  return (    &lt;Layout&gt;      &lt;div&gt;        &lt;h1 dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: post.title  &amp;#124;&amp;#124; /&gt;        &lt;h3&gt;          date: {post.date} tags: {extractTags(post)}{&#39; &#39;}        &lt;/h3&gt;        &lt;div dangerouslySetInnerHTML=&amp;#123;&amp;#123; __html: post.content  &amp;#124;&amp;#124; /&gt;      &lt;/div&gt;    &lt;/Layout&gt;  )}We can see our function’s signature has changed to allow data to be passed (which will be provided by a GraphQL later on). The key things to notice here is the use of  dangerouslySetInnerHTML and the JSX expressions (curly brackets - looks a bit like moustache doesn’t it?).dangerouslySetInnerHTML is a React’s replacement for innerHTML, why but is it dangerous? Setting HTML from code (or in this case from a database) is risky because it’s easy to expose yourself to XSS). So by using this helper you acknowledging, you’re doing something DANGEROUS, but before we complete freak out, let’s do a quick assessment of the data we’re parsing. Our WordPress blog, with content  we’ve written. So unless we have a serious case of self-loathing, it’s unlikely we’re at risk here. If you do need to parse HTML safely,  there are options like html-react-parserIf the field our post object we’re interested doesn’t have any markup, then we can use expressions to rendered them {post.date}. You can stick any valid JavaScript with these expressions and extractTags is a helpful function to flatten the array of tags for our posts. Here’s the definition of that helpful function (add after our default function).const extractTags = post =&gt;  post.tags ? post.tags.map(x =&gt; x.name).join(&#39;, &#39;) : &#39;none&#39;Almost there, just need to add our GraphQL that will power this template.export const pageQuery = graphql`  query($id: String!) {    wordpressPost(id: { eq: $id }) {      title      tags {        name      }      date(formatString: &quot;Do MMMM YYYY&quot;)      content    }  }`The pageQuery takes the WordPress Post Id we passed in the context when we created our static pages in gatsby-node.js (see below) and allows us to pull a specific WordPress post using this Id wordpressPost(id: { eq: $id }), we don’t need to pull everything we just need the title, tags, publish date and content. Note: we used GraphQL to format the date.createPage({  path: `/${edge.node.slug}`,  component: path.resolve(`./src/templates/post.js`),  context: {    id: edge.node.id,  },})Demo: remember our friend dangerouslySetInnerHTML, you could avoid the risks by using expressions, but as you can see if there’s any markup it will be sanitised i.e. it will be escaped.Checkpoint: let’s see that contentIt’s probably best we restart gatsby to see our changes.If you got stuck, you can check out the following Git hash: 50156723a4b21f08baeece9a5f7cd3936e384ee8The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Setup",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-setup",
        "content" : "This is the start of a blog post series about creating Gatsby site with content pulled in from a WordPress site.SetupCreate a new Gatsby site called “wordsby” gatsby new wordsby, this will create a new site using the starter site.Enter your new site: cd wordsbySetup a new GitHub repo and push your changes up to your repoCheckpoint: start up our site locallyLet’s start our new site up gatsby developOpen up your new site on http://localhost:8000Open gatsby-config.js in your editor and change siteMetadata.title to something different, notice the site changed after you saved the changes?If you got stuck, you can check out the following Git hash: 168483273e38b94dd35dd5063a44c4696c69ea11Setup WordPress and the Gatsby pluginLets setup WordPress, go ahead and sign up for a free WordPress site. It’s fine to uses one of the free non-WordPress.com sites.Setup a new developer app that will allow us to connect to the blog: Developer.WordPress.comMake a note of yourWordPress URLCredentials (your email address and password that you used to create your blog)Client IdClient SecretInstall the WordPress plugin npm install --save gatsby-source-wordpressLet’s edit our gatsby-config.jsAdd the following entry to your plugins section    {      resolve: `gatsby-source-wordpress`,      options: {        baseUrl: process.env.WORDPRESS_BASE_URL,        protocol: process.env.WORDPRESS_PROTOCOL,        hostingWPCOM: (process.env.WORDPRESS_HOSTING_WPCOM === &#39;true&#39;),        useACF: (process.env.WORDPRESS_USE_ACF === &#39;true&#39;),        verboseOutput: (process.env.WORDPRESS_VERBOSE_OUTPUT === &#39;true&#39;),        auth: {          wpcom_app_clientSecret: process.env.WORDPRESS_CLIENT_SECRET,          wpcom_app_clientId: process.env.WORDPRESS_CLIENT_ID,          wpcom_user: process.env.WORDPRESS_USER,          wpcom_pass: process.env.WORDPRESS_PASSWORD,        },        includedRoutes: [          &quot;**/posts&quot;,          &quot;**/pages&quot;,          &quot;**/tags&quot;,        ],      },    },You’ve probably noticed we’ve made use of environmental variables to store WordPress info. This is good practice and prevents accidental leakage of secrets  (storing them in a config file and committing to source control).Let’s test our credentials by adding them to a .env fileWORDPRESS_BASE_URL=xxx.wordpress.comWORDPRESS_PROTOCOL=httpsWORDPRESS_HOSTING_WPCOM=trueWORDPRESS_USE_ACF=falseWORDPRESS_VERBOSE_OUTPUT=trueWORDPRESS_CLIENT_SECRET=xxxWORDPRESS_CLIENT_ID=xxxWORDPRESS_USER=xxx@example.comWORDPRESS_PASSWORD=xxxDon’t forget to include  .env in your .gitignore (the starter Gatsby site adds it by default, but it’s good practice to check it’s not being tracked by Git).Whilst the dotenv module is installed as part of Gatsby it’s not enabled by default, so  find your  module.exports line add this  code above  it:require(&#39;dotenv&#39;).config();Checkpoint: verify WordPress credentialsLet’s restart the Dev environment using gatsby developLet’s go to the graphic explorer http://localhost:8000/___graphql this time. Paste the following graphql query into the left panel and hit the play button:{  allWordpressPost {    edges {      node {        title        excerpt        tags {          name        }      }    }  }}If the plugin has successfully loaded and was able to connect to your WordPress blog you should be able to see data on the right panel.TroubleshootingIf you’ve copied the default WordPress for verbose output then, there the plugin should be in debug mode.  Look for the following warning:warning The gatsby-source-wordpress plugin has generated no Gatsby nodes. Do you need it?This indicates there was an error parsing one of the values you gave in .env.If you see the following:Path: /oauth2/tokenThe server response was &quot;400 Bad Request&quot;Then the username or password may be wrong. Test them by logging into WordPress.com.If you see the following:=== [ Fetching wordpress__ ] ====== [ Fetching wordpress__ ] ====== [ Fetching wordpress__ ] ===I’m currently investigating this, and I believe this could be to do with your email address containing + e.g. joe+wordpress@example.comIf you got stuck, you can check out the following Git hash: 3ee8aae5e0c8d6f45127963f776fd1c9358dd647The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "Gatsby and WordPress: Keeping it cheap and staying in touch",

        
        
        
        "tags": [
        
            "wordpress",
            
        
            "gatsby"
            
        
        ],
        "href" : "2019/gatsby-wordpress-keeping-it-cheap-and-staying-in-touch",
        "content" : "This is the start of a blog post series about creating Gatsby site with content pulled in from a WordPress site.Gatsby is a static site generator based around the JAMStack (JavaScript, APIs and Markup) and uses React and GraphQL to create blazingly fast sites. It’s fast because the sites it creates are static and uses modern web techniques like service worker and webpack. You can pull content from various sources using plugins, once pulled in to Gatsby they can be turned into static assets.By the end of this series you’ll have learnt:How to create a new Gatsby siteHow to configure the WordPress plugin to connect to a WordPress.com blogHow to adapt the starter site to use the newly created WordPress Posts and Pages nodesAutomatically publish to Netlify via GitHubFinally set up the WordPress.com blog to notify Netlify when a new post has been published to trigger a new build of the Gatsby siteThings you’ll need to follow along:A GitHub accountA netlify accountA WordPress.com accountFavourite snacks and beverageWe’ll also assume you understand Git basics, Node (and friends) and are comfortable with the command line. If you get stuck you can always check out my reference site on GitHub. At the end of a section, I’ll stick in the commit hash that closely matches the changes we made.The Gatsby x WordPress Blog Post seriesKeeping it cheap and staying in touchSetupCreating ContentCreating an index pageCreating WordPress Page TypesNetlify or Die!Yarr! Cutlasses and Webhooks!Summary"
    },


    { 
        "title" : "When is a MySQL error not a MySQL error",

        
        
        
        "tags": [
        
            "devops",
            
        
            "dbops"
            
        
        ],
        "href" : "2018/when_is_a_mysql_error_not_a_mysql_error",
        "content" : "I came across this error recently: Mysql2::Error: Can't connect to MySQL server on 'some-db-server.example.com' (113)A quick search on the Internet, resulted in various Q &amp; A sites hinting at a connectivity/routing issue to/from the MySQL server. Whilst this was probably enough information for me to fix, if the problem exists on a 3rd party's infrastructure you want to provide a bit more information.The first port of call was to see if the error code 113 appears in the MySQL reference. You can imagine my surprise when I couldn't find 113 anywhere in this chapter.Luckily there is help available from MySQL in the form of a utility called perror that allows you to look up MySQL error codes.By typing perror along with error code, you'll get the following:$ perror 113OS error code 113:  No route to hostSo the reason we can't find this error in either the Client or Server sections of the MySQL reference manual is that it's an operating system error.The operating system in question is Linux, so we know we're looking for C error number codes (errno.h). If you've got access to the kernel source you can find it in /usr/src/linux-source-&lt;VERSION&gt;/include/uapi/asm-generic/errno.h if you don't have the source installed you can see it see a definition of 113 GitHub:#define    EHOSTUNREACH    113    /* No route to host */So armed with this information, I could contact the 3rd party and ask them to check routing and firewall rules between us and the database server."
    },


    { 
        "title" : "Craft as a Fox",

        
        
        
        "tags": [
        
            "study",
            
        
            "learning"
            
        
        ],
        "href" : "2018/web_app_dev_core_skill_fox",
        "content" : "This is part of my &quot;Blogging my Homework&quot; blog post series, the introductory post can be found here.Caveat emptor: any errors or misunderstanding around concepts will almost certainly be my own rather than my employee (Made Tech). This is after all my own study and mistakes do occur during learning.Updated 26th of September: I passed by Fox assessment!Web App Dev Core Skills Level 2 aka FoxThis is the next level after Hedgehog, the introductory post for this core skill can be found on my previous post called Web App Development Core Skill: Hedgehog - Part One. The specific of Fox can be found on learn.madetech.comUnlike the previous level, this assessment mostly deals with purely the practical side of React development. I say mostly because there is a question around the uses of the key prop.AssumptionsIn this assessment, you will be expected to continue working on your application that was started in Hedgehog.StorybookAs part of my actual assessment, I decided to use Storybook to demonstrate my components whilst also showing the code in my editor. For the blog post, I made a decision to instead use codepen.io, because I felt it was more example to show code examples rather than demonstrate Storybook.But fear, not I will be writing a separate blog post extolling the virtues of Storybook. It's really quite amazing!Getting crafty with ReactThis assessment sees you creating components in your application and you will be assessed on the following:Contains a component without any props or stateContains a use of the key propContains a component which makes use of a function passed in as a propContains a component which renders differently based on state changesContains a component which changes state in response to an onClick eventContains a component which has been styled without global stylesContains a component which renders differently based on the following prop types being passed in:A booleanA stringThe children propThere was a small theoretic piece added the assessment around what the key prop Is used for. We’ll address this when we demo “Contains a use of the  key prop”.Now let’s tackle each part of the assessment as a separate subsection!A word about the example codeAll the example code that follows assumes the HTML has a div with the id of root:&lt;div id=&quot;root&quot;&gt;&lt;/div&gt;The example code snippets you'll see is just the JSX part that creates the component. A full working demo can be found in the codepen link.Contains a component without any props or stateconst element = &lt;h1&gt;Hello, World!&lt;/h1&gt;;ReactDOM.render(element, document.getElementById(&#39;root&#39;));codepen.ioContains a use of the key propfunction NumberList(props) {    const numbers = props.numbers;      const listItems = numbers.map((number) =&gt;      &lt;li key={number.toString()}&gt;{number}&lt;/li&gt;    );    return (&lt;ul&gt;{listItems}&lt;/ul&gt;);}const numbers = [1, 2, 3, 4, 5];ReactDOM.render(  &lt;NumberList numbers={numbers} /&gt;,  document.getElementById(&quot;root&quot;));codepen.ioReferencesLists and Keys – ReactIndex as a key is an anti-pattern – Robin Pokorny – MediumReconciliation – ReactContains a component which makes use of a function passed in as a propSee the example in the next section.Contains a component which renders differently based on the following prop types being passed inA booleanA stringThe children propclass Printer extends React.Component {  constructor(props) {    super(props);  }    render() {    let childCount = React.Children.count(this.props.children);    let message;    if (childCount &lt; 1) {      message = (        &lt;span&gt;js type: {typeof(this.props.value)}&lt;/span&gt;      );    } else {      message = (&lt;span&gt;not a js type: {this.props.children}&lt;/span&gt;);    }    return (&lt;h1&gt;{message}&lt;/h1&gt;);  }}function Func() {  return 1;}function Child() {  return &#39;I am a child component!&#39;;}function App() {  return (    &lt;div&gt;&lt;Printer value=&quot;i am a string&quot; /&gt;&lt;br /&gt;&lt;Printer value={true} /&gt;&lt;br /&gt;&lt;Printer value={Func} /&gt;&lt;br /&gt;&lt;Printer&gt;&lt;Child/&gt;&lt;/Printer&gt;    &lt;/div&gt;  );}ReactDOM.render(&lt;App /&gt;, document.getElementById(&#39;root&#39;));codepen.ioContains a component which renders differently based on state changesThis was shamelessly stolen from the official React docs!class Clock extends React.Component {  constructor(props) {    super(props);    this.state = {date: new Date()};  }  componentDidMount() {    this.timerID = setInterval(      () =&gt; this.tick(),      1000    );  }  componentWillUnmount() {    clearInterval(this.timerID);  }  tick() {    this.setState( {      date: new Date()    });  }  render() {    return (      &lt;div&gt;        &lt;h1&gt;{this.state.date.toLocaleTimeString()}&lt;/h1&gt;        &lt;h2&gt;This component is changing based on it&#39;s state&lt;/h2&gt;      &lt;/div&gt;    );  }}codepen.ioContains a component which changes state in response to an onClick eventclass App extends React.Component {  constructor(props) {    super(props);    this.state = { playing: false };  }  playPause = () =&gt; {    if (this.state.playing) {      this.setState({ playing: false });    } else {      this.setState({ playing: true });    }  };  render() {    const { playing } = this.state;    return (      &lt;div className=&quot;app&quot;&gt;        &lt;button onClick={this.playPause}&gt;{playing ? &quot;Pause&quot; : &quot;Play&quot;}&lt;/button&gt;      &lt;/div&gt;    );  }}ReactDOM.render(&lt;App /&gt;, document.getElementById(&quot;root&quot;));codepen.ioReferences:State and Lifecycle – ReactContains a component which has been styled without global stylesconst divStyle = {  color: &#39;blue&#39;};function App() {  return &lt;h1 style={divStyle}&gt;Inner Style Example&lt;/h1&gt;;}ReactDOM.render(&lt;App /&gt;, document.getElementById(&#39;root&#39;));codepen.ioReferences:DOM Elements – React"
    },


    { 
        "title" : "Being a Hedgehog - Part Two",

        
        
        
        "tags": [
        
            "study",
            
        
            "learning"
            
        
        ],
        "href" : "2018/web_app_dev_core_skill_hedgehog_part_two",
        "content" : "This is part of my &quot;Blogging my Homework&quot; blog post series, the introductory post can be found here.Caveat emptor: any errors or misunderstanding around concepts will almost certainly be my own rather than my employee (Made Tech). This is after all my own study and mistakes do occur during learning.Update 22nd of September: I passed my Hedgehog assessment!Welcome back to part two of my study guide for Web Application Development! In the previous post, we covered the practical side of the assessment, this time we cover the theoretical side.UnderstandingThere are nine areas where you will be assessed (in the form of Q &amp; A) on your knowledge of React, we’ll cover each one as a separate section.As part of your study material for this core skill, the documentation for React is cited as a good source of information especially the Main Concepts section. And whilst not part of the assessment you are expected to understand JSX and Elements.If like me you haven’t touched JavaScript in a while, the React documentation provides excellent references to reacquaint yourself and to understand some of the new syntax:Referenceshttps://developer.mozilla.org/en-US/docs/Web/JavaScript/A_re-introduction_to_JavaScripthttps://gist.github.com/gaearon/683e676101005de0add59e8bb345340cUseful React knowledgeWhilst these are not part of the assessment, you’re expected to have an understanding of them.JSXIs short for Javascript Syntax eXtensions, it produces React “elements”.Important: JSX is closer to JavaScript than HTML, React DOM uses camelCase property naming convention instead of the one used for HTML attribute. e.g. tabindex becomes tabIndexUseful properties of JSX:Prevents injection attacks by escaping any values embedded before renderingRepresents objectsReferencesIntroducing JSX – ReactElementsReact elements are:Are plain objects and cheap to create, unlike browser DOM elements.Are not components (more on this later)Immutable, that is once created, you can’t change its children or attributes.ReferencesRendering Elements – ReactSituations in which using React would be a benefitThe Learn Tech resources provide a good starting point for understanding when we should use React or in fact any type of framework behind just HTML and CSS:Does your application require a lot of client-side business logic?Do you need simple and reusable components?Could your application benefit from selective reloading? I.e. does anything on the page need to react?Is an existing customer development team already using React or Node?Does your application require a lot of DOM manipulation?Would the customer benefit significantly from having fast page load times?The last point I feel relates to the modern framework’s build tools that are optimising pages for faster load times rather than an attribute specific to React.Now take a moment and think about your own idea for an application, if you answered, “Yes” to most of these questions then perhaps React is the best tool.It’s also useful to think about when it would be inappropriate to React, here are some examples I could think of:Does the application remain static after it has been built i.e. a static site generator like Jekyll?Does the application not require any interaction beyond page navigation?We want to use the latest and greatest technology!Javascript is everywhere, so why not just go with the flow!The last two I paraphrased from CSS-Tricks’ excellent When Does a Project Need React? article.ReferencesWeb Application Development with React | Learn Tech by Made TechWhat a React component isComponents allow you to split UI into independent, reusable pieces that can be worked on in isolation.If you’ve ever done Forms or Windows application development, you can think of the controls toolbox as being analogue to components.The React page also provides a simple analogy for components they are like JavaScript functions.Convention: Component names always start with a capital letter, this is because React trees components that start with a lowercase letter as DOM tags.Components can refer to other components for their output. The convention is to create an App component that calls other components. This is important to know for refactoring.ReferencesComponents and Props – ReactWhat are props within a componentIf we continue the analogue of controls for an application, then we can think of props (short for properties) as exactly the same thing. You define the properties of a control.An alternative view again provided by the React page is that “props” are inputs and they return a React element describing what should appear on the screen.Important: Props are read-only if your component modifies it’s own props it is considered “impure”. You cannot break this rule in React.ReferencesProps are Read-Only – ReactWhat is state within a componentStates are similar to props but are private and fully controllable by the component. Recall in the last section that props are immutable that is they are read-only.Local states are only available to JavaScript classes, they will not work for JavaScript functions.States are private and fully controlled by the component.Do not modify states directly, always use setState()State updates maybe asynchronous, this is because React may batch multiple setState() calls into a single update for performance. Use the second form of setState() and pass it a function rather than an object.State updates are mergedReferencesState and Lifecycle – ReactWhat props.children refers toAll the children of a component, children can be:Everything, don’t have to be other componentsTextJavaScript functionsReact says children are an opaque data structure (Reach.children)ReferencesA quick intro to React’s props.children – codeburstReact This Props Children - Learn.coA deep dive into children in React - Max Stoibers BlogHow React allows you to respond to user eventsThey act like event handling on DOM elements, there are syntactic differences:React events are named using camelCase rather than lower case.JSX you pass a function as an event handler rather than a stringYou must call preventDefault explicitly rather than return false.ReferencesHandling Events – ReactOne approach to styling react componentsInline stylingRadiumExternal stylesheetsCSS moduleReferencesStyling and CSS – ReactDOM Elements – ReactStyling ReactStyling React Components | Jake TrentThe component lifecycleLifecycle hooks are used to free up resources components when they are destroyed.Set up (React parlance: mounting) using the componentDidMount methodTeardown (React parlance: unmounting)  using the componentWillUnmount methodLifecycle methods ensure that valuable resources are freed up when a component is destroyed. By defining componentDidMount  (allocates) and componentWillUnmount  (frees) methods. This is probably analogue to programming languages that have a garbage collector.ReferencesAdding Lifecycle Methods to a ClassAn approach to managing Application stateReduxContext API (React)Component state (very basic)JS module singleton patternUnstatedMobXReferencesThe state of the state: React state management in 2018 - DEV Community 👩‍💻👨‍💻The 5 Types Of React Application State - James K NelsonApplication State Management – kentcdoddsManaging React Application State with Mobx — Full stack tutorial (Part 1)Further ReadingLearning/Intro5 Practical Examples For Learning The React Framework - Tutorialzinehttps://medium.com/in-the-weeds/learning-react-with-create-react-app-part-1-a12e1833fdcStep by Step Guide To Building React Redux Apps – rajaraodv – MediumMake a Mobile App with ReactJS in 30 Minutes ― ScotchIntegrations w/ Web PlatformFullstack React: How to get “create-react-app” to work with your Rails APIFullstack React: How to get “create-react-app” to work with your APIVS CodeLive edit and debug your React apps directly from VS Code — without leaving the editor 🔥 🎉🎈React JavaScript Tutorial in Visual Studio CodeDebugging a Create React App with VS Code · ManorismsMiscHow Airtable uses React – Matt Bush – Medium"
    },


    { 
        "title" : "Being a Hedgehog - Part One",

        
        
        
        "tags": [
        
            "study",
            
        
            "learning"
            
        
        ],
        "href" : "2018/web_app_dev_core_skill_hedgehog_part_one",
        "content" : "This is part of my &quot;Blogging my Homework&quot; blog post series, the introductory post can be found here.Caveat emptor: any errors or misunderstanding around concepts will almost certainly be my own rather than my employee (Made Tech). This is after all my own study and mistakes do occur during learning.Web App Dev Core Skills Level 1 aka HedgehogThis core skill is based around as the name suggests Web Application Development. It's focussed around the React framework because of this our chosen framework for front-end work at Made Tech.The first level of the Web Application Development with React core skill primarily deals with setting up a new project that is ready for production use.The core skill assessment is split into two parts: a practical demonstration (Application) and a theoretical (Understanding) in a Q &amp; A format. I have also split the blog posts accordingly so the first part will be the practical demonstration.AssumptionsYou have Node and Docker installed.Our applicationAs part of the core skill, you are expected to work on an application. My choice was to create a guiding breathing app. Why this choice of app? Well sometimes, you need to take a breather when it all gets a bit much.More details of the will follow, but for now, it should be thought of in terms of the following:UIHas a settings panel that allows you:to define intervals between breathing in and out, and holding the breath.set the volume of soundsHas a timer to define how long you wish to have you're guided breathing session.BehavioursHas a countdown timerHas visual and audio cues for when for guided breathingSetupCreate our app using npx create-react-app breatheEnter our app cd breatheStart our app  npm startnpx is a handy way to install and run create-react-app.ApplicationThe practical part of the assessment has six areas where you will be assessed. I've created a header for each area.Has created an application that is independent of the node install on the candidate's machineHere we are being assessed on be able to provide a reproducible build. Docker is our prefer way of doing this, there are other alternatives.FROM node:10WORKDIR /appCOPY package.json ./RUN npm installCMD [&quot;npm&quot;, &quot;start&quot;]The general idea behind this Dockerfile is to use define an image with Node, copy the package.json, install its dependencies and start the application.Tip: During the early stages of prototyping I like to add —verbose to my npm installInfo: I’ve provided more information about the Dockerfile syntax in the references section below.Tip: If your application has many Dockerfiles or related scripts, you may find it prudent to keep application and Docker artefacts separate.Finally, we build and test our dockerized application.docker build -t . &lt;DOCKER_ID/MY_APP&gt;docker run -it --rm -p 3000:3000 -v ${PWD}/public:/app/public -v ${PWD}/src:/app/src &lt;DOCKER_ID/MY_APP&gt; Once the app has finished running, you can open the site using http://localhost:3000Tip: if you’ve done any kind of web development, you’ll have noticed that port 3000 is prevalent. Make sure you flush cookies associated with this address to avoid unexpected behaviour.You’re probably wondering why we’re using volume mounting (-v) this is to enable hot re-loading (which is a requirement).The docker run  feels a bit clunky, so let’s use a docker-compose.yml  file to simplify docker invocation.version: &#39;2&#39;services:  web:    build: .    volumes:      - ./public:/app/public      - ./src:/app/src    ports:      - &quot;3000:3000&quot;Again we’ll re-test.docker-compose build webdocker-compose run --rm --service-ports webReferencesDockerizing a Node.js web app | Node.jsDockerfile reference | Docker DocumentationCompose file version 2 reference | Docker DocumentationHas a make recipe for serve, which runs the application in development modeWhilst command history is great, you want the process of building and starting of your app to be a zero effort. To do this at Made Tech we utilise Makefiles.PHONY:docker-builddocker-build:    docker-compose build webPHONY:serveserve:        docker-compose run --rm --service-ports webIn case you’re wondering why we opted for these target names, (docker_build and serve) it’s because we decided to define a standard for our Makefiles. This is to ensure that any engineer should be able to know without any documentation about how to build, serve and test an application. You can see our standard for Makefiles in our Requests of Comment repo: Makefile Standards.If you’re wondering why we didn’t just stick our targets in package.json, it’s because not all our projects are node based. Whereas a Makefile can always exist in any solution.Again we can test both these targets by the following invocations:make docker_buildmake serveHas a make recipe for test, which runs the tests for the applicationTime to add test to Makefile. Our Makefile now looks like this:PHONY:docker-builddocker-build:    docker-compose build webPHONY:serveserve:        docker-compose run --rm --service-ports webPHONY: testtest:    docker-compose run --rm web npm testTip: If you want a single shot test run i.e you do not want to use the watch process, then add an environment variable called CI with the value true to web  section of docker-compose.ymlTo test our target test we type make testHas a sandbox environment which allows them to quickly prototype componentsFor this requirement, we install Storybook.npm i -g @storybook/clistorybook init # assumes we&#39;re still in our app&#39;s folderWe also add the following scripts to package.json    &quot;storybook&quot;: &quot;start-storybook -p 9009 -s public&quot;,    &quot;build-storybook&quot;: &quot;build-storybook -s public&quot;ReferencesGetting started with Storybook for ReactHas the ability to hot-reload based on changesYou get this out of the box with React, this is done via react-scripts. If you’ve got the started via make serve, you can see this being demonstrated by changing the text in src/App.jsReferencesDisabling opening of the browser in server start · Issue #873 · facebook/create-react-app · GitHubConfigure create-react-app without ejecting ⏏ – Kitze – Mediumcreate-react-app/README.md at master · facebook/create-react-app · GitHubHas a failing test that appears when running make testWe can write a failing test by throwing an exception, which causes the test to fail immediately!"
    },


    { 
        "title" : "Blogging my Homework",

        
        
        
        "tags": [
        
            "study",
            
        
            "learning"
            
        
        ],
        "href" : "2018/blogging-my-homework",
        "content" : "The company I work for Made Tech care about their employees' personal development. They take this matter so seriously that on Friday afternoons, we spend this time honing our skills. To help focus our studies we began working on material that forms the basis of our core skills assessments.The idea behind these core skills is to cover various disciplines that make a full stack engineer. To that end, the current core skills are:Test Drive DevelopmentWeb Applications Development with ReactFrontend Development (coming soon)There are three levels of competencies:Basic/FoundationIntermediateAdvancedI recently passed the TDD core skill level 1 (Giraffe) and I've decided to blog my study to aid my learning. This will form the basis of a blog post series called &quot;Blogging my Homework&quot;.All the learning material as well as the assessment criteria is publicly available at learn.madetech.com.I'd love your feedback and tips, hopefully you can also learn a few things from me too!"
    },


    { 
        "title" : "Building and installing a Ready game on iOS",

        
        
        
        "tags": [
        
            "videogames",
            
        
            "ios",
            
        
            "unity",
            
        
            "ready",
            
        
            "xcode"
            
        
        ],
        "href" : "2018/building-and-installing-a-ready-game-on-ios",
        "content" : "I’ve been playing around with Ready which is a lovely and easy to use tool that allows you to create video games. Ready is aimed at the class room to teach kids about the basics of programming in a fun way, if you’ve seen Scratch, you’ll know what I’m talking about.Ready is made using Unity and also has the ability to export your creations and build them on Unity. Naturally once you can build them in Unity, why not go all the way and deploy them to an iOS device?I should point out that you can play your own creation on an iOS device if you have the Ready app installed. So this is only if you’re interested in customising the behaviour of your Ready game, or perhaps you’ve wanted to know how to build your Unity game for iOS.This blog post assumes you have Ready, Unity and Xcode installed on the same computer.I’ll be using the personal/private use iPhone developer license (free), but the instructions are the same if you’ve got a paid iPhone developer license. I’ve used Xcode 9.4.1 (9F2000)Unity has a free tier, so long as your new game doesn’t exceed 10,000 US dollars in profit you won’t need to buy a  license 😉! I’ve used Unity 2018.1.0f2. I’ve also installed ‎Unity Remote 5 on m iOS device to allow for testing before we export the game to Xcode.Make a game in Ready and export itVideo of Ready GameAs you can see I’ve gone for a really basic animated Sprite with drag behaviour. The game was created for landscape mode. This will be important to know later on.In Ready, click the Gear icon (game settings), then click the  Export to Unity3D button.Create a new folder and select it as export target.Playable in UnityOpen Unity &gt; click on Open  button (next to My Account link) and click into your newly exported Ready game folder, then click Open button. You want to open the project where you can see Assets and ProjectSettings folders.In the Project window.Select the Assets folder.Double click on ReadyPackage  asset to decompress the data.In the Import Unity Package dialogue, click the Import button. This may take a while.Back in the Project window.Select the Scenes folder.Double click on the  PlayerScene asset to load it.In the Unity menu bar, click on  Ready &gt; Recreate Project.Click the Play icon to test the build.Click the Play icon to stop playing.Playable in Unity RemoteIn the Unity menu bar click on  File  &gt;  Build Settings….Click on the  Add Open Scenes buttons.Select iOS as your platform.Click Switch Platform. This may take a while.Close pop up windowConnect your iOS device to your Mac.In the Unity menu bar click on  Edit &gt; Project Settings &gt; EditorIn the  Unity Remote section, select your connected Device from the drop down listStart up the Unity Remote 5 on your iOS deviceClick the Play icon to test the build on your iOS device.Click the Play icon to stop playing.You will notice that the game looks skewed in portrait mode, if you tilt your iOS device into landscape mode you’ll see the game correct it’s scaling.Playable on iOSIn the Unity menubar click  File  &gt;  Build Settings….Select the  iOS  platform.Click the  Player Settings  button.Set your Company name.Set your icon (pick from assets).Set Default Orientation to either Landscape Right or Landscape Left.Click the Build button.Pick target folder and click Save. This will take a whileIn Finder go to the target folder and open Unity-iPhone.xcodeproj, this will launch Xcode.In Project navigator (left most icon [folder]) in picker, select Unity-iPhoneIn the Identity section, update the Bundle Identifier field. Important: this must be a unique.In the Signing sectioncheck Automatically manage signingSelect your Team from drop down listClick on this icon to fix any warningsMake sure your device is connect and has been selected from the drop down listUnlock your iOS device and click Play  icon. This will take a while, you may also see warnings during compilation.Marvel at your new iOS game! Video Game Running on iOSYou may need to trust your personal certificate (they’re renewed every six days) if you see this pop up window whilst deploying the app to your iOS device.In iOS 11 this setting can be found in Settings &gt; General &gt; Profile &amp; Device Management . Select the DEVELOPER APP profile and trust the certificate.ReferencesSpace Chicken - Extra Credit: Exporting the Game to DesktopUnity - Building your Unity game to an iOS device for testing"
    },


    { 
        "title" : "Let's Encrypt certificate renewal without downtime",

        
        
        
        "tags": [
        
            "docker",
            
        
            "nginx",
            
        
            "letsencrypt",
            
        
            "devops"
            
        
        ],
        "href" : "2018/lets-encrypt-certificate-renewal-without-downtime",
        "content" : "Warning: this blog post assumes the following:you are running nginx in a Docker container. Let’s Encrypt has been configured correctly in the nginx container.You are awesome for reading this blog.The default behaviour of certbot (Let’s Encrypt’s command line tool) is to restart the web server. This isn’t desirable in a live environment, ideally you want your web server to reload it’s configuration. For nginx, this involves sending a signal to the process, in this case it’s HUP (hangup).But how can you tell that your certificates have been renewed?The recommended way by NGINX (the organisation rather than the web server) is to check the PIDs (Process Ids) before triggering nginx to reload the configuration.docker top &lt;NGINX_CONTAINER_ID&gt; axw -o pid,ppid,command | egrep &#39;(nginx|PID)&#39;PID                 PPID                COMMAND2089                31208               tail -f /var/log/nginx/access.log3509                31222               nginx: worker process31222               31208               nginx: master process nginx -g daemon off;The PID you want to observe is nginx worker process (COMMAND) which is in this example is  3509.Pro-tip: You can pass docker top  subcommand ps flags? Neat huh?Now let’s send a HUP signal to the container to force nginx to reload the configuration: docker kill —signal  HUP &lt;NGINX_CONTAINER_ID&gt;Then re-check PIDsdocker top &lt;NGINX_CONTAINER_ID&gt; axw -o pid,ppid,command | egrep &#39;(nginx|PID)&#39;PID                 PPID                COMMAND2089                31208               tail -f /var/log/nginx/access.log3643                31222               nginx: worker process31222               31208               nginx: master process nginx -g daemon off;The PID of the nginx worker process has now changed to 3643!Further reading:Controlling nginx"
    },


    { 
        "title" : "Version Management for Ruby, Python, Node and Rust",

        
        
        
        "tags": [
        
            "rust",
            
        
            "ruby",
            
        
            "python",
            
        
            "versionmanagement"
            
        
        ],
        "href" : "2018/version-management-ruby-python-node-rust",
        "content" : "Here's a handy cheat sheet if you find yourself needing an exotic version of Ruby, Python, Node or Rust. Other version management tools are available for ruby, python and node, I just happen to like these ones.ActionRubyPythonNodeRustList available versions to installrbenv install --listpyenv install --listnvm ls-remoten/aInstall specific versionrbenv install 2.5.1pyenv install 3.6.6nvm install v10.9.0rustup use nightly-2018-08-01List locally installed versionsrbenv versionspyenv versionsnvm lsrustup showPin a project to a versionrbenv local 2.5.1pyenv local 3.6.6echo v10.9.0 &gt; .nvmrc ; nvm userustup override nightly-2018-08-01Set global versionrbenv global 2.5.1pyenv global 3.6.6n/arustup default nightly-2018-08-01Python virtual environmentsThis assumes you've pinned your project to a specific version of python.# create an virtual environmentpyenv virtualenv thingy# activate!pyenv activate thingy# do your thang!pip install pylint black pytest# exit virtual environmentpyenv deactivateRust components (standard libraries, RLS, clippy)Components will install for the active toolchain (stable, nightly, beta)To install RLS: rustup component add rls-preview rust-analysis rust-srcTo install clippy: rustup component add clippy-previewRust docsDid you know you always get an off-line copy of the Rust documentation (language reference, standard library and the Rust book) suite when you install a toolchain? To open it for the active toolchain: rustup doc"
    },


    { 
        "title" : "How to run Rust in OpenFaaS",

        
        
        
        "tags": [
        
            "openfaas",
            
        
            "faas",
            
        
            "functions",
            
        
            "serverless",
            
        
            "rust"
            
        
        ],
        "href" : "2018/run-rust-in-openfaas",
        "content" : "I’ve been getting into Kubernetes in a big way, this is partly thanks to it being bundled in Docker for Mac Edge edition.Once I'd learnt the basics via Kubernetes by Example, I wanted to learn a bit more about the specifics of the Kubernetes that is bundled with Docker. I found Alex Ellis' blog post incredibly helpful.I also credit this blog post for getting me into OpenFaaS and Serverless Functions.This blog post expands on a tweet I wrote last sunday:Okay time to end this hacking sesh, with Rust in @openfaas! Code is very rough, needs cleaning up before I offer it up as a contribution. I plan to write a blog  post during the week about how I got this working! #rustlang #Serverless #k8s pic.twitter.com/jOSRum8gfa&mdash; ɥǝɯ uıɥsnɯs sı ɥǝddnd loɯs dlɐɥ (@booyaa) July 29, 2018I had just created a proof of concept Rust function template for OpenFaas.Since then I've been able to clean up the template and come up with a more substantial example. At the time I tweeted my success, Alex (OpenFaaS maintainer) told about Erik Stoekl's version. It's reassuring to see we didn't differ too greatly.What are FunctionsFunctions in the context of Functions as a Service, or Serverless Functions, becomes the next level of abstraction after Platforms as a Service (PaaS). If you think of PaaS as a way to simplify the software release process: you make changes to code, you merge them into your master branch, PaaS handles packaging, delivery of the software as well as providing servers tailored to your application.Functions take this abstraction further, but removing concerns around how your software interacts with its environment. You no longer consider issues of RBAC, Presentation (webserver, stdio), you just write the minimum amount of code to get things done.The best summation of what FaaS and to an extent what Microservices are was coined by Ian Cooper. To paraphrase him: Your unit tests are your functions. You do not have concerns about things outside of the function's task.What is OpenFaaSOpenFaaS is an open source implementation of Function as a Service (Serverless Functions, microservices) that you can self host. Rather than list all the various offerings in this space, I'll refer you to the Cloud Native Computing Foundation, in particular the interactive Landscape.You can either deploy existing functions or create new ones. If you create new ones, there's a big list of officially supported languages. Alternative you could turn a CLI into function.Once I'd given Python and Ruby a go as an introduction, I wanted to see how easy it would be to create a Rust template.Anatomy of an OpenFaaS templateA template requires the following:a script/program to act as a driver for the function. This code will not be seen or used by end users using your tempatea script/program with a handle function that will provide the boilerplate for your end usersa Dockerfile that will bundle and build the driver and function, as well as installing a watchdog process (more on this later).a template.yml which at minimum should provide the programming language of the template and the code to start the handler.Decisions behind the design of the Rust templateTo create a Rust template, I created two crates (we'll discuss the functionality later on):the main crate acts as the driver for the functionthe function crate contains the library with the handle functionI used cargo new to create the new crates and to provide all the necessary plumbing required for a Rust project.Important note about the function crate. OpenFaaS expects to find a folder called function, if you call it anything else it will not copy the boilerplate code when a new function is created using the template.The main crate pulls in the function crate as a dependency using a relative path.Cargo.toml (some sections omitted)[dependencies]handler = { path = &quot;../function&quot; }For the Dockerfile, I went for the official stable image of Rust.The flow of functionThe driver reads standard input and passes to the function.use std::io::{self, Read};extern crate handler;fn main() -&gt; io::Result&lt;()&gt; {    let mut buffer = String::new();    let stdin = io::stdin();    let mut handle = stdin.lock();    handle.read_to_string(&amp;mut buffer)?;    Ok(())}I copied this code from the standard library documentation.The function parses the input and returns its output to the driver.pub fn handle(req : String) -&gt; String {    req}Demonstrationfaas-cli template pull https://github.com/booyaa/openfaas-rust-templatefaas-cli new trustinrust --lang rustHere's what our folder structure looks like..├── trustinrust│   ├── Cargo.toml│   └── src│       └── lib.rs└── trustinrust.ymlYou want to edit trustinrust.yml and update the following values (other parts of the code have been omitted):provider:  gateway: http://127.0.0.1:31112functions:  shuffle:    image: DOCKER_ID/shuffleThe gateway assumes you're running on port 8080, kubernetes runs on a different port. I also found the containers couldn't find the local repository so adding your docker id to the image ensures it uploads to Docker Hub instead.The trustinrust folder contains the boilerplate code from the function crate.For our demo, we're going to make a function that shuffle a list of items that are comma separated.We'll add rand crate to our dependancies in the Cargo.toml. As you can see we're pretty much writing standard Rust, add in libraries as we need them.[dependencies]rand = &quot;0.5.1&quot;Here's the code that's going to go in src/lib.rs (apologies for my awful Rust)extern crate rand;use rand::{thread_rng, Rng};pub fn handle(req : String) -&gt; String {    let mut rng = thread_rng();    let split = req.split(&quot;,&quot;);    let mut vec: Vec&lt;&amp;str&gt; = split.collect();    rng.shuffle(&amp;mut vec);    let joined = vec.join(&quot;, &quot;);    format!(&quot;{:?}&quot;, joined)}If you wanted, you can run cargo build to see if the code will build. If you had tests, this would be testable via cargo test.Now we can build, push the image to Docker Hub and deploy to OpenFaaS.faas-cli build -f trustinrust.ymlfaas-cli push -f trustinrust.yml    # pushes image to dockerfaas-cli deploy -f trustinrust.ymlFinally we can test our function!curl -d &#39;alice,bob,carol,eve&#39; \\&gt; http://localhost:31112/function/trustinrust&quot;bob, alice, eve, carol&quot;SummaryIt was a lot of fun experimenting with serverless functions and Rust. I can see the appeal of serverless functions, reducing functionality to the absolutely minimum. I think my next foray into this space is to see if I can convert my Slack slash commands into serverless functions (they're currently hosted on Heroku).If you have to go for yourself, the template and demo function can be found below:Template: github.com/booyaa/openfaas-rust-templateTrust in Rust function: github.com/booyaa/trustinrustFurther readingIf you want to learn more you should look out for videos by the Kubernetes Legend Kelsey Hightower. I can highly recommend Kubernetes For Pythonistas, which features the now-famous Tetris analogy for DevOps. Also seek out Alex Ellis on the youtubes and his excellent collection of Docker, Kubernetes and OpenFaaS videos."
    },


    { 
        "title" : "A sketchpad project based on Sinatra, Nginx using docker-compose",

        
        
        
        "tags": [
        
            "sinatra",
            
        
            "nginx",
            
        
            "docker-compose"
            
        
        ],
        "href" : "2018/sketchpad-project-sinatra-nginx-docker-compose",
        "content" : "I recently came across a monolithic application at work that used various frameworks, which in turn made extending existing routes very difficult to implement.The general consensus amongst my peers was to stick a proxy in front of the application. The only decision left was to determine if we could do this using the existing reverse proxy (Nginx), or write a bespoke proxy (between Nginx and the existing monolith).The requirement for the proxy was to take a value in the URL, and pass it on as a request parameter to the monolith.An example incoming URL might look like this /api/parp/foo. We want this URL to be rewritten to the monolith as /api/foo and parp to be appended to the request parameter as value for the key fart_noise. The only liberty I've taken with the solution is that the value of fart_noise will always be parp in this scenario.You can do this easily enough in Nginx using proxy_set_body directive.In addition to this requirement, I needed to leave incoming URLs without parp as-is, this is because we expect the client to provide an alternative value for fart_noise i.e. toot.I created a test server to simulate the monolith using Sinatra. Which is great for creating REST based APIs, I think it actually edges Flask out for simplicity.I used the namespace directive to add prefix /api to the existing end points i.e. /api/foo. I only needed to prove the URL rewrites would work for GET and POST methods, so I echoed the params variable back as a response which allowed me to verify the parameters were being modified correctly.class MyWay &lt; Sinatra::Base  register Sinatra::Namespace  namespace &#39;/api&#39; do    get &#39;/foo&#39; do      logger.info &quot;api/foo parameters: #{params}&quot;      params.inspect    end    post &#39;/bar&#39; do      logger.info &quot;api/bar parameters: #{params}&quot;      params.inspect    end  endendThe Nginx config is a fairly standard reverse proxy setup. The only modification that I did was to created two location blocks: one to handle the rewrite and appending a parameter to the request, and the other for URLs that did not need modification.server {    listen 80;    location /api/parp { # fix parps        rewrite (.*)/parp/(.*) /$1/$2 break;        proxy_set_body $request_body&amp;fart_noise=parp;        # other proxy directives...    }    location /api { # leave as-is        # other proxy directives...    }}Obviously I didn't come up with this conclusion immediately, I needed some kind of sketchpad / scratch space to try out the various ideas for rewriting the requests. Enter docker-compose, a handy way to join a bunch of containers together without having brain meltdown from remembering the various incantations that are the individual docker commands.Here's the docker-compose.yml I used:version: &#39;3&#39;services:  app:    build: ./app    command: [&quot;bundle&quot;,&quot;exec&quot;,&quot;rackup&quot;,&quot;--host&quot;,&quot;0.0.0.0&quot;,&quot;--port&quot;,&quot;4567&quot;]    volumes:      - ./app/:/app  proxy:    image: nginx    command: [&quot;nginx-debug&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]    volumes:      - ./proxy/nginx.conf:/etc/nginx/nginx.conf    ports:      - 8080:80    depends_on:      - appThe items you may find useful are:the proxy (Nginx) is dependant on app (Sinatra) starting up first we're using volumes to get the nginx container to read our nginx.conf, removing the need for a pointless Dockerfile that copies this file.You can see the whole project on GitHub.Shrewd readers will notice (in the repo) that parp isn't the item in the URL that we wanted to change. The problem was actually that there are different versions of the data set. Some view this as a change to the API therefore requiring a version tag in the URL. Others do not, which is why version was being passed as a parameter."
    },


    { 
        "title" : "Mozilla Global Sprint",

        
        
        
        "tags": [
        
            "rust",
            
        
            "mozilla"
            
        
        ],
        "href" : "2018/2018-0005-mozilla-global-sprint",
        "content" : "I wanted to share my experience of participating at Mozilla's Global Sprint 2018. The Global Sprint runs once a year and is a two day hackathon event. The projects you can work on, have a central focus of promoting a healthy Internet.At last count there were 145 projects that you could participate in during the sprint, including a certain project called: Content-o-Tron, but more on this later!Open Leaders ProgrammeA significant amount of the projects that were submitted for this year's sprint came from an excellent programme run by Mozilla called Open Leaders. This is a 14-week online course on working open, which will help you take an idea and turn it into a project that is ready to accept contributors from all over the world.I've participated in lots of Open Source projects, and even created a few of my own. So you would think I would be well versed in &quot;Working Open&quot;, but this course has taught me so much more!I plan to write a separate blog post about the programme, but I will say that if you are looking to harness the power of crowd sourcing, I recommend that you apply when applications are accepted in 2019!Content-o-TronContent-o-Tron is the evolution of an idea that I submitted as part of my application for the Open Leaders programme. Whilst on the programme, I have watched my project evolve from something that would generate more &quot;Rust&quot; content into a toolkit that can be used to grow diversity and inclusivity in communities.My personal ambition is for this project to be a stand alone resource that can referenced like Open Leaders or RailsBridge.A Sprint Finish for Content-o-TronPre-sprintA few weeks before the sprint, I started creating tasks (GitHub issues) labelled as . Previously these would've been tasks that I would fixed myself (fix typos, or write introductory text), but it was time to start sharing the work.I also spent a lot of time spamming twitter and keeping a thread of all related #mozsprint tweets.Sprint-eveSupport from friends and family is incredibly important. I had a big confidence boost, when my friend Florian told me he would be checking in occassionally and helping out with a few of the tasks!Sprint DaysI was planning to use IRC as a way to coordinate with contributors in real time (versus commenting in GitHub Issues). In the end I decided to use gitter, because I felt it would be easier to setup (it's web based) and has a good initial user experience. Also most of the sprint participants would already have GitHub accounts (which is a single sign on option in gitter).I also created a tracking issue for the Global Sprint. This is an issue where people go to register their intent to help out on the project. I then used bit.ly to create a memorable URL to this tracking issue  (bit.ly/cot-sprint-18) so it would be easier to share out during our video call &quot;pitches&quot;.It wasn't long before people started to contribute, it was interesting to see the team dynamics change as more people started to join. I like to think that because myself and Florian greeted people (in gitter) and responded to queries in a timely manner this set the tone for collaboration.Once someone had contributed to the project I immediately made them a maintainer of the repository. I wanted to remove any potential barriers, that could prevent them from getting things done. This had the added advantage of making more reviewers for PRs.When I resumed work on day two of the sprint, it was thrilling to see that people had continued to work on the project overnight. This meant I had provided enough work and that the instructions were clear.Here's a snapshot of the activity on the project during the sprint. I couldn't capture the activity for the two days, so this may have included some last minutes fixes on Sprint-eve.I did some further analysis and concluded the following occurred during the sprint.11 out of 12 pull requests closed.9 out of 23 tasks (issues) closed that were labelled .  (excluding the tracking issue)10 issues labelled as . .4 out of 10 issues closed that were labelled .  (did not require software development).2 issues that were labelled .  that required development. One of the issues (curation tool) is pending peer review, so could potentially be closed soon.ThanksI'd like to thank the following people in making this such an awesome &quot;Global Sprint&quot; experience:The Mozilla Global Sprint Team for an awesome event.The Mozilla Open Leaders Team and the cohort hosts and guest speakers. You helped me turn an idea into something that I hope will help lots of people.The Content-o-Tron contributors who helped make this a truely Open project!"
    },


    { 
        "title" : "Tunnelling TCP over DNS in 2018",

        
        
        
        "tags": [
        
            "tools"
            
        
        ],
        "href" : "2018/2018-0004-tunnelling-tcp-over-dns-in-2018",
        "content" : "I wrote this article after seeing that no one had written anything about tunnelling tcp traffic over dns since 2016. A common use of this type of tunnelling is to gain free Internet access by tunneling through a WiFi captive portal. I'd also say it's probably a good idea to get some practice before similar types of network restriction are imposed by your ISP or the state.Just like stateful packet inspection will reveal ssh tunneling over https, this is by no means a method of concealment so caveat emptor! My choice of tunneling tool is iodine. The code can be found on GitHub, the site is hosted on code.kryo.se.The setup hasn't changed along with the current version (0.7.0). Important thing to note here is that you should always have parity with the client and server version of iodine.Setting up iodine:Provision a server (via DigitalOcean et al).install iodine (ubuntu/debian) apt-get install iodinerun as a service iodined -f -c -P f00b44 10.0.0.1 t1.yourdomain.tldnote 1: f00b44 is your shared secret between client and servernote 2: just as you would with a vpn, you create a private network for your tunnel, in our case we're using the note 3: here's my DigitalOcean referral link if you're feeling particularly generous.Create DNS records to point to your servert1ns A your_vps_ipt1 NS t1ns.yourdomain.tldtip: I use cloudflare to manage my dns, which means my DNS records propagate fairly quickly across the Internet.Install the client (mac OS)git clone https://github.com/yarrick/iodine.gitlscd iodine/lsmakemakePREFIX=/usr/local make installConnect the client to the serversudo /usr/local/sbin/iodine -d utunX -f -P f00b44 t1.yourdomain.tldnote: you need sudo to use the tunnelling network adaptor.Setup verificationTo verify setup we'll setup a ssh as a SOCKS proxy ssh -D 9999 10.0.0.1.note: we're setting up the the SOCKS proxy via our server's ip address.And then use httpbin to verify our ip address curl --socks5-hostname 127.0.0.1:9999 -L http://httpbin.org/ipWhen tunnelling you got expect a considerable loss of bandwidth, I'd only recommend using ssh over this type of tunnelling, it's just too slow to do anything else useful.random-tip: &lt;enter&gt; ~. to escape hung ssh sessions"
    },


    { 
        "title" : "Open source tools and work proxies",

        
        
        
        "tags": [
        
            "work",
            
        
            "tools"
            
        
        ],
        "href" : "2018/open-source-tools-and-work-proxies",
        "content" : "I often use a lot of open source tooling at work, initially I started with with node and npm (for our front end), and more recently python, Go and of course Rust.Unfortunately a lot of tools, expect you to have direct access to the Internet, if you're behind a work proxy most will fail to connect to their registries or pull down code from source control repositories.To make matters worse, work proxies often require Windows authentication. So rather than stick your credentials in a plain text file, you might prefer to use something like Fiddler. Fiddler is an excellent diagnostics tool for troubleshooting web apps, a bonus feature is that it also provides a local proxy (usually listen on port 8888), using your existing browser proxy settings.After that, it's just a case of configuring your tools to use this local proxy. Just be mindful that if you can't access websites like GitHub or npm in your browser, you won't magically get access this way, those restrictions will remain in place.Here's a list of ways to get your open source tool to work with this local proxy:Environment variables (env vars)Have these setup as a minimum, most tools will look of these env vars and should start working.HTTP_PROXY=http://127.0.0.1:8888HTTPS_PROXY=http://127.0.0.1:8888This works with rustup,npmnpm config set proxy http://127.0.0.1:8888npm config set https-proxy http://127.0.0.1:8888cargoedit %USERPROFILE%\\.cargo and add the following:[http]proxy = &quot;http://127.0.0.1:8888&quot;gitgit config --global http.proxy http://127.0.0.1:8888This should fix problems with golang and possibly cargo (when pull dependencies).python (Anaconda)edit %USERPROFILE%\\.condarc and add the following:channels:- defaults# Show channel URLs when displaying what is going to be downloaded and# in &#39;conda list&#39;. The default is False.show_channel_urls: Trueallow_other_channels: Trueproxy_servers:    http: http://127.0.0.1:8888    https: http://127.0.0.1:8888vscodeedit your user settings file and add  &quot;http.proxy&quot;: &quot;http://127.0.0.1:8888&quot;,  &quot;http.proxyStrictSSL&quot;: false,"
    },


    { 
        "title" : "Rust2018 - A year of talks",

        
        
        
        "tags": [
        
            "rust",
            
        
            "talks",
            
        
            "community",
            
        
            "meetups"
            
        
        ],
        "href" : "2018/rust2018-a-year-of-talks",
        "content" : "Background: This my blog post for the Rust team's request for community blog posts.Before I address my hopes for Rust in 2018, I thought I'd look back at a year of meet ups that we've organised for London's Rust user group.meet_ups.len() // The Numbers Don't LieWe had 7 talks with an average RSVP of 56 people per meet up. There were 6 &quot;Hack and Learn&quot; events (where we work on Rust together) with an average RSVP of 30 people per meet up. Finally we hosted our year end &quot;Show and Tell&quot; (lightning talks, where no demo is too small) with 22 RSVPs. That's 14 events, which given that we ran out of speakers (more on this later) was an amazing achievement!Furthermore I feel we gave our user group a choice to either listen to talks about Rust, or what I'm sure is preferable to most, to hack on Rust!I'm particularly fond of the &quot;Show and Tell&quot; because it's a relaxed format that allows people to try things out. We've had a few people go on to do main talks, so I feel it's a confidence booster.str::replace(&quot;These are a few of my Favourite Things&quot;, &quot;hing&quot;, &quot;alk&quot;)I've loved all the talks that I've heard this year, but here are a few that stood out.Jonathan Pallant (JPster) - Embedded Rust Talk (video) - This was my first example of Rust running on embedded hardware (vs talking to a server via serial). This has so far been our only talk involving physical computing, I hope it's not the last!Niko Matsakis - Compiler Internals (video) - Not our first remote talk, but certainly the most exciting because it was provided by a member of the core team. Trivia: this wasn't the first time Niko had given a talk to our group. Over a year ago he along with a sizable contingency of the core team descended upon our humble group and gave us a serious overload of Rust! You can see those videos here. Pete Hayes - Intecture 2: Tokio Drift (video) - In 2016 (video) Pete introduced us to his product Intecture. In 2017 he came back to show us his progress, which was pretty exciting, and included replacing a lot of zeromq with Tokio!Lessons learntI need to improve my time keeping; we've been close to being kicked out of the venue because we've run so late. So to address this I plan to nominate someone as time keeper.Our remote speaking set-up isn't ideal. I'd love to use Mozilla's Vidyo, but licenses don't extend to meet ups. Also Google hangout's UI is so counter-intuitive (don't @ me) that frequently new users end up not screen switching and you spend the first 5 minutes with a very personal face to face chat.I'm going to try Zoom, but if you have something you swear by that ideally allows 2 way video and screen sharing, drop me a note!On the subject of remote speakers, if you're lucky enough to have excellent A/V team who provide 2 way audio this is almost as good as having your speaker there with you. If we hadn't made use of remote speakers, I'm certain we'd have fewer meet ups in 2017. Turns out even the capital only has a finite number of people who can and want to talk about Rust!So what's in store for the talks meetup in 2018?My overarching goal is to encourage female and non-binary attendees to come to our meetups. Currently the percentage is under 3%. It would be great to get to 51%, to prove that the Rust community is as welcoming and supportive as we'd like to be.I'm going to be relying on the kindest of friends to help me achieve this, and I'll follow this post up with an outline of the actual plan.My stretch goals are:Produce more home grown speakers i.e. from our attendee list.Try out the Fishbowl (thanks Florian) and Open Space format meet ups.Find a better way to solicit feedback about the events, speakers and suggestions.ThanksBefore I end this blog, I'd like to thank the following group of people without whose help these meet ups wouldn't happen.Christian, my fellow organiser, who single handledly took on the Hack and Learn meet ups and has had equally great success.Skillsmatter for hosting us and providing video recordings. I'd like to single out the A/V team in particular who were unfazed by any of our display connectivity issues and remote talk quirks.Speakers for taking the time to talk about your passion project.The attendees for being the vital ingredient to make a meet up successful.You're still reading?Then why not enjoy our recorded talks from 2017!February Jonathan Pallant - Embedded Rust Talk (video)May - Blockchains (only themed event) Parity Tech - Building blockchains at Parity Technologies (video)MaidSafe - Building the SAFE Network in Rust (video)JulyMe - Generator X: The State of Rust Static Site Generators (video)AugustAidan Hobson Sayers - Rewrite it in Rust? Some experiences in journeys from C and C++ (video)Me - Rust Language Server And You! (video)SeptemberNiko Matsakis - Compiler Internals (video)Amanieu d'Antras - Intrusive collections for Rust (video)David Harvey-Macaulay - three-rs: High-level 3D in Rust (video)OctoberDavid Dawson - Building Message and Event based APIs using Muon - Rust Edition (video)Diane Hosfelt - Attacking Rust for Rust for Fun and Profit (video)Pete Hayes - Intecture 2: Tokio Drift (video)NovemberApoorv Kothari - Ownership, the Core Principal of API Design (video)Comments or feedback?Here's the URLO thread to do this.FINhonest."
    },


    { 
        "title" : "Flocking shell",

        
        
        
        "tags": [
        
            "til",
            
        
            "flock",
            
        
            "linux"
            
        
        ],
        "href" : "2018/flocking-shell",
        "content" : "Yesterday, I had an interesting problem. My cron task spawned hundreds of copies of itself because it was blocking on a database call. If a process spawns enough times, you'll eventually run out of file descriptors and will be unable to fork more processes. To avoid further repeats, I needed to add a check to see if the script was already running and exit early.My requirements for the script in question, also requires that it be able to spawn a specific instance. Instance in this case, could mean connecting to a different database. The important takeaway is that each instance, must be allow a spawn single copy of itself.I could've gone down the route of using creating a PID or lock file (storing the current process id of the script), checking if the current process and the PID file matched and exiting if not.Instead I fancied trying something different and according to StackOverflow flock was a popular choice.Here's a snippet of how to enable file locking in your scripts.# how to allow the script multiple times for different instancesreadonly LOCKFILE=&quot;${LOCKFILE_DIR}/${PROGNAME}-${INSTANCE}.lock&quot;# to avoid command block, link file descriptor (auto incremented) to our lock fileexec {lock_fd}&gt;&quot;$LOCKFILE&quot;# early exit if instance is already runningflock -n ${lock_fd} || exit 1The funny notation {lock_fd} is an auto-incrementing named file descriptor which doesn't appear until bash 4.1.x.x (so you're out of luck Mac users). To add the Mac woes, flock isn't bundled with Mac, but someone's created a cross platform version with the same name.To prove my script no longer spawned multiple copies I wrote the following script (safe-driver.sh):#!/bin/bashclearfor i in $(seq 3)do     (         echo &quot;&gt; BEGIN FOO $i&quot;        safe.sh FOO        echo &quot;&gt; END FOO $i exit code: $?&quot;    ) &amp; doneif [ ! -z &quot;$IN_DOCKER&quot; ]; then    sleep 1 # allow scripts to run (needed for docker)fiprintf &quot;\\n\\njobs running (should only see one process running)\\n&quot;jobs -lprintf &quot;\\n\\nlist file locks\\n&quot;lsof /tmp/safe*.lockif [ ! -z &quot;$IN_DOCKER&quot; ]; then    printf &quot;\\n\\npausing, press any key to return early\\n&quot;    read -rfiReferencesElegant locking of bash program blog post - I cribbed the idea of not running flock as a command block from Kfir's post, but I drew the line with how the code was organised. Where possible I try to avoid imposing coding style from other languages. I also still think bash can be consumed by two parties operational staff and developers, I would prefer to cater for ops since they usually end up looking after these scripts.&lt;/soapbox&gt;exec examples from bash-hackers.org - This was my first time to use exec in anger and I think it helped me understand the role the file descriptor played in my flock script.Advanced Bash-Scripting Guide (Special Characters - This is my goto resource for searching for various symbols and glyphs often used blindly in bash. In particular I used this to find out the proper name for () (command block).UpdatesYou may have seen an earlier post (on the 10th), which I withdrew because I didn't feel I had solved the problem sufficiently and there was a misunderstanding of how automatic file descriptor allocation works."
    },


    { 
        "title" : "Porting python turtle examples to turtle.rs",

        
        
        
        "tags": [
        
            "turtle",
            
        
            "rust",
            
        
            "python"
            
        
        ],
        "href" : "2017/porting-python-turtle-examples-to-turtle-rs",
        "content" : "I've been tinkering around with the Rust version of Turtle graphics. Turtle graphics, was a key feature of the programming language Logo, and has frequently been ported to other programming languages as a visual way to teach programming.I read somewhere that the author Sunjay Varma of Rust port of Turtle was inspired by python's own module. There's a few minor cosmetic changes pendown() vs pen_down(). So I decided to have a go at running existing python examples and I came across this code from the module documentation.from turtle import *color(&#39;red&#39;, &#39;yellow&#39;)begin_fill()while True:    forward(200)    left(170)    if abs(pos()) &lt; 1:        breakend_fill()done()The only item that threw me was the abs(pos()). pos() is an alias to the position method which returns the cartesian coordinates (x,y). I figured some form of special handling was taking place.A quick search of the code.$ python3 -v*lots of irrelevant noise*&gt;&gt;&gt; import turtle# /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/turtle.pyc matches /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/turtle.pyA quick peek at the source code identified the position method returns a class called Vec2D, which in turn has a definition for abs' behaviour within this class.def __abs__(self):    return (self[0]**2 + self[1]**2)**0.5My geometry is too sketch to guess at why you'd want to square the pair of cartesian coordinates, add the two numbers together. Finally exponentiation the result by 0.5, but there you go, this is what absolute value of a pair of cartesian coordinates should be.The equivalent Rust code would look something like this:fn abs(point : &amp;Point) -&gt; f64 {    (point[0].powf(2.0) + point[1].powf(2.0)).powf(0.5)}The only method to address is color which in it's two parameter form sets the pen and fill colour.fn color(turtle: &amp;mut Turtle, pen_color: &amp;str, fill_color: &amp;str) {    turtle.set_pen_color(pen_color);    turtle.set_fill_color(fill_color);}Here's the code in it's entirety:extern crate turtle;use turtle::{Turtle,Point};fn main() {    let mut turtle = Turtle::new();    color(&amp;mut turtle, &quot;red&quot;, &quot;yellow&quot;);    turtle.begin_fill();    loop {        turtle.forward(200.00);        turtle.left(170.00);        if abs(&amp;turtle.position()) &lt; 1.0 {            break;        }    }    turtle.end_fill();}fn abs(point : &amp;Point) -&gt; f64 {    (point[0].powf(2.0) + point[1].powf(2.0)).powf(0.5)}fn color(turtle: &amp;mut Turtle, pen_color: &amp;str, fill_color: &amp;str) {    turtle.set_pen_color(pen_color);    turtle.set_fill_color(fill_color);}The result is fairly close!"
    },


    { 
        "title" : "London Perl Workshop 2017",

        
        
        
        "tags": [
        
            "perl",
            
        
            "lpw",
            
        
            "rust"
            
        
        ],
        "href" : "2017/london-perl-workshop",
        "content" : "Almost a month a go, around the same time as MozillaFest, NeilBowers put out a request for a Rust talk viatwitter.I would love someone to give an intro talk on #rustlang at #lpw201720 minute intro from anyone?— Neil Bowers (@neilbowers) October 27, 2017This was my first request for a Rust talk at a conference! I'm glad I didn't think too hard about that fact, as I suspect I would've been more freaked out!The talk submission process, as well as the communications leading up to the event were excellent. One of the nicest emails, was tips for new speakers, it was like having a distilled version of Scott Hanselman's speaking tips.I managed to attend two talks:Abigail's [Regexp Mini-Tutorial: Character Classes]Sue Spence's [Spiders, Gophers &amp; Butterflies]I was surprised at how much Regexp's character classes had expanded to accommodate unicode characters. I thought the class properties were very descriptive. It was nice to see Perl continue to evolve.Sue's talk was about concurrency and the differences between implementation of a webspider in Go and Perl 6. It inspired me to have a go in Rust to see how comparable to the other two languages. I found Perl 6's syntax easier to read.My talk was well attended, I managed to do my own take on the famous &quot;Control/Safety&quot; spectrum of programming languages. A few days earlier, I tested the talk on our local Rust user group and got invaluable feedback. Despite having to rejigging the slides I still managed to deliver the Introduction to Rust in time!The people who I met at the conference were lovely. The AV staff and volunteers were incredibly helpful. I'd recommend submitting a talk for London Perl Workshop 2018!My deck can be found at SpeckerDeck.The video will follow shortly, I will update this post when it's available."
    },


    { 
        "title" : "MozFest 2017 Rust Resources",

        
        
        
        "tags": [
        
            "rust",
            
        
            "rustlang",
            
        
            "mozfest",
            
        
            "mozilla"
            
        
        ],
        "href" : "2017/mozfest-2017-rust",
        "content" : "Here's the websites I've been touting at MozFest:The API documentation for the rand crate, to demonstrate the amazing detail and functionality you can add to your crate's documentation.The documentation and their location in the source code.Platforms supported by Rust (OS, chipset, etc) PodcastsNew RustaceanRequest for ExplanationRusty SpikeFriends of Rust (Organizations using Rust in Production)community.rs24 days of Rust - one of the best curated lists of Rust crates and features."
    },


    { 
        "title" : "Remote speaking tips",

        
        
        
        "tags": [
        
            "talks",
            
        
            "publicspeaking",
            
        
            "hangout",
            
        
            "remote",
            
        
            "livecoding",
            
        
            "demo"
            
        
        ],
        "href" : "2017/remote-speaking-tips",
        "content" : "On Monday I did my first remote talk (for the RustEdinburgh user group). I thought I'd sharemy experience to help others who want to do the same.tl;dr - your check list:Get a good quality headset, don't use the internal microphone on your computer.Do a mirror (selfie) test, make sure there's no embarrassing stuff in the background.Make more slides, allow the information to trickle through to re-enforce concepts. Avoid information dense slides.Do a test video conference call beforehand.Pre-bake your demos unless you're a live coding veteran.Don't ramble, short sound bites are more memorable.Practice.GearGet a decent headset, do not use your internal microphone on your computer! Iborrowed my better half's pair of HyperX CloudII,they're her gaming headphones, but they also function excellently for videoconferencing.An acceptable low-fi alternative is a phone hands free kit. The ones that comebundled with iPhones are surprisingly good for this task.SetupIf this is your first time to do video conferencing, then do a mirror or selfietest. In google hangout you can do this by clicking on the video call icon.Look around you, is there anything that you wouldn't want to appear in thebackground? Our home office is also our guest bedroom and where we keep ourclothes dryer - you get the idea.Bonus points if you can stick cool posters or a talk related paraphernalia(mascots and the like) in the background!Deck DesignThis is the area that I'm still trying to master, so I will prolly revisit in aseparate post, but here's tips I was given by more experienced speakers. Keep the detail on each slide, light, but also have more slides to re-enforcethe concepts you're trying to get across. You no longer have the advantage ofseeing the audience reaction i.e. how well they're grasping the subject matter.So drip feed the information, gradually.Google HangoutsAt the moment I'm still using Google Hangout to conduct my remote talks and tofacilitate remote speakers for the London Rust user group. Provided bothparties have Chrome, it's zero software install. However, I do have concerns, in particular the lack of multi-display support(more on this in a moment) and the inability to increase the size of the otherparty's video window.Multi display (or lack of)When doing talks in person, nearly all A/V equipment allow you to use theprojector/TV as an additional display. This allows me to configure Keynote (Macpresentation tool) in presenter mode. The slides appear on the project/TV asyou would expect and on your own computer's display you can see the timeelapsed, next slide and presenters notes.Google hangout allows you to share your screen in two modes: full screen orapplication. Neither modes are multi display aware. This means Keynote willonly display the slides, no presentation mode! Imagine my surprise when Idiscovered as I start my talk!Luckily I had my laptop as a second computer (for my live coding script). So afew seconds later I had the presentation on this computer with notes. Thealternative would be to use Keynote on your iOS device as a presentationremote. You can then configure it to display your presentation notes alongsidethe current slide.The inability to increase the size of the other party's videoThis means you can't see the audience, well not well enough to know if they'refollowing your talk.Live CodingDon't do it unless you can already touch type and talk to someone at work! It'sbetter to pre-record you live coding, then you can edit out pauses. You'll alsofind that providing commentary is easier, when you're not worry about what totype next.If you do decide you want to do live coding, keep a script of what you want toshow. If it makes sense have the code snippets handy. My talk about was the Rust Language Server and it's integration into VisualStudio Code, so I needed to make errors to demonstrate functionality. So codesnippets wouldn't have been useful.If you have a suitable screen recording that also records your microphone, useto practice. Once the lengthy pauses have stopped you're be ready!I did do live coding, but I've done this talk twice. And I also practiced thescript everyday until I did the talk.PreparationIf possible do a test call before the day of the talk. This gives you time toiron out any technical glitches. Also expect to do another setup call a fewminutes before you talk, your hosts will usually want this.The Rust Edinburgh organisers also asked for a copy of my slides. The plan wasthat if the internet connection dropped off, they would call my phone and theywould go through the slide on my behalf.PerformanceRambling in general is a bad idea in talks. In remote talks it can spoil yourperformance. Remember you can't see your audience, if you could see the yawnsor people staring at their phones you would know you're rambling.Finally this one is a given if you want to do talks: practice makes perfect! -- Don't forget to take an audience selfie!"
    },


    { 
        "title" : "National Novel Generation Month 2016 Reflection",

        
        
        
        "tags": [
        
            "procgen",
            
        
            "nanogenmo",
            
        
            "rust",
            
        
            "bash"
            
        
        ],
        "href" : "2017/nanogenmo-2016-reflection",
        "content" : "National Novel Generation Month aka NaNoGenMo is a month long contest to write code to generate a novel of 50k+ words.Before I start talking about my plans for this year, I thought it would be a good idea to reflect on my previous efforts. In last year's contest I created two novels(!):Banned Books CensoredThe statement of intent on NaNoGenMo: github.com/NaNoGenMo/2016/issues/94.I redacted the following books (via Project Gutenberg) listed in Anne Haight's Banned Books (booked banned by authorities or religious organisations at some point in time). The numbers relate to the Project Gutenberg book ID:The_Clouds by Aristophanes (2562)The Birds by Aristophanes (3013)The Analects of Confucius by Confucius (3100)The Analects of Confucius by Confucius (3330)The Odyssey_by_Homer (3610)The Analects of Confucius by Confucius (3610)Lysistrata by Aristophanes (7700)The code to do these novels can be found at github.com/booyaa/fuckcensorship. The language I used was Bash. The main crux of the code is to replace non-whitespace characters with Unicode FULL BLOCK character (█):for book in $FOLDER_IN/*.txt; do    echo &quot;Processing $book...&quot;    FILE_OUT=$FOLDER_OUT/$(basename &quot;$book&quot; .txt).censored.txt    $SED_EXEC  &#39;s/\\S/\\xE2\\x96\\x88/g&#39; &lt; &quot;$book&quot; &gt; &quot;$FILE_OUT&quot;doneHere's a sample of The Odyssey.███████████████████████ ██ ██ ████ ███ ██████ ██ ██████████ ██ █████████ ██ █████████████ ██ ██ ███████ ████ ████ ██ ██ ███████ █████ ███ ███ ███████ █████ ██ ████ ███ ████ ███████ ███████████ ██████ ████ ███ ███████████ █████████ ██ ███ ██████████ ██ ████ ███████████ ███████ █████████████ █████████ █████ █████████ ██████████ █████████ ██ ████ ████████ ███ ███████ ███ ███████ █████ █████ ████ ██ ██ ██████ ██ ██████ █████ ██████████ █████████ █████ ██ ███ ████ ██ ██ █████ █████████ ███████ ██ ████████The collection of censored books can be found on github.com/booyaa/bannedbookscensored/tree/master/censoredA book of NovemberThis novel was a simple concept, you need to write 1,666 words a day to hit the 50k target for NaNoWriMo. So why not write chapter consisting of the number of the day for the month i.e. the 1st of the month would be one?A book of NovemberStatement of intent on NaNoGenMoThe code to create this novel can be found at github.com/booyaa/nanogenmo2016. The language I used to write the code was Rust. The code snippet I'm particular proud of is my sentence length variation func. fn get_sentence(word: &amp;str) -&gt; (String, u64) {    let mut rng = rand::thread_rng();    let faces = Range::new(1, 9);    let num = dice(&amp;faces, &amp;mut rng);    let sentence_sizes = match num {        1 | 4 | 7 =&gt; 5,        2 | 5 | 8 =&gt; 7,        3 | 6 | 9 =&gt; 13,        _ =&gt; 0,    };    let mut sentence = String::new();    for i in 0..sentence_sizes {        sentence.push_str(&amp;format!(&quot;{} &quot;, word));    }    sentence = sentence.trim().to_string();    sentence.push_str(&quot;. &quot;);    (sentence, sentence_sizes)}Admittedly the effect is a bit subtle to be noticed the first time around. Also in reflection I should've capitalized the start of the sentence.Here's a sample of the novel (brace yourself)oneone one one one one one one one one one one one one. one one one one one. one one one one one one one. one one one one one one one. one one one one one. one one one one one. one one one one one. one one one one one one one. one one one one one. one one one one one one one. one one one one one one one. one one one one one one one. one one one one one one one. one one one one one one one one one one one one one. one one one one one one one one one one one one one. one one one one one one one. one one one one one. one one one one one one one one one one one one one.SummaryI didn't spend the whole month writing code to write a novel, but the exercise was a lot of fun. Were my novels any good? No not really, they should be filed under the joke or stunt category. However, don't let my efforts convince you that it's not a worth while project. Here's some of my favourites from last year:Captain's Log - Ronseal: eoinnoble.github.io/captains-log/output/captains-log.htmlDear Santa - Search twitter for requests to add to Santa's list 2016-11-25 aka Black FridayAnnales - World history generator github.com/spikelynch/annales/blob/master/output/annales.pdfTiny Tarot Stories - A story generated through tarot readings github.com/enkiv2/misc/blob/master/nanogenmo-2016/tinyTarotStories.md"
    },


    { 
        "title" : "Alexandria PL/SQL Utility Library",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "ooxml",
            
        
            "microsoft"
            
        
        ],
        "href" : "2017/alexandria-plsql-utility-library",
        "content" : "Imagine if BatMan was an Oracle DBA, his utility belt would be the Alexandria PL/SQL Utility Library.How to install Microsoft Office document parsers (OOXML)AssumptionsThis has been tested on 12c r1, I imagine it should still work for 11G.SCHEMA_ALEX_INSTALL - This is the schema you have created that will host the Alexandria library.Using SQLPlus or Oracle SQL Developer (in SQLPlus mode)Script have installed to a path that SQLPlus can findInstallation instructionsAs sys or similiargrant execute on dbms_random to SCHEMA_ALEX_INSTALL;grant execute on dbms_lob to SCHEMA_ALEX_INSTALL;grant execute on util_file to SCHEMA_ALEX_INSTALL;grant create any directory to SCHEMA_ALEX_INSTALL;As SCHEMA_ALEX_INSTALLset scan off;prompt Creating types@types.sqlprompt Creating MICROSOFT package specifications@../ora/string_util_pkg.pks@../ora/zip_util_pkg.pks@../ora/xml_util_pkg.pks@../ora/sql_util_pkg.pks@../ora/file_util_pkg.pks@../ora/debug_pkg.pks@../ora/xml_builder_pkg.pks@../ora/xml_dataset_pkg.pks@../ora/xml_stylesheet_pkg.pks@../ora/xml_util_pkg.pks@../ora/ooxml_util_pkg.pksprompt Creating MICROSOFT package bodies@../ora/string_util_pkg.pkb @../ora/zip_util_pkg.pkb@../ora/xml_util_pkg.pkb@../ora/sql_util_pkg.pkb@../ora/file_util_pkg.pkb@../ora/debug_pkg.pkb@../ora/xml_builder_pkg.pkb@../ora/xml_dataset_pkg.pkb@../ora/xml_stylesheet_pkg.pkb@../ora/xml_util_pkg.pkb@../ora/ooxml_util_pkg.pkbprompt Done!Verifying the installationAs SCHEMA_ALEX_INSTALL HOST md d:\\temp\\devtest -- Un*x users: mkdir /path/to/devtestCREATE DIRECTORY devtest_temp_dir AS &#39;d:\\temp\\devtest&#39;; -- Un*x SET SERVEROUTPUT ONDECLARE    l_blob    BLOB;    l_props   ooxml_util_pkg.t_xlsx_properties;BEGIN    debug_pkg.debug_on;    l_blob := file_util_pkg.get_blob_from_file(&#39;DEVTEST_TEMP_DIR&#39;,&#39;hello_excel.xlsx&#39;);    l_props := ooxml_util_pkg.get_xlsx_properties(l_blob);    debug_pkg.printf(        &#39;title = %1,modified = %2,creator = %3,application = %4&#39;,        l_props.core.title,        l_props.core.modified_date,        l_props.core.creator,        l_props.app.application    );END;/"
    },


    { 
        "title" : "Oracle tips",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "tips"
            
        
        ],
        "href" : "2017/oracle-tips",
        "content" : "Hiding user inputSometimes you need to keep something secret (shoulder surfing), this will only work in SQL/Plus or Oracle SQL Developer (F5/Run script mode aka broken SQL/Plus mode).SET SERVEROUTPUT ONSET VERIFY OFFACCEPT sekrit PROMPT &#39;enter a secret (warning we&#39;&#39;re going to print it on screen!)&#39; HIDEBEGIN    dbms_output.put_line(&#39;sekrit: &#39; || &#39;&amp;sekrit&#39;);END;/If you ran the script correctly, the input dialogue will echo stars * instead of your &quot;secret&quot;.If you can see the data you're entering, you ran Oracle SQL Developer statement mode (CTRL-ENTER).SessionsFindingSET LINESIZE 140SET PAGESIZE 50COL sid FORMAT a5COL serial FORMAT a5COL username FORMAT a30COL osuser FORMAT a30COL machine FORMAT a30COL client_ip FORMAT a20SELECT    sid,    serial# AS serial,    osuser,    username,    machine,    logon_time,    utl_inaddr.get_host_address(regexp_replace(machine,&#39;^.+\\\\&#39;) ) AS client_ipFROM    v$sessionWHERE        username IS NOT NULL    AND        status &lt;&gt; &#39;KILLED&#39;;Killingalter system kill session 'sid,serial#';Code generated exampleSELECT     &#39;alter system kill session &#39;&#39;&#39; ||sid||&#39;,&#39;||serial#|| &#39;&#39;&#39;;&#39; as sqlFROM    v$sessionWHERE    username = &#39;VICTIM&#39;;"
    },


    { 
        "title" : "C Sharp and .NET tips",

        
        
        
        "tags": [
        
            "c#",
            
        
            "csharp",
            
        
            "dotnet",
            
        
            ".net",
            
        
            "dotnetcore"
            
        
        ],
        "href" : "2017/csharp-dotnet",
        "content" : "NUnit TestCases with instances of a typepublic IEnumerable&lt;TestCaseData&gt; CanParseAsThingyTestCases{    get    {        Setup();        yield return new TestCaseData(@&quot;foo&quot;, new Thingy { name = &quot;foo&quot; });        yield return new TestCaseData(@&quot;bar&quot;, new Thingy { name = &quot;bar&quot; });            }}[TestCaseSource(&quot; CanParseAsThingyTestCases&quot;)]public void CanParseAsThing(string input, Thingy expected){...source: https://stackoverflow.com/a/4230328/105282RegExA somewhat convoluted example to demonstrate to avoid magic strings (ish)/string pattern = @&quot;^(?&lt;WANT&gt;Foo)bar$&quot;;string input = @&quot;Foobar&quot;;Regex r = Regex(patern, RegexOptions.SingleLine);Match m = r.Match(input);foreach (string groupName in r.GetGroupNames()) {  if (groupName != &quot;0&quot;) {    Console.WriteLine($&quot;Found {m.Groups[groupName].Value} in {groupName}&quot;);  }}Note to self:Turn this into a useful examplehttps://codereview.stackexchange.com/a/6962/2500https://stackoverflow.com/a/10417114/105282A gitignore for dotnet coresource: https://gist.github.com/booyaa/db187f5555afdba82d371b76119920c5# miscellany and obsolete artifact*.swp*.*~project.lock.json.DS_Store*.pyc# Visual Studio Code.vscode# Build results[Bb]in/[Oo]bj/"
    },


    { 
        "title" : "A developer on-boarding guide for Rust",

        
        
        
        "tags": [
        
            "learning",
            
        
            "rust"
            
        
        ],
        "href" : "2017/learning-rust-meta",
        "content" : "Background: We needed a meta reference for our study group event!Setting up your IDE/Code editor for RustNick's RLS showcase blog postbooyaa's RLS talk - if you want to see the past London group's talks you can see them here.Neovim plugin that support RLSCode completion in emacs using emacs-racerUseful resourcesThe Rust forum aka URLOThe Rust Community Team Website[The Request for Explanation Podcast(https://request-for-explanation.github.io/podcast/) - A weekly discussion of Rust RFCs.The official Rust youtube channelHack and Learn Group InfoOur siteChatroom - Looks like we've reached max connections at London! We'll try to find a better solution for next time!"
    },


    { 
        "title" : "Troubleshooting the Rust Language Server",

        
        
        
        "tags": [
        
            "rls",
            
        
            "vscode",
            
        
            "help"
            
        
        ],
        "href" : "2017/troubleshooting-rls",
        "content" : "To understand how to troubleshoot the Rust Language Server (RLS), it helps to know what RLS is and how the components interact.RLS is a Rust implementation of the Language Server Protocol (LSP). LSP is based on the client server architecture, and simplifies the way code editors and IDEs interact with a programming language.The client in this instance is the official RLS extension. The server is rls which is another Rust command line tool like rustfmt, racer and rustsym. The extension is responsible for setting up and starting rls.If you want to find out more I recommend checking out @nrc's introductory blog post: What the RLS can do.Whilst this post is tailored towards the VS Code extension, where possible I'll let you know when you can run this against your own setup.The up to date mantraAlways have the latest version of Rust nightly and RLS. rustup updateKeep Visual Studio Code and the Rust RLS extension up to date.Increase logging levelTo turn on the fire hose:RUST_LOG=rls=debug code .To reduce the noise you can drop down to informational:RUST_LOG=rls=info code .This tip assumes you have Visual Studio Code in your path so it can be launched from the command line.Diagnostics will appear in the Output panel for the Rust Language Server (the drop down to the right of the panel).This can also be used for other editors, just replace code with your own editor.Additional configuration settingsIn your user or workspace settings (settings.json) add the following  configuration parameters:{    // Includes standard error from RLS in the Output panel. This isn&#39;t     // particular useful if you&#39;ve already enabled logging at debug level.    &quot;rust-client.showStdErr&quot;: true,        // This will also log everything that appears in the Output panel to a     // log file in the root of your workspace.    &quot;rust-client.logToFile&quot;: true,}Both of these will require a restart of the editor. If your LSP client implements theworkspace/didChangeConfiguration method, then add the same keys to your client's configuration file.Alternatively send the abovementioned method and that example json fragment to RLS.Where to go if you need more help?@nrc has just written the definitive guide to debugging and troubleshootingRLS. I would recommend you also visit this article.As always with any good open source project, help is always available via a new github issue. Alternative if you use irc, you can ask in #rust-dev-tools on the Mozilla network."
    },


    { 
        "title" : "Regular Expressions Miscellany",

        
        
        
        "tags": [
        
            "regularexpression",
            
        
            "regex",
            
        
            "oracle",
            
        
            "notepad++"
            
        
        ],
        "href" : "2017/message-queue-miscellany",
        "content" : "Oracle related regular expressionsregexp_count and regexp_replace functionsLet say you have a column that contains source for an Oracle object. We want to extract  documentation hidden in sql multiline comment block /* .... */, it's been wrapped in a pair of @@ for easy identification.IDsourcedocumentation1/@@treasure@@/treasureselect foo from bar2select fizz from buzzThe little gotcha is that there's a linebreak between the documentation and the actual code.Whilst not regular expression as per se you want to use translate to remove any type of non printing whitespace (tabs, line breaks and carriage returns).translate(source, CHR(10)||CHR(11)||CHR(13), '    ')This will allow you to supply regexp_replace with the following regular expression '^..@@(.*)@@(.*)' and extract the documentation.Here's the entire solution with a common table expression to provide some test data.COL source FORMAT a30COL documentation FORMAT a30WITH data AS (    SELECT        &#39;/*@@treasure@@*/&#39;         || CHR(10)         || &#39;select foo from bar&#39; AS source    FROM        dual    UNION ALL    SELECT        &#39;select fizz from buzz&#39;    FROM        dual) SELECT    source,        CASE            WHEN regexp_count(source,&#39;^..@@(.*)@@(.*)&#39;) &gt; 0 THEN regexp_replace(                translate(                    source,                    CHR(10)                     || CHR(11)                     || CHR(13),                    &#39;    &#39;                ),                &#39;^..@@(.*)@@(.*)&#39;,                &#39;\\1&#39;            )        END    AS documentationFROM    data;The only addition is a check to see if the regular expression was matched against the source field so we only processed those columns and return no data in the documentation field.I used this to enhance this handy ORDS to Swagger script to provide documentation in the description tags rather than the source code (which it to fail validation).notepad++Creating fake data as an in-memory SQL tableyou want to turn thisJEB JIBBLY 1234 3BOBBY TABLES 2314 1WHISKY JACK 2241 2into thisWITH FOO AS ( SELECT &#39;JEB JIBBLY&#39; &quot;FULL_NAME&quot;, &#39;1234&#39; &quot;PIN&quot;, &#39;3&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL   UNION ALL  SELECT &#39;BOBBY TABLES&#39; &quot;FULL_NAME&quot;, &#39;2314&#39; &quot;PIN&quot;, &#39;1&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL  UNION ALL  SELECT &#39;WHISKY JACK&#39; &quot;FULL_NAME&quot;, &#39;2241&#39; &quot;PIN&quot;, &#39;2&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL)SELECT * FROM FOO;  notepad++ regexpfind what: (.*)\\s+(\\d{4})\\s(\\d+)replace with: SELECT '\\1' &quot;FULL_NAME&quot;, '\\2' &quot;PIN&quot;, '\\3' &quot;FAILED_ATTEMPTS&quot; FROM DUAL UNION ALLmake sure you have enabled regular expression in search mode!then obviously you'll have to format and remove unnecessary UNIONs"
    },


    { 
        "title" : "London Rust User Group Meetup No. 15",

        
        
        
        "tags": [
        
            "rust",
            
        
            "meetup",
            
        
            "usergroup"
            
        
        ],
        "href" : "2017/london-rust-15",
        "content" : "InterwebsFixed as of 0.3.2 2017-08-15 on #Rocket&lt;Sergio&gt; Quick announcement: latest Rust nightly breaks Rocket&#39;s lints. Please use a nightly between 2017-08-10 and 2017-08-13 while the issue is resolved.URLO: VendoringThis week in Dev Tools No. 1AnnoucenmentsRust Study Group and Hacking On Meetup No. 2RequestsTalk Spots (add comment on the related issue if you want to talk)September 20th - 2 lightning talks (5-15m) availableMore dates - We're pre-booked with Skillsmatter up to the end of the year!JobsSee me after the talks if you want to advertise!"
    },


    { 
        "title" : "Message Queue Miscellany",

        
        
        
        "tags": [
        
            "mq",
            
        
            "messagequeue"
            
        
        ],
        "href" : "2017/message-queue-miscellany",
        "content" : "This is a bit of a hodge podge of message queue notes. Very IBM centric at the moment.Command line toolsCheatsheetSET MQSERVER=CHANNEL.NAME/TCP/HOST OR IP ADDRESS(PORT)AMQSPUTC QUEUENAME QUEUEMANAGER &lt; SOME\\FILE\\CALLED\\HELLO.TXTAMQSGETC QUEUENAME QUEUEMANAGERDisplay library versionWin (or c:\\windows\\assembly)C:\\IBM\\WebSphere MQ\\dspmqver -iUnix (should be in pathdspmqver -isource: http://www-01.ibm.com/support/docview.wss?uid=swg21621707tag: mq , .net , dllRFHUtilcQueue Manager Name: CHANNEL NAME/TCP/HOSTNAME(PORT)"
    },


    { 
        "title" : "Using Rust with Visual Studio Code",

        
        
        
        "tags": [
        
            "rust",
            
        
            "vscode",
            
        
            "tips"
            
        
        ],
        "href" : "2017/rust-vscode",
        "content" : "Which extension?First off if you can, you should be using the Rust Language Server (RLS) extension. Yes it's beta, but the user experience has been the best I've had, outside of in-house language support by giants like Microsoft and JetBrains! The extension will even install dependencies for you if you have rustup installed!If you didn't want to use RLS, then the alternative is to install various Rust related tools (racers, rustfmt and rustsym) manually. The only Rust extension that support non-RLS is Kalita Alexey's.Whilst we're on the subject of Kalita's extension, this was a fork of the RustyCode extension which was no longer being actively maintained.The biggest draw this extension at the time was that it was available on the Visual Studio Marketplace, where as the RLS team extension had to be manually installed via git.It will be interesting two see how the two active extensions progress.If you want a proper whistle stop tour of RLS I recommend you pop over to @nrc blog as he's done a thorough job of it in this post. DebuggingI'm ashamed to admit that I still find setting up debugging Rust a bit of a black art if you're not using gdb. However thanks to the LLDBDebugger extension it's become a little bit easier.The only bit that caught me out was the launch.json boiler plate code (see below for a sample), specifically what would be the correct value for program key. This is the path to your debug binary.{    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        {            &quot;type&quot;: &quot;lldb&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;name&quot;: &quot;Debug&quot;,            &quot;program&quot;: &quot;${workspaceRoot}/&lt;your program&gt;&quot;,            &quot;args&quot;: [],            &quot;cwd&quot;: &quot;${workspaceRoot}&quot;        }    ]}So if the binary you wish to debug is called foo, your value for the program key would look like this:{    &quot;program&quot;: &quot;${workspaceRoot}/target/debug/foo&quot;}note: I've omitted the rest of the json keys that don't change for the sake of brevity.If you wanted to keep things generic and only compile a binary that matches the cargo folder name, you could use ${workspaceRootFolderName} variable substitution.{    &quot;program&quot;: &quot;${workspaceRoot}/target/debug/${workspaceRootFolderName}&quot;,}If you're interested in what other variables substitutions are available the Visual Studio Code Debuggerguide has a handy list.One last option to enable is sourceLanguages with the value of &quot;rust&quot;, this option enables visualisation of built-in types and standard library types i.e. you can peek into the contents of a Vec etc.Here's a complete example of the launch.json for reference.{    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        {            &quot;type&quot;: &quot;lldb&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;name&quot;: &quot;Debug&quot;,            &quot;program&quot;: &quot;${workspaceRoot}/target/debug/${workspaceRootFolderName}&quot;,            &quot;args&quot;: [],            &quot;cwd&quot;: &quot;${workspaceRoot}&quot;,            &quot;sourceLanguages&quot;: [&quot;rust&quot;]        }    ]}"
    },


    { 
        "title" : "Rust Language Server and Visual Studio Code",

        
        
        
        "tags": [
        
            "rls",
            
        
            "rust",
            
        
            "vscode",
            
        
            "languageserverprotocol"
            
        
        ],
        "href" : "2017/vscode-rls",
        "content" : "Click here to skip the history lesson and go straight to the tips.I first heard about the Rust Language Server (RLS), via Phil Dawes' Racer talkat the 4th London Rust UserGroup.Immediately I knew the significance of this strategy and how it would reducethe friction required to get Rust working on the vast array of editors andIDEs.This was also around the same time I switched from using vim to Visual StudioCode as my primary editor. Until code landed, I had to make do with the nowdefunct Rusty Code which like all other editor add-ons requires a fair bit ofwork to have a coding environment that is Rust savvy.As soon as RLS alpha came out, I happily gave it a go (along with it'sreference Visual Studio Code extension). Eventually we'd see RLS move into therust-lang-nursery repo (where great ideas get incubated) and also becomeanother rustup component (like src).I would flit between using the reference extension and Kalita Alexey's fork,waiting for the day when the reference extension also got added to the VisualStudio Marketplace.Finally the big day has come and the reference Visual Studio Code extension hasnow landed in the Marketplace! So getting Rust to work with Visual Studio Codeis super simplified. In fact I think if you already have a recent version ofrustup, installing theextensionwill trigger the installation of RLS automatically!So you can imagine over time you start to gather a lot of Rust extension leavings...I'd recommend you go through each of your workspace or user (global) settingsfile and remove anything rust related.Rusty Code{    &quot;rust.racerPath&quot;: null, // Specifies path to Racer binary if it&#39;s not in PATH    &quot;rust.rustLangSrcPath&quot;: null, // Specifies path to /src directory of local copy of Rust sources    &quot;rust.rustfmtPath&quot;: null, // Specifies path to Rustfmt binary if it&#39;s not in PATH    &quot;rust.rustsymPath&quot;: null, // Specifies path to Rustsym binary if it&#39;s not in PATH    &quot;rust.cargoPath&quot;: null, // Specifies path to Cargo binary if it&#39;s not in PATH    &quot;rust.cargoHomePath&quot;: null, // Path to Cargo home directory, mostly needed for racer.                                 // Needed only if using custom rust installation.    &quot;rust.cargoEnv&quot;: null, // Specifies custom variables to set when running cargo. Useful for                            // crates which use env vars in their build.rs (like openssl-sys).    &quot;rust.formatOnSave&quot;: false, // Turn on/off autoformatting file on save (EXPERIMENTAL)    &quot;rust.checkOnSave&quot;: false, // Turn on/off `cargo check` project on save (EXPERIMENTAL)    &quot;rust.checkWith&quot;: &quot;build&quot;, // Specifies the linter to use. (EXPERIMENTAL)    &quot;rust.useJsonErrors&quot;: false, // Enable the use of JSON errors (requires Rust 1.7+).                                  // Note: This is an unstable feature of Rust and is still in the process of being stablised    &quot;rust.useNewErrorFormat&quot;: false, // &quot;Use the new Rust error format (RUST_NEW_ERROR_FORMAT=true).                                      // Note: This flag is mutually exclusive with `useJsonErrors`.}Kalita's Rust fork of the reference extensionI used the RLS integration rather than the legacy setup. The legacy setuprequires you to install rust's friends like rustfmt and racer manually.{    &quot;rust.rls&quot;: {         &quot;executable&quot;: null, // The path to an executable to execute         &quot;args&quot;: null, //  an array of strings. Arguments to pass to the executable         &quot;env&quot;: null, //  An object with the environment to append to the current environment to execute the executable         &quot;revealOutputChannelOn&quot;: null, // A string. Specifies the condition when the output channel should be revealed         &quot;useRustfmt&quot;: null // either a boolean or null. Specifies whether rustfmt should be used for formatting    }}Finally it looks like rls.toml is being deprecated in favour of similiary named settings.json parameters.so build_lib: falseworkspace_mode: truebecomes{    &quot;rust.build_lib&quot;: false,    &quot;rust.workspace_mode&quot;: true,}"
    },


    { 
        "title" : "London Rust User Group Meetup No. 14",

        
        
        
        "tags": [
        
            "rust",
            
        
            "meetup",
            
        
            "usergroup"
            
        
        ],
        "href" : "2017/london-rust-14",
        "content" : "InterwebsCambridge Meetup No. 2Request for Explanation Episode No. 2 - Stealing Chickens on the Internet. No. 4 landed on Monday!RustFest Zurich CFP: closes July 23rd 10PM UTCRequestsTalk Spots (add comment on the related issue if you want to talk)August 16th - 2 lightning talks (5-15m) availableSeptember 20th - 2 lightning talks (5-15m) availableMore dates - We're pre-booked with Skillsmatter up to the end of the year!JobsSee me after the talks if you want to advertise!"
    },


    { 
        "title" : "Cargo cult - the problem with copying existing code",

        
        
        
        "tags": [
        
            "cargocult",
            
        
            "visualstudio",
            
        
            "msbuild"
            
        
        ],
        "href" : "2017/cargo-cult-visual-studio",
        "content" : "A quick note to myself on how to identify build errors. This is also a timelyreminder that you should create code from scratch once in a while, rather thancopying an existing project. This is a form of cargo cult (I've linked to theWikipedia page in my references below, if you've never heard of the termbefore)The pain to get it working will be worthwhile, as you will have learntsomething new. In my case how to run MSBuild (Microsoft build toolchain) andidentify what version of SSIS (Microsoft flavoured ETL) my project was basedon.I started to wire up my brand new SSIS project to TeamCity (JetBrains flavouredCI). Immediately the build failed. Asking the team and no one else had seen theerror before.Googling suggested that I try and run MSBuild on TeamCity. One problem, Devsdon't get direct access to the CI/CD infrastructure. So time to learn how torun MSBuild locally.In the start menu &gt; All Programs &gt; VisualStudio 2015 &gt; VisualStudio Toolsthere's a handy shortcut called MSBuild Command Prompt for VS2015 this willsetup a command prompt with the correct config to run MSBuild.After copying over any dependencies for the build task (in my case the SSISDLLs from the TeamCity server), it was just a case of pointing MSBuild to mybuild file.C:\\Path\\To\\SSIS\\Package\\LOL&gt;msbuild LOL.buildMicrosoft (R) Build Engine version 14.0.25420.1Copyright (C) Microsoft Corporation. All rights reserved.Build started 19/07/2017 11:29:31.Project &quot;C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build&quot; on node 1 (default targets).SSISBuild:  **************Building SSIS project: LOL.dtproj **************  ------  Loading project file &#39;LOL.dtproj&#39;*snipping verbosity*  C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build(9,3): error : Error while loading  package &#39;WAT.dtsx&#39;: The package failed to load due to error 0xC0011008 &quot;Error  loading from XML. This occurs when CPackage::LoadFromXML fails.\\r*snipping verbosity*C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build(9,3): error :    at Microsoft.SqlServer.Dts.Runtime.Package.LoadFromXML(String packageXml, IDTSEvents events)\\r    0 Warning(s)    2 Error(s) &lt;-- not entirely sure why MSBuild felt the need to run twice,                   but it did...Time Elapsed 00:00:01.16This was great, because this was exactly the error I got in TeamCity! Turns outour existing SSIS projects target SQL Server 2012. The new project inVisualStudio 2015 target SQL Server 2016 by default. So the error is literallyMSBuild saying, &quot;The SSIS DLLs don't know what the hell this file!&quot;To downgrade the project, it was just the case of right clicking on the SSISproject &gt; Configuration Properties &gt; General &gt; TargetServerVersion andswitching to SQL Server 2012.Then as if by magic...C:\\Path\\To\\SSIS\\Package\\LOL&gt;msbuild LOL.buildMicrosoft (R) Build Engine version 14.0.25420.1Copyright (C) Microsoft Corporation. All rights reserved.Build started 19/07/2017 11:10:42.Project &quot;C:\\Users\\staama1\\Source\\Workspaces\\ESPP\\Tasks\\SSIS\\LOL\\Main-Dev\\LOL\\LOL.build&quot; on node 1 (default targets).SSISBuild:  **************Building SSIS project: LOL.dtproj **************  ------  Loading project file &#39;LOL.dtproj&#39;  Setting output directory to &#39;bin\\Development&#39;  Setting project ProtectionLevel to &#39;DontSaveSensitive&#39;  Loading ConnectionManager &#39;ADONET.Chronos.conmgr&#39;  Loading ConnectionManager &#39;OLEDB.Chronos.conmgr&#39;  Loading package &#39;WAT.dtsx&#39;  Changing protection level of package to &#39;DontSaveSensitive&#39;  Saving project to: &#39;bin\\Development\\LOL.ispac&#39;Done Building Project &quot;C:\\Users\\staama1\\Source\\Workspaces\\ESPP\\Tasks\\SSIS\\OPF_REPSYS\\Main-Dev\\LOL\\LOL.build&quot; (default targets).Build succeeded.    0 Warning(s)    0 Error(s)Time Elapsed 00:00:01.43ReferencesCargo cultMSBuild walkthroughBuilding MSBuild Project From Scratch"
    },


    { 
        "title" : "SSIS Variable Dispenser Template",

        
        
        
        "tags": [
        
            "ssis",
            
        
            "bids",
            
        
            "template"
            
        
        ],
        "href" : "2017/variable-dispenser",
        "content" : "I don't write enough SSIS script blob tasks to commit this to memory. This is the safest way to access variables without inadvertantly locking them after a crash.public void Main() {  Variables vars = null;  try {    Dts.VariableDispenser.LockForWrite(&quot;User::strWritable&quot;);    Dts.VariableDispenser.LockForRead(&quot;User::strReadable&quot;);    Dts.VariableDispenser.GetVariables(ref vars);    vars[&quot;User::strWritable&quot;].Value = vars[&quot;User::strReadable&quot;].Value.ToString();    Dts.TaskResult = (int) ScriptResults.Success;  }  catch(Exception) {    Dts.TaskResult = (int) ScriptResults.Failure;  }  finally {    vars.Unlock();  }}"
    },


    { 
        "title" : "ORDS tips",

        
        
        
        "tags": [
        
            "ords",
            
        
            "xml",
            
        
            "mime_types",
            
        
            "oracle"
            
        
        ],
        "href" : "2017/ords",
        "content" : "XML over ORDSTurns out you can emit XML via ORDS. I'll assume you have a working ORDS install and the schema is already enabledBEGIN  ORDS.DEFINE_MODULE(      p_module_name    =&gt; &#39;reporting&#39;,      p_base_path      =&gt; &#39;/reporting/&#39;,      p_items_per_page =&gt;  0,      p_status         =&gt; &#39;PUBLISHED&#39;,      p_comments       =&gt; NULL);        ORDS.DEFINE_TEMPLATE(      p_module_name    =&gt; &#39;reporting&#39;,      p_pattern        =&gt; &#39;hello_xml&#39;,      p_priority       =&gt; 0,      p_etag_type      =&gt; &#39;HASH&#39;,      p_etag_query     =&gt; NULL,      p_comments       =&gt; NULL);  ORDS.DEFINE_HANDLER(      p_module_name    =&gt; &#39;reporting&#39;,      p_pattern        =&gt; &#39;hello_xml&#39;,      p_method         =&gt; &#39;GET&#39;,      p_source_type    =&gt; &#39;plsql/block&#39;,      p_items_per_page =&gt;  0,      p_mimes_allowed  =&gt; &#39;&#39;,      p_comments       =&gt; NULL,      p_source         =&gt; &#39;DECLARE    l_clob   CLOB;BEGIN    SELECT        &#39;&#39;&lt;xml&gt;hello world&lt;/xml&gt;&#39;&#39;    INTO        l_clob    FROM        dual;    owa_util.mime_header(        ccontent_type   =&gt; &#39;&#39;text/xml&#39;&#39;,        bclose_header   =&gt; true,        ccharset        =&gt; &#39;&#39;ISO-8859-4&#39;&#39;    );    htp.print(l_clob);END;&#39;      );  COMMIT; END;$ curl -v http://oracle.rocks:8080/ords/scott/reporting/mime_works* About to connect() to oracle.rocks port 8080 (#0)*   Trying 1.2.3.4... connected* Connected to oracle.rocks (1.2.3.4) port 8080 (#0)&gt; GET /ords/scott/reporting/mime_works HTTP/1.1&gt; User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.27.1 zlib/1.2.3 libidn/1.18 libssh2/1.4.2&gt; Host: oracle.rocks:8080&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Content-Type: text/xml; charset=ISO-8859-4&lt; ETag: &quot;WaloI8WDL3PY2G0ZN6+I8C+c0FxVaUBuDc/v7LKXTpE6dTuJR1s2bLF/0hqW2fVzaXNYpr9TFXqucyoq6dO2Xw==&quot;&lt; Transfer-Encoding: chunked&lt;&lt;xml&gt;hello world&lt;/xml&gt;* Connection #0 to host oracle.rocks left intact* Closing connection #0Important item of note is the mime header, it's important that bclose_header is always true because this is literally the last HTTP header before the response body. Setting this to false will result in no data.ReferencesORACLE-BASEEmitting JSON via refcursorEarlier this year Tim Hall did an excellent talk called, &quot;Make the RDBMS Relevant Again with RESTful Web Services and JSON&quot; (youtube) at Oracle Code 2017. The big takeaway for me was to not throw away your existing investment and rewrite everything to fit the new technology (ORDS). Instead you should use ORDS to wrap around the existing objects. At the day job we use strongly typed reference cursor as an out parameter (this helps interoperability with Microsoft's SSRS). Here's the example &quot;reporting&quot; package.CREATE OR REPLACE PACKAGE foo_pkg AS     /* strongly typed reference cursors or gtfo */    TYPE bar_reftype IS RECORD (        object_name user_objects.object_name%TYPE,        object_type user_objects.object_type%TYPE    );    TYPE bar_refcur IS REF CURSOR RETURN bar_reftype;        PROCEDURE bar_get (p_recordset IN OUT bar_refcur);END foo_pkg;/CREATE OR REPLACE PACKAGE BODY foo_pkg AS    PROCEDURE bar_get ( p_recordset IN OUT bar_refcur )        AS    BEGIN        OPEN p_recordset FOR            SELECT                uo.object_name,                uo.object_type            FROM                user_objects uo            WHERE                ROWNUM &lt; 5;    EXCEPTION        WHEN OTHERS THEN            IF                p_recordset%isopen            THEN                CLOSE p_recordset;            END IF;            RAISE;    END bar_get;END foo_pkg;/Here's the ORDS wrapper, yes unfortunately you're going to need APEX_JSON, but the example should be easy enough to understand. It would be great if one day we didn't need this boilerplate, but for now this is a handy way of reusing code.BEGIN  ORDS.DEFINE_MODULE(      p_module_name    =&gt; &#39;foo&#39;,      p_base_path      =&gt; &#39;/foo/&#39;,      p_items_per_page =&gt;  0,      p_status         =&gt; &#39;PUBLISHED&#39;,      p_comments       =&gt; NULL);        ORDS.DEFINE_TEMPLATE(      p_module_name    =&gt; &#39;foo&#39;,      p_pattern        =&gt; &#39;bar/&#39;,      p_priority       =&gt; 0,      p_etag_type      =&gt; &#39;HASH&#39;,      p_etag_query     =&gt; NULL,      p_comments       =&gt; NULL);  ORDS.DEFINE_HANDLER(      p_module_name    =&gt; &#39;foo&#39;,      p_pattern        =&gt; &#39;bar/&#39;,      p_method         =&gt; &#39;GET&#39;,      p_source_type    =&gt; &#39;plsql/block&#39;,      p_items_per_page =&gt;  0,      p_mimes_allowed  =&gt; &#39;&#39;,      p_comments       =&gt; NULL,      p_source         =&gt; &#39;DECLAREl_cursor SYS_REFCURSOR;BEGIN        foo_pkg.bar_get(l_cursor);    apex_json.open_object;    apex_json.write(&#39;&#39;bar&#39;&#39;, l_cursor);    apex_json.close_object;END;&#39;      );  COMMIT; END;/"
    },


    { 
        "title" : "Add support for tags to Cobalt",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "tags",
            
        
            "taxonomies",
            
        
            "categories"
            
        
        ],
        "href" : "2017/cobalt-tags",
        "content" : "A while back I started adding a new front matter attribute to all my postscalled tags. I initially did this to improve my site search indexing (you canread about that here).After spending some time thinking about how to add tags to the site, and Ithink I've come up with a reasonable solution.The biggest problem with tags is that you can't easily codegen the templatelike the one I used for the documents collectionfeed to seed the lunrindexer.The code for the tag template looks like this.&lt;/ul&gt;{% endfor %}{% endfor %}    {% endif %}        {% break %}&lt;/a&gt;&lt;/li&gt;            {{ post.title }}&quot;&gt;{{post.permalink }}            &lt;li&gt;&lt;a href=&quot;/{% if tag == filter_tag %}        {% for tag in tags %}    {% assign tags = post.data.tags | replace: &quot; &quot;, &quot;&quot; |  split: &quot;,&quot; %}    {% for post in collections.posts.pages reversed %}&lt;/h1&gt;&lt;ul&gt;{{ filter_tag }}&lt;h1&gt;Posts tagged as The code is fairly trivially, grab the tags attribute turn it into an arrayof tags and then loop over it and add a link if the tag is found.If you were to create a template per tag, there's a risk of code duplication.After that, it's only a matter of time, when you &quot;fix&quot; one template, but forgetapply to the others.You could mitigate against this by recreating all the tag pages at build time.I may revisit this solution at a later date to see if there's any otherbenefits that might have been overlooked.In the end I decided to use the include liquid tag to reuse the tag templatecode, again I got the idea from Johann's excellent blogwhere he broke up his default layout into components using include. One thingto note about using the include tag is that you shouldn't include a frontmatter section otherwise this will also be rendered.tags/cobalt.liquidfront matterextends: default.liquidtitle: booyaa&#39;s boggy bloggypath: tags/cobalt/comment: collection of cobalt related posts---The only item of note here is the path, I've decided to utilise tags as asub-path. I've not created a index page for this sub-path, so it'll give a 404for now.content{% include &quot;_tag.liquid&quot; %}{% assign filter_tag = &quot;cobalt&quot; %}The tag page content literally define the tag filter that will be used by thetags template and then includes the tags template.The end result can be found here. I should add that, I'm stillthinking about a way to add this link within related posts, but I think that'sanother problem altogether! The source code can be foundhere."
    },


    { 
        "title" : "Setting up exercism python track with Visual Studio Code",

        
        
        
        "tags": [
        
            "python",
            
        
            "exercism",
            
        
            "pylint",
            
        
            "pytest",
            
        
            "pep8",
            
        
            "vscode"
            
        
        ],
        "href" : "2017/exercism-python",
        "content" : "Here's a fairly good setup for getting the python track of exercism workingwith virtual env and Visual Studio Code.virtual envcreate one, and install the following packages:pip install pylint autopep8 # alwayspip install pytest pytest-cache # minimumpip install pytest-pep8 pdb # bonusvisual studio codeDon Jayanmanne’s Python extensionunit testing (using virtualenv/pyvenv)Configure using Run All Unit Tests  which will addsettings.json{    &quot;python.unitTest.pyTestArgs&quot;: [        &quot;.&quot;    ],    &quot;python.unitTest.pyTestEnabled&quot;: true}ignore certain pylint warningssettings.json{    &quot;python.linting.pylintArgs&quot;: [        &quot;--disable=C0111&quot;     ]}setup command line toolsfirst time setupinstall command line tools so you can launch code using code .activate your virtual envnav to your exercism execise path (don’t open the main python folder, code can only handle a single exercise.code .verify virtual environment is being used trigger command pallet and run Python: Select Interpreter Workspace (this doesn’t work at the moment since downgrading to 3.5.2check that intellisense and linting are workingsetup unit testsuseful http://exercism.io/languages/python/testsQuestionshow do i get code to run unit tests on saveenable pdb on unit test failure"
    },


    { 
        "title" : "python tips",

        
        
        
        "tags": [
        
            "python",
            
        
            "homebrew",
            
        
            "raspbian"
            
        
        ],
        "href" : "2017/python",
        "content" : "A random collection of Python tips, I also write Python code in MacOS andraspbian so you'll see tips for those platforms.Docker images (python related)httpbin - docker run -it --rm -p 8000:8000 citizenstig/httpbinjupyter notebook - docker run -it --rm -p 8888:8888 jupyter/all-spark-notebookpylint messagesto suppress # pylint: disable=invalid-nameto see a list pylint --list-msgs | grep -i W05Returning something useful from a classstr methodclass foo:  def __str__(self):    return “something rendered nicely”SimpleHTTPServer for python 3python3 -m http.server 8080TabulatingPanda’s probably a better fit because it can generate all kinds of tables (markdown, ascii, html etc)If you need something quick use tabulate.&gt;&gt;&gt; list(zip(*table2))[(1, &#39;A&#39;, &#39;@&#39;), (2, &#39;B&#39;, &#39;$&#39;), (3, &#39;C&#39;, &#39;!&#39;)]&gt;&gt;&gt; print(tabulate([list(x) for x in zip(*table2)], headers=[&quot;i1&quot;,&quot;i2&quot;,&quot;i3&quot;]))  i1  i2    i3----  ----  ----   1  A     @   2  B     $   3  C     !&gt;&gt;&gt; table2[[1, 2, 3], [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;], [&#39;@&#39;, &#39;$&#39;, &#39;!&#39;]]versioningdowngrading to python 3.5.2 in homebrewbrew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f377ed1a1b568d5624164146ec7d6855fe175826/Formula/python3.rbpython3 -Vbrew pin python3switching back to 3.6.0brew unpin python3brew switch python3 3.6.0updating python3 pypieasy_install3 -U pippython -m pip install —upgrade pip # betterupgrading pip (python2)pip install —upgrade pippip install —upgrade setuptoolsif it fails (pip’s a module, avoid using this unless you’re using debian)apt-get remove pipeasy_install pippip install —upgrade pip"
    },


    { 
        "title" : "youtube-dl gems",

        
        
        
        "tags": [
        
            "youtube-dl"
            
        
        ],
        "href" : "2017/youtube-dl",
        "content" : "A handy collection of youtube-dl incantations, remember it's not just for youtube-dl! There's plenty more, but these are the one use I use on a daily basis.If you're a windows users, these should work on your platform provided you change any directory references from / to \\. And remove \\ and concatenate the entire incantation into a single line.youtube-dl -F VIDEO_URL # see all video formats available for a given urlyoutube-dl -f mp4 VIDEO_URL # download specific formatyoutube-dl -f mp4 -o &#39;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&#39; \\     &#39;YOUTUBE_PLAYLIST_URL&#39; # downloads playlist in number orderyoutube-dl --playlist-items ITEM_SPEC &#39;YOUTUBE_PLAYLIST_URL&#39; \\    # download items from a playlist where ITEM_SPEC is 1,2,3 or 1-3,4,9-100 youtube-dl --verbose --username PS_USER --password PS_PASS \\    --sleep-interval 200 -o &quot;%(playlist)s/%(chapter_number)s.%(chapter)s/%(playlist_index)s.%(title)s.%(ext)s&quot; \\    --restrict-filenames PLURALSIGHT_URL \\    # where PS_USER and PS_PASS are your pluralsight credentials"
    },


    { 
        "title" : "Adding an archive page to your Cobalt blog",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-an-archive",
        "content" : "To avoid slowing down the index page, there's a point where you need to limithow many blog posts you want to appear on screen. This in turn presents anotherproblem, how do you then provide a way to display older posts? Enter an archivepage!archive.liquidfront matterextends: default.liquidtitle: archivepath: /archiveroute: archive---templateThis is a quick work around until I&#39;ve had a good think about a way to organise the archive.&lt;h2&gt;2017&lt;/h2&gt;&lt;ul&gt;{% for post in collections.posts.pages %}{% assign year = post.permalink | truncate: 4, &quot;&quot; %}{% if year == &quot;2017&quot; %}  &lt;li&gt;&lt;a href=&quot;/{{ post.permalink }}&quot;&gt;{{ post.title }}&lt;/a&gt; - {{ post.content | strip_html | truncatewords: 25, &#39;...&#39; }}&lt;/li&gt;{% endif %}{% endfor %}&lt;/ul&gt;The template is placed in the root of your cobalt source dirctory. Yes it's abit of hack, but I'm still playing around with the format. I think eventually Imay even turn the archiving into a code generated process during build time."
    },


    { 
        "title" : "Flashback, what did this data look like previously?",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "flashback",
            
        
            "lsd"
            
        
        ],
        "href" : "2017/flashback",
        "content" : "I'm only scratching the surface of what you can do with flashbacks in Oracle. Our DBAs are absolute ninjas when it comes to using this witchcraft from Oracle.The example below lets us look at a data dictionary for materialized refreshgroups and what the values were an hour ago.CLEAR SCREENCOL name FORMAT a30COL rname FORMAT a30SELECT    name,    rnameFROM    all_refresh_children AS OF TIMESTAMP ( trunc(SYSDATE - 1 / 24) )WHERE    rname = &#39;RFG_GROUP2&#39;;where:1 / 24 is an hour ago36 / 24 is yesterday an hour and half ago"
    },


    { 
        "title" : "SQL Developer's new format hints",

        
        
        
        "tags": [
        
            "sqldeveloper",
            
        
            "csv",
            
        
            "format"
            
        
        ],
        "href" : "2017/sql-developer-format-hints",
        "content" : "One of my favourite cool features in SQL Developer is the ability to turn a sqlquery output into an entirely different format. It doesn't even require breaking out the import/export wizard.SPOOL C:\\TEMP\\foobar.csvSELECT /*csv*/ *  FROM foo  LEFT  JOIN bar    ON foo.id = bar.id WHERE foo.name LIKE &#39;%HURR%&#39;;  SPOOL OFFComplete list of formatsSELECT /*csv*/ * FROM scott.emp;SELECT /*xml*/ * FROM scott.emp;SELECT /*html*/ * FROM scott.emp;SELECT /*delimited*/ * FROM scott.emp;SELECT /*insert*/ * FROM scott.emp;SELECT /*loader*/ * FROM scott.emp;SELECT /*fixed*/ * FROM scott.emp;SELECT /*text*/ * FROM scott.emp;Full details and source can be  here."
    },


    { 
        "title" : "Bitmasks in SQL",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "sql",
            
        
            "bitmasks"
            
        
        ],
        "href" : "2017/bitmasks-sql",
        "content" : "bitmasks are really handy way to express predicates without becoming overlyverbose with parens and logical operators (AND and OR). Assume we have the following table.| what        | wanted ||-------------|--------|| need me too | 4      || alpha       | 1      || beta        | 2      |with data as(select &#39;appears_in_both&#39;, 4 as wanted from dualunion allselect &#39;alpha&#39;, 1 as wanted from dualunion allselect &#39;beta&#39;, 2 as wanted from dual)...Get &quot;need me too&quot; and &quot;alpha&quot; together...select * from datawhere bitand(wanted, 5) &lt;&gt; 0; -- (4 + 1)Get &quot;need me too&quot; and &quot;beta&quot; together...select * from datawhere bitand(wanted, 6) &lt;&gt; 0; -- (4 + 2)"
    },


    { 
        "title" : "Dirty Dynamic SQL",

        
        
        
        "tags": [
        
            "dynamic",
            
        
            "sql",
            
        
            "danger",
            
        
            "oneliners",
            
        
            "codegen"
            
        
        ],
        "href" : "2017/dirty-dynamic-sql",
        "content" : "Not all dynamic sql is strictly for immediate execution, nor is it dirty (Ineeded aninteresting title). I learnt this tricks from my friend at work, he'sa big believer of using sql to code generate more sql.Altering column lengthYou'll still get an error when running the generated sql if you try to shrink a column.SELECT &#39;ALTER TABLE &#39; || table_name || &#39; MODIFY &#39; || column_name || &#39; VARCHAR2(&#39; || CASE column_name  WHEN &#39;FOO&#39; THEN &#39;10&#39;  ELSE &#39;20&#39; END || &#39;);&#39; AS sql  FROM user_tab_columns  WHERE column_name IN ( &#39;FOO&#39;,&#39;BAR&#39;)   AND table_name IN (SELECT table_name FROM user_tables)      AND NOT regexp_like(table_name, &#39;_(NEW|BAK)$&#39;) -- exclude backupsInserting (hurr) output from dynamic sqlNormally you'd expect to get away with using something like SELECT blah INTO v_foo, but with dynamic sql you need place the INTO in the EXECUTE IMMEDIATE statement.SET SERVEROUTPUT ONDECLARE  v_sql VARCHAR2(8000);  v_count NUMBER;  CURSOR c_tables      IS  SELECT table_name    FROM user_tables;BEGIN  FOR rec in c_tables  LOOP    v_sql := &#39;SELECT count(*) FROM &#39; || rec.table_name;    EXECUTE IMMEDIATE v_sql INTO v_count;        DBMS_OUTPUT.put_line(rec.table_name || &#39; has &#39; || v_count);  END LOOP;END;/tags: rowcount , row , countUsing substution variablesclear screenset serveroutput ondeclare  cursor c_tables is select table_name from user_tables;  v_data VARCHAR2(2000);  v_sql VARCHAR2(2000);  v_stuff VARCHAR2(50) := &#39;&amp;1&#39;; -- try entering MAX(DATADATE) or COUNT(*) begin  for rec in c_tables  loop    v_sql := &#39;SELECT TO_CHAR(&#39; || v_stuff || &#39;) FROM &#39; || rec.table_name;        begin      execute immediate v_sql into v_data;      dbms_output.put_line(rec.table_name || &#39;: &#39; || v_data);        exception      when others then        dbms_output.put_line(&#39;failed to run: &#39; || v_sql);        raise;    end;    end case;  end loop;end;/One linersGrantsSELECT &#39;GRANT &#39; || privilege || &#39; ON &quot;&#39; || GRANTOR ||&#39;&quot;.&quot;&#39; || TABLE_NAME ||        &#39;&quot; TO &quot;&#39; || GRANTEE || &#39;&quot;;&#39; FROM dba_tab_privs WHERE grantor = &#39;FOO&#39;AND grantee = &#39;BAR&#39;;Recreating synonyms as sysselect &#39;CREATE OR REPLACE SYNONYM &quot;&#39; || OWNER || &#39;&quot;.&quot;&#39; || SYNONYM_NAME ||        &#39;&quot; FOR &quot;&#39; || TABLE_OWNER || &#39;&quot;.&quot;&#39; || TABLE_NAME || &#39;&quot;;&#39; from dba_synonyms where table_owner = &#39;FOO&#39;;"
    },


    { 
        "title" : "When XML attacks!",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "xmltype",
            
        
            "xml",
            
        
            "extractvalue"
            
        
        ],
        "href" : "2017/when-xml-attacks",
        "content" : "At some point in your xml wrangling career you will hit an node whose data istoo big for Oracle's EXTRACTVALUE (I think the upper limit is 4000characters) and get this lovely message.ORA-01706: user function result value was too large01706. 00000 -  &quot;user function result value was too large&quot;To mitigate this problem, you need to switch to XMLTABLE and it's usefulPASSING attribute:SELECT    fix.biggie AS notoriousFROM    source_table_with_xmltype_column o,    XMLTABLE ( &#39;/*&#39;        PASSING o.xml_field COLUMNS            biggie VARCHAR2(4000) PATH &#39;substring(/path/to/offending/item/text(),1,3999)&#39;    ) fix;The gist of the fix is to use xpath to truncate the text value before it'shanded off to Oracle. Trying to cast the xmltype column as a varchar2(4000)will not fix it because the problem happens as extractvalue is parsing thenode.Items of noteXML ( '/*' - how to specify the root of an xml documentbiggie VARCHAR2(4000) PATH 'substring(/path/to/offending/item/text(),1,3999)' how to get the text from an xpath and truncate it to 4000 chars.sources:XMLTABLE : Convert XML Data into Rows and Columns using SQLSO (Answer): How to use xmltable in oracle?SO (Answer): Oracle SQL - Extracting clob value from XML with repeating nodes"
    },


    { 
        "title" : "Bind vs Substitution variables",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "variables",
            
        
            "bind",
            
        
            "substitution"
            
        
        ],
        "href" : "2017/bind-vs-substitution",
        "content" : "I always have difficulty remember the difference between these type of variables. Although now, that I've started doing a lot of ORDS related work,the difference is become more apparent.Also substitution variables are really a binding to a user variable.Bind variablesPROMPT bind variables are...VAR foo VARCHAR2SET SERVEROUTPUT ONBEGIN  :FOO := &#39;in PL/SQL blocks&#39;;  DBMS_OUTPUT.PUT_LINE(&#39;mostly used...&#39;);END;/SET SERVEROUTPUT OFFPRINT foooutputbind variables are...PL/SQL procedure successfully completed.mostly used...FOO---in PL/SQL blocksSubstitution variablesPROMPT Where as substitution variable are...DEFINE FOO = &#39;useful in SQL scripts&#39;SET VERIFY OFFSELECT &#39;&amp;FOO&#39; AS BAR FROM DUAL;SET VERIFY ONoutputWhere as user variable are...BAR                 ---------------------useful in SQL scriptsTo be explict, use bind vars to interact with PL/SQL blocks and substituionvariables could be used anywhere. The only downside to substituion variables isthat you can't DEFINE a variable with another user variable.Also you can break substitution variables, if there's an errant SET DEFINE OFF, still not sure how you could test for this? Perhaps using default valuesand testing for it i.e. like SQLCMD variables.Further reading:Literals, Substitution Variables and Bind VariablesPL/SQL 101 : Substitution vs. Bind Variables"
    },


    { 
        "title" : "PL/SQL script to query a refcursor",

        
        
        
        "tags": [
        
            "cursors",
            
        
            "oracle",
            
        
            "plsql"
            
        
        ],
        "href" : "2017/plsql-script-cursors",
        "content" : "This will probably work in pipelined functions or packages too. Note the use ofthe bind variable to link the PL/SQL script variables to the out refcursor.SET SERVEROUTPUT ONCLEAR SCREENVAR P_NAME CHARVAR P_TABLES REFCURSORDECLARE  PROCEDURE table_get(p_name IN VARCHAR2, p_tables OUT SYS_REFCURSOR)  AS  BEGIN    OPEN p_tables      FOR        SELECT *           FROM user_tables          WHERE TABLE_NAME LIKE P_NAME;  END;BEGIN    :P_NAME := &#39;%REQ%&#39;;  table_get(:P_NAME, :P_TABLES);END;/PRINT P_TABLES"
    },


    { 
        "title" : "Defensive coding in SQL",

        
        
        
        "tags": [
        
            ""
            
        
        ],
        "href" : "2017/defensive-coding-sql",
        "content" : "Always wrap ON clauses in parens to avoid predicates being deletedaccidentally. The following code will scream if you delete the AND clause.SELECT *  FROM foo  LEFT JOIN bar    ON ((foo.id = bar.id)        AND (foo.fizz = bar.buzz))Where as the following won't.SELECT *  FROM foo  LEFT JOIN bar    ON foo.id = bar.id        AND foo.fizz = bar.buzz"
    },


    { 
        "title" : "Collapsible Sections in HTML",

        
        
        
        "tags": [
        
            "html",
            
        
            "github"
            
        
        ],
        "href" : "2017/collapsible-sections",
        "content" : "But first a demo...  Works great in GitHub! (click me)  Dipshit with a nine-toed woman. Dolor sit amet, consectetur adipiscing elit praesent ac magna. You don’t go out and make a living dressed like that in the middle of a weekday. Justo pellentesque ac lectus quis. Yeah. Roadie for Metallica. Speed of Sound Tour. Elit blandit fringilla a ut turpis praesent felis. Keep your ugly fucking goldbricking ass out of my beach community! Ligula, malesuada suscipit malesuada non, ultrices non urna sed orci ipsum, placerat id. Code&lt;details&gt;  &lt;summary&gt;Click to expand...&lt;/summary&gt;  &lt;p&gt;Imagine this is the entire text of War and Peace.&lt;/p&gt;&lt;/details&gt; "
    },


    { 
        "title" : "See hidden files in Finder",

        
        
        
        "tags": [
        
            "mac",
            
        
            "finder"
            
        
        ],
        "href" : "2017/hidden-files",
        "content" : "If you search for how to do this, you get a lot of nonsense involving messing with defaults write and horrific applescript bodges.CMD + SHIFT + .That's command key, shift and full stop (period) pressed at the same time."
    },


    { 
        "title" : "Using pre-built lunr indexes",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "search",
            
        
            "lunr"
            
        
        ],
        "href" : "2017/prebuilt-lunr-indexes",
        "content" : "I've implemented pre-built indexes vs. on demand i.e. generating them when the search page is being loaded. I'm not entirely happy with the solution yet for the following reasons:Additional requirements - pre-built indexes require node.js to generate theindexes offline.Documents collection - the original search page still depends on the reffield (the link to the relevant blog post) and title because the lunr index doesn't provide this. The content field is now surplus to requirements once the site has been built, but it is needed during index generation.Added complexity - I know adding node.js isn't a big ask, but it has resulted in a new makefile. And until I update my travis config, pre-built indexes onlyget created when I've written a new article.One novel approach to solving both these problems is to utilise a V8 crate andrun lunr index natively. This is the approach adopted by the lunr indexer for middleman (a static site generate written in ruby).Since we're not looking at native Cobalt solutions i.e. using liquid templates, another approach would be to modify the node.js script used to pre-build the index, to parse the content natively and remove the need for the lunr.liquid template. The script would also need to generate the documents collection that's still used by the search page minus the content field.The code changes can be found in the following commits:node.js script - used to pre-build the index, mostly cribbed from the lunr guide on that subject.makefile and search template - the makefile is cribbed from Matthias Endler's own version used to build his Cobalt site. The search template adds the prebuilt index to the list ofassets to be loaded when the page is loaded."
    },


    { 
        "title" : "Useful git commmands",

        
        
        
        "tags": [
        
            "git"
            
        
        ],
        "href" : "2017/useful-git-commands",
        "content" : "Commit logsNow that I've started adding useful commit messagesit's really use to make sense of my commits, more so when you use single line mode.$ git log --oneline3b04513 doc: add &#39;comment&#39; tag to explain purpose of new templates86cc454 feat: add tags to search7286f90 feat: add twitter share button24cfc85 feat: add todo20d28d7 chore: remove manually generated lunr index...Started working on a feature/fix, but forgot you we're in master?This assumes you haven't committed your changes...git checkout -b new_branch# add and commit your changesgit checkout master# clean up any files you don&#39;t want to commit...Sync your repo with the upstreamSetup a remote to the upstream.git remote add upstream &lt;path/to/upstream&gt; # e.g. https://github.com/cobalt-org/liquid-rust.gitNow sync!git fetch upstream  # gets branches and commits from upstreamgit checkout master # switch to your fork’s master branch (if you&#39;re not already there)git merge upstream/master # syncgit push -u origin master # update remote repo of your forkThis can also be used to push your GitHub repos to other hosted SCM providers i.e. GitLab and BitBucketSubmodulesYou want to add repo Bar to your own repo Foogit submodule add https://path/to/barTo pull the changes ingit submodule initTo do this at clone timegit clone --recursive https://repo/contain/submodulesTo refresh the submodule (assuming the dir name is the same as the submodule)git submodule update --remote submodule_namePossible gotcha if you refresh your submodule, you will need to commit newversion into your own repo.TagsN.B. Tags when push to GitHub become releases!to taggit tag v1.0.0to see what tag we're ongit tagto pushgit push --tagsto pullgit fetch --tagsFixing &quot;fatal: refusing to merge unrelated histories&quot;Warning: this is destructive, if you've got local uncommitted changes do not proceed!$ git pull origin masterFrom github.com:booyaa/broken_repo * branch            master     -&gt; FETCH_HEADfatal: refusing to merge unrelated histories$ git fetch # may be superflurous$ git reset --hard origin/master$ git branch --set-upstream-to=origin/master masterBranch master set up to track remote branch master from origin.Updating a branch with new changes from mastergit checkout out_of_date_branchgit merge origin/mastergit push origin out_of_date_branch"
    },


    { 
        "title" : "Adding search to your cobalt site - Part Two",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "search",
            
        
            "lunr",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-search-to-your-cobalt-site-part-two",
        "content" : "This will be a two part post, where I detail the steps it took to enable search on my Cobalt site.As you may have gathered in part one creating manual document collections is a bit of a chore, and can be easily done using the liquid templating engine.lunr.liquidfront mattertitle: lunr indexpath: /js/lunr_docs.json---The item of note here, is the path which Cobalt will use to create the lunr document collection.content{% assign idx = 0 %}{% assign post_count = collections.posts.pages | size %}[{% for post in collections.posts.pages %}{% assign idx = idx | plus: 1 %}    {         &quot;title&quot; : &quot;{{ post.title }}&quot;,        {% assign tags_list = post.data.tags | replace: &quot; &quot;, &quot;&quot; | split: &quot;,&quot;  %}        {% assign tags_size = tags_list | size | minus: 1 %}        {% assign idx2 = 0 %}        &quot;tags&quot;: [        {% for tag in tags_list %}            &quot;{{ tag }}&quot;{% if idx2 &lt; tags_size %},{% endif %}            {% assign idx2 = idx2 | plus: 1 %}        {% endfor %}        ],        &quot;href&quot; : &quot;{{ post.permalink }}&quot;,        &quot;content&quot; : &quot;{{ post.content | strip_html | strip_newlines | replace: &quot;\\&quot;, &quot;\\\\&quot; }}&quot;    }{% if idx &lt; post_count %},{% endif %}{% endfor %}]The only real difference here between a blog index is that I'm tracking the last post using an index so I can omit a trailing comma.Don't forget to remove the manually generated document collection (lunr_docs.json) from the assets directory for javascript files (js/) , as this caught me out and made me wonder why the index wasn't being updated.Updates2017-06-21 - Change references to the lunr index to the lunr document collection."
    },


    { 
        "title" : "Adding search to your cobalt site - Part One",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "search",
            
        
            "lunr",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-search-to-your-cobalt-site-part-one",
        "content" : "This will be a two part post, where I detail the steps it took to enable search on my Cobalt site.In this first post I will detail how to integrate lunr  using a manually created document collection. If you already know how to wire up lunr, you can skip to second post, where I create the document collection using a liquid template.I love blogs, but after the initial excitement of discovery you become curious about the blog author and if they have other posts of a similar topic. At this point you expect to have either a local search or some form of taxonomy.This is often a shortcoming of using a static site generator to power your blog. It's great at producing pre-rendered static pages, but search or tag views are the domain of a dynamic content management system.As the title of this blog post suggests, I will be looking to solve the problem of local search. I'll leave the subject of taxonomies for a future blog post.It turns out that we can approximate a resonable search experience using lunr. lunr is a light weight implementation of Solr the enterprise search platform (off the shelf search engine). With a little bit of jQuery and the lunr library we can cobble together a search page in Cobalt.My Cobalt source directory structureI've shared my own blog's structure as an way to refer to locations within a Cobalt source directory..├── _drafts├── _layouts├── blog├── img└── jsThere should be no surprises here, our primary areas of interest will be the js directory where javascript related assets live. And the root of the directory where my index, about and licenses pages live (all extending the default liquid template).search.liquidI placed my search template in the root of my Cobalt source and it looks like this.frontmatterextends: default.liquidtitle: searchpath:  search/---contentThe general gist (pun intended) of the template is to create an text input box and a hook for the results to appear in.The template is a copy of this gist for getting lunr to work with hugo (an excellent static site generator written in Go).It has some tweaks to get it working with lunr v2.1.0.&lt;input id=&quot;search&quot; type=&quot;text&quot; size=&quot;25&quot; placeholder=&quot;search for stuff here...&quot;        autofocus&gt;&lt;br /&gt;&lt;ul id=&quot;results&quot;&gt;&lt;/ul&gt;&lt;script type=&quot;text/javascript&quot;         src=&quot;https://code.jquery.com/jquery-2.1.3.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;https://unpkg.com/lunr/lunr.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt;var lunrIndex,    $results,    pagesIndex;// This is pretty much a copy of // https://gist.github.com/sebz/efddfc8fdcb6b480f567// Initialize lunrjs using our generated index filefunction initLunr() {    // First retrieve the index file    $.getJSON(&quot;/js/lunr_docs.json&quot;)        .done(function(index) {            pagesIndex = index;            // Set up lunrjs by declaring the fields we use            // Also provide their boost level for the ranking            lunrIndex = lunr(function() {                this.field(&quot;title&quot;, {                    boost: 10                });                this.field(&quot;content&quot;);                // ref is the result item identifier (I chose the page URL)                this.ref(&quot;href&quot;);                                // Feed lunr with each file and let lunr actually index them                pagesIndex.forEach(function(page) {                    this.add(page)                }, this);            });                    })        .fail(function(jqxhr, textStatus, error) {            var err = textStatus + &quot;, &quot; + error;            console.error(&quot;Error getting cobalt index file:&quot;, err);        });}// Nothing crazy here, just hook up a listener on the input fieldfunction initUI() {    $results = $(&quot;#results&quot;);    $(&quot;#search&quot;).keyup(function() {        $results.empty();        // Only trigger a search when 2 chars. at least have been provided        var query = $(this).val();        if (query.length &lt; 2) {            return;        }        var results = search(query);        renderResults(results);    });}/**    * Trigger a search in lunr and transform the result    *    * @param  {String} query    * @return {Array}  results    */function search(query) {    // Find the item in our index corresponding to the lunr one to have more     // info    // Lunr result:     //  {ref: &quot;/section/page1&quot;, score: 0.2725657778206127}    // Our result:    //  {title:&quot;Page1&quot;, href:&quot;/section/page1&quot;, ...}    return lunrIndex.search(query).map(function(result) {            return pagesIndex.filter(function(page) {                return page.href === result.ref;            })[0];        });}/**    * Display the 10 first results    *    * @param  {Array} results to display    */function renderResults(results) {    if (!results.length) {        return;    }    // Only show the ten first results    results.slice(0, 10).forEach(function(result) {        var $result = $(&quot;&lt;li style=\\&quot;list-style:none;\\&quot;&gt;&quot;); // FUUUUUU!        $result.append($(&quot;&lt;a&gt;&quot;, {            href: result.href,            text: &quot;» &quot; + result.title        }));        $results.append($result);    });}// Let&#39;s get startedinitLunr();$(document).ready(function() {    initUI();});&lt;/script&gt;An artisanal lunr document collectionTo get my proof of concept going, I needed to feed lunr a distilled form of my blog posts, which I called lunr_docs.json and stored it in /js. In an earlier version of this post, I mistakenly referred to this as the lunr index.It's the data that will be used to generate lunr's index. The index has different structure that we'll learn about in part two.[{    &quot;title&quot;: &quot;Useful commit messages&quot;,    &quot;href&quot;: &quot;/2017/useful-commit-messages/&quot;,    &quot;content&quot;: &quot; Keeping a copy of this excellent bit of advice until I&#39;ve committed (no pun) it to memory. &quot;}, {    &quot;title&quot;: &quot;Add reading time in Cobalt&quot;,    &quot;href&quot;: &quot;/2017/add-reading-time/&quot;,    &quot;content&quot;: &quot; I wanted to add an approximate reading time to each of my blog posts, like those seen in medium posts. &quot;}, {    &quot;title&quot;: &quot;Using a custom domain with GitHub Pages&quot;,    &quot;href&quot;: &quot;/2017/gh-pages-custom-domain/&quot;,    &quot;content&quot;: &quot; It took far too long to work out how to do this on the GitHub help pages... &quot;}, {    &quot;title&quot;: &quot;Using Cobalt with GitHub pages&quot;,    &quot;href&quot;: &quot;/2017/cobalt-github/&quot;,    &quot;content&quot;: &quot; It turns out using Cobalt and your personal GitHub page is a bit trickier to setup. Your personal GitHub page as oppose to your repo GitHub page, must have the content in the master branch. Repository/Project GitHub pages can live in a subdir of default branch i.e. docs &quot;}, {    &quot;title&quot;: &quot;MacBook Air Setup&quot;,    &quot;href&quot;: &quot;/2017/mba-setup/&quot;,    &quot;content&quot;: &quot; Here&#39;s my current setup for my MacBook Air Setup. I use a range of tools like homebrew, Visual Studio Code and vim. &quot;}]The format of the document collection is fairly trivial and only has a very small fragment of the blog post, which will affect searching.Incidentally the boost property for the title field in the search template is probably superflorous as it's the only item being searched again. The original source for the code also utilised a tag field, and boost allows you to give a weighting for which field should be favoured when searching the index.As you can imagine hand crafting an document collection is a bit lo-fi, so if you want to see what I used in the end (another liquid template), check out part two of this blog post.Putting it all togetherOnce you've created the search template and the lunr index, all you need to do is perform your usual cobalt build workflow.If you've followed my structure, your search page can be found in /search. Search results should appear immediately as you start to type in the input box.But is it webscale?I have no idea, I don't have a large enough volume of data to test against. However lunr v2.x's lunr.Index.load function allows you to load a pre-built index, but this does add an extra of complexity. And at time of writing will require either node.js or some form of v8 context to generate it. The idea is that you index the document collection and serialise the index generated.More details can be found here.Anything else?I'd love to remove the dependency on jQuery, but to be fair I can't be arsed to rewrite in vanilla js as it just works ™. Saying that, it has got me thinking perhaps, I should create a liquid-rust extension (not sure if it'll be a tag, filter or other) to generate vanilla js for things that causes people to reach for jQuery.Also I have no idea why the damn input box is so small.Any guidence or help with either of these two issues would be greatly appreciated.Updates2017-06-21 - Change references to the lunr index to the lunr document collection."
    },


    { 
        "title" : "Useful commit messages",

        
        
        
        "tags": [
        
            "git"
            
        
        ],
        "href" : "2017/useful-commit-messages",
        "content" : "Keeping a copy of this excellent bit of advice until I've committed (no pun) itto memory.feat: add hat wobble^--^  ^------------^|     ||     +-&gt; Summary in present tense.|+-------&gt; Type: chore, docs, feat, fix, refactor, style, or test.Source: https://seesparkbox.com/foundry/semantic_commit_messages"
    },


    { 
        "title" : "Add reading time in Cobalt",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/add-reading-time",
        "content" : "I wanted to add an approximate reading time to each of my blog posts, like those seen in medium posts. There's a lot of really nice javascript libraries out that, but I kinda figured this defeat the whole purposeof using a static site generator.I was also looking for an excuse to use liquid (the template rendering engine for Cobalt).It's a fairly simplistic approach, perform a word count and divide by 200 which is the average of words read per min.{% assign reading_wpm = 200 %}{% assign word_count = page.content | split: &quot; &quot; | size %}{% assign reading_time = word_count | divided_by: 200 %}{% case reading_time %}  {% when 0 %}    {% assign phrase = &quot;less than a minute.&quot; %}  {% when 1 %}    {% assign phrase = &quot;about a minute.&quot; %}  {% else %}    {% assign phrase = &quot; minutes.&quot; | prepend: reading_time %}{% endcase %}Reading time: {{ phrase }}"
    },


    { 
        "title" : "Using a custom domain with GitHub Pages",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "dns"
            
        
        ],
        "href" : "2017/gh-pages-custom-domain",
        "content" : "It took far too long to work out how to do this on the GitHub help pages...Assumptions:I've only tested for personal/user domain i.e. the doc root forhttp://USERNAME.github.io/.You've already have an A (APEX) record for your existing site.You've already got a CNAME record that points to the A record.InstructionsEnable custom domain in your repository (settings).Update your A record to point to IP addresses: 192.30.252.153 and 192.30.252.154. Pro-tip: Switch your DNS management to cloudflare if you want super fast switch from your old hosting to GitHub.echo &quot;your-domain-name&quot; &gt; CNAME in the default branch repo. This will besource if you're doing this for a Cobalt site."
    },


    { 
        "title" : "Using Cobalt with GitHub pages",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github"
            
        
        ],
        "href" : "2017/cobalt-github",
        "content" : "It turns out using Cobalt and your personal GitHub page is a bit trickier to setup. Your personal GitHub page as oppose to your repo GitHub page, must have the content in the master branch. Repository/Project GitHub pages can live in a subdir of default branch i.e. docsThis is not a criticism of Cobalt, but rather myself demonstrating my lack of git prowess.The general gist of this how to, is that you place your Cobalt project in thesource branch and then use cobalt import --branch master to transfer therendered content to master.InstructionsCreate your user/personal page repo (USERNAME.github.io)Follow instructions for initialising a repogit checkout -b sourcecobalt init# do cobalty stuff..cobalt build# commit, maybe even push to sourcecobalt import --branch mastergit checkout master# commit, definitely pushIf you want to check out my setup you can find it here.Things I've not worked out yetThe build folder gets copied to the master branch during cobalt import.The folder doesn't contain any files. My work around at the moment is to add itto .gitignore in the master branch.Thanks to...Johann Hofmann for allowing me to copy his blog's style (and his travis-ci auto deploy setup).Cobalt for creating this easy to use static site generator."
    },


    { 
        "title" : "MacBook Air Setup",

        
        
        
        "tags": [
        
            "mac",
            
        
            "homebrew",
            
        
            "vscode",
            
        
            "vim",
            
        
            "setup"
            
        
        ],
        "href" : "2017/mba-setup",
        "content" : "Here's my current setup for my MacBook Air Setup. I use a range of tools like homebrew, Visual Studio Code and vim.homebrewbrew install \\        python python3 bash-git-prompt elixir figlet ffmpeg go httpie \\         imagemagick jq nmap zeromq reattach-to-user-namespace tmux sqlite \\         watch vim yarn youtube-dl doctlvscode extensionsgitlenshipsumInsert Date StringLiquid LanguagePrettyify JSONPython (Don's)vscode-rustvim plugins$ ls ~/.vim/bundle/go-explorer rust.vim  tagbar       vim-fugitive  vim-go    webapi-vimnerdtree    syntastic tcomment_vim vim-gitgutter vim-racer"
    },


    { 
        "title" : "Code generation scripts in PL/SQL",

        
        
        
        "tags": [
        
            "codegen",
            
        
            "plsql",
            
        
            "oracle",
            
        
            "xml"
            
        
        ],
        "href" : "2017/flattening-xml",
        "content" : "Flattening XML pathsThe table elephant_castle has a XMLTYPE column details that we want toget a list of distinct node paths.WITH node_list AS (    SELECT        x.*    FROM        elephant_castle ec        CROSS JOIN XMLTABLE ( &#39;declare function local:path-to-node( $nodes as node()* ) as xs:string* {     $nodes/string-join(ancestor-or-self::*/name(.),&#39;&#39;/&#39;&#39;) };for $i in $doc//*    let $node_path := local:path-to-node($i)      (: still don&#39;&#39;t have a clue how this func works :)    let $node_value := $i/text()    where string-length($node_value) &gt; 0            (: how to exclude nodes without values :)    return &lt;data&gt;                &lt;path&gt;{$node_path}&lt;/path&gt;                &lt;value&gt;{$node_value}&lt;/value&gt;            &lt;/data&gt;&#39;                PASSING ec.details AS &quot;doc&quot; COLUMNS                    node_path VARCHAR2(4000) PATH &#39;path&#39;,                    node_value VARCHAR2(4000) PATH &#39;value&#39; /* debugging */            ) x) SELECT distinct nl.node_pathFROM    node_list nl;    Generate a query that combines existing fields with newly flatten XML pathsThis script allows you to apply boiler plate (fields from the existing tableusing the data dictionary) and then make call out to another sqlplus scriptthat contains table specific mappings. The call out script is assumed to becalled TableName.sql.REM Suppress all headings, page breaks, titlesSET PAGESIZE 0REM Do not list the text of a command before and after replacing substitution REM variables with valuesSET VERIFY OFFREM  Do not display the number of records returned (when rows &gt;= n )SET FEEDBACK OFFCLEAR SCREENDEFINE TableName=STANDINGORDERDEFINE TableAlias=soDEFINE TableSchema=BPHADMIN/********************************************************************************* Generate existing column list, excluding XMLTYPE, BLOB and CLOB types.*******************************************************************************/WITH column_list AS (    SELECT        column_id,        column_name    FROM        all_tab_cols    WHERE        owner = &#39;&amp;TableSchema&#39;    AND        table_name = &#39;&amp;TableName&#39;    AND        data_type NOT IN (            &#39;XMLTYPE&#39;,&#39;BLOB&#39;,&#39;CLOB&#39;        )    ORDER BY column_id) SELECT    CASE column_id          WHEN 1 THEN &#39;SELECT &amp;TableAlias&#39;|| &#39;.&#39; || column_name        ELSE &#39;,&amp;TableAlias&#39;|| &#39;.&#39; || column_name    END as sqlFROM    column_list;/********************************************************************************* Call out script with custom XML flattening logic (usually field name tweaks)*******************************************************************************/@&amp;TableName/********************************************************************************* Add &quot;FROM&quot; statement*******************************************************************************/PROMPT FROM &amp;TableName &amp;TableAlias;;"
    }

]
